{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the documentation for the Cross-Dataset Discovery Service.</p> <p>This service provides an API for performing natural language search queries across a pre-indexed collection of datasets.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<ul> <li>System: Learn about the high-level Architecture, Security model, and Datastores.</li> <li>API: Find detailed information on the API endpoints and error codes.</li> <li>Setup &amp; Monitoring: Get instructions on Deployment, Configuration, and Logging.</li> </ul>"},{"location":"api-overview/","title":"API Overview","text":"<p>The Cross-Dataset Discovery service exposes a RESTful API for performing search operations.</p>"},{"location":"api-overview/#base-url","title":"Base URL","text":"<p>The API is served from the root of the application. All endpoint paths are relative to the base URL where the service is deployed:</p> <p>Base URL: https://datagems-dev.scayle.es/cross-dataset-discovery/</p>"},{"location":"api-overview/#authentication","title":"Authentication","text":"<p>All endpoints are protected and require a valid JWT Bearer token. See the Security page for more details.</p>"},{"location":"api-overview/#endpoints","title":"Endpoints","text":""},{"location":"api-overview/#search","title":"Search","text":"<p>Performs a search query against the indexed datasets.</p> <ul> <li>Endpoint: <code>POST /search/</code></li> <li>Description: Submits a natural language query and returns a ranked list of relevant results. The search can be optionally filtered to a specific set of datasets. The results returned are automatically filtered based on the user's access permissions.</li> <li> <p>Request Body:</p> <pre><code>{\n  \"query\": \"string\",\n  \"k\": 5,\n  \"dataset_ids\": [\n    \"string\"\n  ]\n}\n</code></pre> </li> <li> <p>Response Body (Success):</p> <pre><code>{\n  \"query_time\": \"retrieval time in seconds\",\n  \"results\": [\n    {\n      \"content\": \"The main text content of the search result.\",\n      \"dataset_id\": \"uuid-of-the-source-dataset\",\n      \"object_id\": \"uuid-of-the-source-object\",\n      \"similarity\": \"simialrity-score (higher the better)\"\n    }\n  ]\n}\n</code></pre> </li> </ul>"},{"location":"api-overview/#health-check","title":"Health Check","text":"<p>Verifies the operational status of the API and its dependencies.</p> <ul> <li>Endpoint: <code>GET /health</code></li> <li>Description: Checks the availability of the search component and the existance of the index. Returns a <code>200 OK</code> if all systems are healthy.</li> <li> <p>Response Body (Success):</p> <pre><code>{\n  \"status\": \"ok\",\n  \"message\": \"All dependencies are healthy.\"\n}\n</code></pre> </li> </ul>"},{"location":"architecture/","title":"System Architecture","text":"<p>The Cross-Dataset Discovery Service is a self-contained application designed to act as a specialized search layer within the DataGEMS ecosystem.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ol> <li> <p>FastAPI Application: The heart of the service is a Python web application built with FastAPI. It exposes the REST API, handles incoming HTTP requests, and orchestrates all internal operations.</p> </li> <li> <p>Search Component: An internal module responsible for executing queries against the search index. It takes the user query and returns a ranked list of document IDs.</p> </li> <li> <p>Security Layer: This layer intercepts all incoming requests to perform authentication and authorization. It integrates with an external OIDC provider to validate JWTs and with the DataGEMS Gateway to fetch user-specific permissions.</p> </li> </ol>"},{"location":"architecture/#request-flow","title":"Request Flow","text":"<p>A typical search request follows this path:</p> <ol> <li>A user sends a <code>POST /search/</code> request with a valid JWT.</li> <li>The Security Layer validates the token and checks for the required user roles.</li> <li>The service calls the DataGEMS Gateway to get the list of datasets the user is authorized to see.</li> <li>The user's query is passed to the Search Component, which queries the pre-built index returning an authorized dataset list.</li> <li>The final, authorized results are returned to the user.</li> </ol>"},{"location":"automations/","title":"Automations","text":"<p>This project leverages GitHub Actions to automate builds, code analysis, security scanning, and documentation deployment. The following sections detail the specific workflows configured for this repository.</p>"},{"location":"automations/#dockerfile","title":"Dockerfile","text":"<p>The creation of a production-ready container image is fully automated by the <code>Dockerfile</code> in the root of the repository. This build process ensures a consistent environment for the application.</p> <ul> <li>Stage 1 (Builder): This stage prepares the environment by installing all necessary build-time dependencies, including OpenJDK (for the search library) and the full set of Python packages.</li> <li>Stage 2 (Final): This stage constructs the final, lean image by copying only the essential artifacts from the builder stage. It creates a non-root <code>appuser</code> for security, copies the application code, and sets the <code>gunicorn</code> server as the entry point.</li> </ul>"},{"location":"automations/#docker-image-publishing","title":"Docker image publishing","text":"<p>Workflow: <code>.github/workflows/docker-publish.yml</code></p> <p>This workflow automates the process of building and publishing the service's Docker image to the GitHub Container Registry (ghcr.io).</p> <ul> <li>Trigger: This workflow runs automatically whenever a new Git tag matching the pattern <code>v*</code> (e.g., <code>v1.0.0</code>, <code>v1.1.0</code>) is pushed to the repository.</li> <li>Process:<ol> <li>Logs into the GitHub Container Registry using a temporary <code>GITHUB_TOKEN</code>.</li> <li>Uses the <code>docker/metadata-action</code> to automatically generate appropriate Docker tags based on the Git tag.</li> <li>Builds the Docker image using the <code>Dockerfile</code>.</li> <li>Pushes the newly built and tagged image to the <code>ghcr.io/datagems-eosc/cross-dataset-discovery-api</code> repository, making it available for deployment.</li> </ol> </li> </ul>"},{"location":"automations/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Workflow: <code>.github/workflows/vulnerability-scan-on-demand.yml</code></p> <p>This workflow provides on-demand security scanning of the project's configuration and Docker images using the Trivy security scanner.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>). It requires an <code>image_tag</code> as input to specify which published Docker image to scan.</li> <li>Process:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository configuration files for potential security misconfigurations.</li> <li>Image Scan: Pulls the specified Docker image from the GitHub Container Registry and scans its operating system packages and Python libraries for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity.</li> <li>Artifacts: The JSON reports from both the configuration scan and the image scan are uploaded as build artifacts for review.</li> </ol> </li> </ul>"},{"location":"automations/#static-code-analysis","title":"Static Code Analysis","text":"<p>Workflow: <code>.github/workflows/code-scan-on-demand.yml</code></p> <p>This workflow performs in-depth static code analysis using GitHub CodeQL to find potential bugs, security vulnerabilities, and quality issues in the codebase.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>It runs two separate analysis jobs: one for the Python source code and one for the GitHub Actions workflow files themselves.</li> <li>For each language, it initializes CodeQL using an extended set of security and quality queries.</li> <li>It performs the analysis and uploads the results directly to the repository's \"Security\" tab under \"Code scanning alerts\".</li> </ol> </li> </ul>"},{"location":"automations/#code-metrics","title":"Code Metrics","text":"<p>Workflow: <code>.github/workflows/code-metrics-on-demand.yml</code></p> <p>This workflow generates a report on the complexity and maintainability of the application's Python code using the Radon library.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>Installs the <code>radon</code> Python package.</li> <li>Runs three types of analysis on the <code>cross_dataset_discovery/api_datagems_cross_dataset_discovery/app</code> directory:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain.</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code, indicating its complexity.</li> <li>Raw Lines of Code (LOC): Provides basic code size metrics.</li> </ul> </li> <li>The complete report is compiled into a text file and uploaded as a build artifact named <code>code-metrics-report</code>.</li> </ol> </li> </ul>"},{"location":"automations/#documentation","title":"Documentation","text":"<p>Workflow: <code>.github/workflows/deploy-docs-on-demand.yml</code></p> <p>The deployment of this documentation site is automated by a dedicated GitHub Action workflow.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process: When triggered with a version number (e.g., <code>1.0.0</code>), the workflow:<ol> <li>Installs <code>mkdocs</code>, <code>mike</code>, and other required tools.</li> <li>Builds the static HTML site from the markdown source files in the <code>docs/</code> directory.</li> <li>Uses the <code>mike</code> tool to commit the built site to the <code>gh-pages</code> branch, organized under the specified version.</li> <li>Updates the <code>latest</code> alias to point to this newly deployed version, ensuring visitors see the most recent documentation by default.</li> </ol> </li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The Cross-Dataset Discovery service is configured using a combination of a local configuration file and environment variables.</p>"},{"location":"configuration/#configuration-files","title":"Configuration files","text":"<p>For local development, the service can be configured using a <code>.env</code> file placed in the root of the repository. The application will automatically load variables from this file on startup.</p> <p>Warning The <code>.env</code> file is intended for local development only and should be added to your <code>.gitignore</code> file to prevent committing secrets to version control.</p>"},{"location":"configuration/#example-env-file","title":"Example <code>.env</code> file","text":"<pre><code># .env\n\n# OIDC Authentication\nOIDC_ISSUER_URL=\"https://datagems-dev.scayle.es/oauth/realms/dev\"\nOIDC_AUDIENCE=\"cross-dataset-discovery-api\"\nGATEWAY_API_URL=\"https://datagems-dev.scayle.es/gw\"\n\n# Database\nDB_CONNECTION_STRING=\"postgresql://user:password@localhost:5432/datagems_db\"\nTABLE_NAME=\"your_table_name\"\n\n# Application &amp; Search Index\nROOT_PATH=\"\"\nINDEX_PATH=\"./search_index\"\n\n# Secrets (for local use only)\nIdpClientSecret=\"your-local-dev-secret\"\n</code></pre>"},{"location":"configuration/#environment-overrides","title":"Environment Overrides","text":"<p>In any environment, especially in production, environment variables will always override values set in the <code>.env</code> file. This is the standard and recommended way to configure the service when deploying it as a container.</p> <p>The following table lists all available configuration variables:</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider for token validation. https://datagems-dev.scayle.es/oauth/realms/dev OIDC_AUDIENCE The audience claim that must be present in the JWT. cross-dataset-discovery-api GATEWAY_API_URL The base URL of the DataGEMS Gateway API, used to fetch user permissions. https://datagems-dev.scayle.es/gw DB_CONNECTION_STRING The full connection string for the PostgreSQL database. postgresql://user:pass@host:port/dbname TABLE_NAME The name of the table to check in the database during health checks. your_table_name IdpClientSecret The client secret for the identity provider. your-secret ROOT_PATH The path prefix for the API if it's running behind a reverse proxy. /cdd INDEX_PATH The path inside the container to the directory containing the search index files. /app/search_index"},{"location":"configuration/#secrets","title":"Secrets","text":"<p>Certain configuration variables contain sensitive information and must be handled securely.</p>"},{"location":"configuration/#identified-secrets","title":"Identified Secrets","text":"<ul> <li>DB_CONNECTION_STRING: Contains the database username and password.</li> <li>IdpClientSecret: The client secret used for communication with the identity provider.</li> </ul>"},{"location":"datastore/","title":"Datastores","text":"<p>The Cross-Dataset Discovery service relies on a search index for fast retrieval.</p>"},{"location":"datastore/#search-index","title":"Search Index","text":"<p>The core of the retrieval functionality is powered by pre-built search indexes. The API is configured to use an index compatible with Pyserini (which is based on Apache Lucene).</p> <ul> <li>Technology: Apache Lucene</li> <li>Purpose: Provides fast, full-text search capabilities for the <code>/search/</code> endpoint.</li> <li>Location: The index files are stored on the local filesystem. The path to the index directory must be provided to the running container via a volume mount.</li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>The Cross-Dataset Discovery service is designed for containerized deployment using Docker. This guide provides instructions for building the image, configuring the service, and understanding its dependencies.</p>"},{"location":"deployment/#docker","title":"Docker","text":"<p>The primary method for deploying the service is via a Docker container. The repository includes a <code>Dockerfile</code> for the build process.</p>"},{"location":"deployment/#dockerfile-stages","title":"Dockerfile Stages","text":"<ol> <li> <p>Builder Stage:</p> <ul> <li>Starts from a <code>python:3.11-slim</code> base image.</li> <li>Installs OpenJDK, which is a required dependency for the underlying search library (Pyserini/Lucene).</li> <li>Installs all Python dependencies from <code>requirements.txt</code>.</li> </ul> </li> <li> <p>Final Stage:</p> <ul> <li>Starts from a fresh <code>python:3.11-slim</code> image.</li> <li>Creates a non-root user (<code>appuser</code>) for security.</li> <li>Copies the installed OpenJDK and Python packages from the builder stage.</li> <li>Copies the application source code into the container.</li> <li>Sets <code>appuser</code> as the active user.</li> <li>Exposes port <code>8000</code>.</li> <li>Includes a <code>HEALTHCHECK</code> instruction that queries the <code>/health</code> endpoint to monitor container health.</li> <li>The default command (<code>CMD</code>) starts the application using <code>gunicorn</code> with a <code>uvicorn</code> worker, making it production-ready.</li> </ul> </li> </ol>"},{"location":"deployment/#build-and-run","title":"Build and Run","text":"<p>To build and run the container, use the standard Docker commands:</p> <pre><code># 1. Build the Docker image\ndocker build -t cross-dataset-discovery .\n\n# 2. Run the container\n# Note: You must provide a volume mount for the search index and all required environment variables.\ndocker run -p 8000:8000 \\\n  -v /path/to/your/search_index:/app/search_index \\\n  -e DB_CONNECTION_STRING=\"your-db-connection-string\" \\\n  -e OIDC_ISSUER_URL=\"your-oidc-issuer-url\" \\\n  -e INDEX_PATH=\"/app/search_index\" \\\n  --name cdd-service \\\n  cross-dataset-discovery\n</code></pre>"},{"location":"deployment/#configuration","title":"Configuration","text":"<p>The service is configured entirely through environment variables. This allows for flexible deployment across different environments without changing the container image.</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider for token validation. https://datagems-dev.scayle.es/oauth/realms/dev OIDC_AUDIENCE The audience claim that must be present in the JWT. cross-dataset-discovery-api GATEWAY_API_URL The base URL of the DataGEMS Gateway API, used to fetch user permissions. https://datagems-dev.scayle.es/gw DB_CONNECTION_STRING The full connection string for the PostgreSQL database (taken as secret from Vault). postgresql://user:pass@host:port/dbname TABLE_NAME The name of the table to check in the database during health checks. your_table_name IdpClientSecret The client secret for the identity provider (taken as secret from Vault). your-secret ROOT_PATH The path prefix for the API if it's running behind a reverse proxy. /cdd INDEX_PATH The path inside the container to the directory containing the search index files. /app/search_index"},{"location":"deployment/#dependencies","title":"Dependencies","text":"<p>The service requires several external systems and resources to be available at runtime to function correctly.</p>"},{"location":"deployment/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>Search Index: - Description: A pre-built Pyserini/Lucene index is required for the core search functionality. The service does not create this index; it only reads from it. - Requirement: The index directory must be mounted into the container at the path specified by the INDEX_PATH environment variable.</p> <p>OIDC Provider: - Description: An OpenID Connect compliant identity provider (e.g., Keycloak) is necessary for authenticating users by validating their JWTs. - Requirement: The OIDC_ISSUER_URL and OIDC_AUDIENCE must be correctly configured to point to the identity provider.</p> <p>DataGEMS Gateway: - Description: The service communicates with the DataGEMS Gateway API to fetch dataset-level permissions for users. This is needed for enforcing data access policies. - Requirement: The GATEWAY_API_URL must be configured, and the service must have network access to it.</p>"},{"location":"deployment/#build-time-dependencies","title":"Build-time Dependencies","text":"<p>OpenJDK: - Description: The Java Development Kit is required by the Pyserini library, which is a Python wrapper around the Java-based Lucene search engine. - Requirement: It is automatically installed during the Docker build process as defined in the Dockerfile.</p>"},{"location":"error-codes/","title":"Status &amp; Error Codes","text":"<p>The API uses standard HTTP status codes to indicate the success or failure of a request.</p> Status Code Meaning Description <code>200 OK</code> Success The request was successful. <code>400 Bad Request</code> Validation Error The request body is malformed or contains invalid data (e.g., <code>k</code> is out of range, <code>dataset_ids</code> is an empty list). The response body will contain details about the validation error. <code>401 Unauthorized</code> Authentication Error The request lacks a valid JWT, the token is expired, or its signature is invalid. <code>403 Forbidden</code> Authorization Error The user is authenticated but does not have the required role (<code>user</code> or <code>dg_user</code>) to perform the action. <code>424 Failed Dependency</code> Downstream Service Error The API could not communicate with a required dependency, such as the OIDC provider or the database. The response body will contain details about the source of the failure. <code>500 Internal Server Error</code> Unexpected Error An unexpected error occurred on the server while processing the request. <code>503 Service Unavailable</code> Service Not Ready A core component, like the searcher, failed to initialize at startup. This is typically seen during health checks."},{"location":"faq/","title":"FAQ & Known Issues","text":"<p>TODO</p>"},{"location":"license/","title":"License","text":""},{"location":"logging/","title":"Logging &amp; Accounting","text":"<p>The service uses <code>structlog</code> for structured, JSON-formatted logging.</p>"},{"location":"logging/#log-structure","title":"Log Structure","text":"<p>All log entries are formatted as JSON objects with a consistent structure, including:</p> <ul> <li><code>@t</code>: ISO 8601 timestamp.</li> <li><code>@mt</code>: The log message (event).</li> <li><code>@l</code>: The log level (e.g., \"Info\", \"Warning\", \"Error\").</li> <li><code>DGCorrelationId</code>: A unique ID that ties together all log entries for a single HTTP request.</li> <li><code>SourceContext</code>: The name of the logger that produced the message.</li> <li><code>Application</code>: The name of the application (<code>cross-dataset-discovery-api</code>).</li> </ul> <p>Additional key-value pairs are added to provide context for specific events.</p>"},{"location":"logging/#correlation-id","title":"Correlation ID","text":"<p>Every HTTP request is assigned a correlation ID. - If the incoming request includes an <code>x-tracking-correlation</code> header, its value is used. - Otherwise, a new UUID is generated.</p> <p>This ID is included in every log message generated during the processing of that request and is also returned in the <code>x-tracking-correlation</code> response header. This allows for easy tracing of a request's entire lifecycle through the system and across different services.</p>"},{"location":"logging/#request-logging","title":"Request Logging","text":"<p>A middleware automatically logs the end of every HTTP request (except for health checks), capturing:</p> <ul> <li><code>RequestMethod</code> (e.g., GET, POST)</li> <li><code>RequestPath</code></li> <li><code>StatusCode</code></li> <li><code>ProcessTimeMS</code></li> <li><code>ResponseSize</code></li> </ul>"},{"location":"maintenance/","title":"Maintenance","text":"<p>TODO</p>"},{"location":"maintenance/#re-indexing","title":"Re-indexing","text":"<p>A key maintenance task for this service is updating the search index.</p>"},{"location":"onboarding/","title":"Developer Onboarding Guide","text":"<p>Welcome to the developer guide for the <code>nlp_retrieval</code> framework.</p> <p>This document provides a comprehensive overview of the framework's architecture and a step-by-step guide for extending it with new components (like loaders, retrievers, or rerankers) and benchmarking their performance.</p>"},{"location":"onboarding/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Framework Overview</li> <li>Core Principles</li> <li>Directory Structure</li> <li>Core Data Models</li> <li>System Architecture &amp; Workflows</li> <li>Core Components &amp; Interfaces</li> <li>Indexing Workflow</li> <li>Search Workflow</li> <li>Part 1: Adding a New Component</li> <li>General Steps</li> <li>Component-Specific Guides</li> <li>Assembling a Pipeline</li> <li>Part 2: Evaluating Your New Component</li> <li>Key Evaluation Classes</li> <li>The Matching Logic</li> <li>How to Run a Benchmark</li> <li>Interpreting the Output</li> </ol>"},{"location":"onboarding/#1-framework-overview","title":"1. Framework Overview","text":"<p>The <code>nlp_retrieval</code> module provides a modular, configurable, and extensible framework for building and benchmarking end-to-end text retrieval pipelines. It is designed around a \"composition over inheritance\" model, where a central <code>Searcher</code> orchestrates various independent components to perform indexing and searching tasks.</p>"},{"location":"onboarding/#core-principles","title":"Core Principles","text":"<p>The framework is built on a few key principles:</p> <ul> <li>Modularity: Each component is a self-contained, swappable Python class.</li> <li>Clear Interfaces: Components inherit from an Abstract Base Class (ABC) that defines a strict contract for its methods.</li> <li>Standardized Data Flow: We use Pydantic models (<code>SearchableItem</code>, <code>RetrievalResult</code>) to pass data between components.</li> <li>Batch-First Design: All components are designed to operate on batches (<code>List</code>) of queries or results, enabling efficient processing.</li> </ul>"},{"location":"onboarding/#directory-structure","title":"Directory Structure","text":"<p>All retrieval-related code resides in <code>darelabdb/nlp_retrieval/</code>.</p> <pre><code>nlp_retrieval/\n\u251c\u2500\u2500 core/                      # Pydantic models &amp; ABCs\n\u251c\u2500\u2500 loaders/                   # Reads data from sources\n\u251c\u2500\u2500 user_query_processors/     # Processes raw user queries\n\u251c\u2500\u2500 retrievers/                # Indexes data and retrieves candidates\n\u251c\u2500\u2500 rerankers/                 # Re-scores and ranks candidates\n\u251c\u2500\u2500 evaluation/                # Tools for calculating metrics\n\u251c\u2500\u2500 benchmarking/              # Orchestrator for running benchmarks\n\u2514\u2500\u2500 searcher.py                # The main pipeline orchestrator\n</code></pre>"},{"location":"onboarding/#core-data-models","title":"Core Data Models","text":"<p>Before you start, familiarize yourself with the two fundamental data models defined in <code>darelabdb/nlp_retrieval/core/models.py</code>:</p> <ul> <li><code>SearchableItem</code>: The canonical representation of a single piece of data to be indexed.</li> <li><code>item_id: str</code>: A unique identifier for the item.</li> <li><code>content: str</code>: The main text content used for retrieval.</li> <li><code>metadata: Dict</code>: A dictionary for any associated metadata.</li> <li><code>RetrievalResult</code>: Represents a single item returned by a search.</li> <li><code>item: SearchableItem</code>: The retrieved item.</li> <li><code>score: float</code>: The relevance score assigned by the retriever or reranker.</li> </ul>"},{"location":"onboarding/#2-system-architecture-workflows","title":"2. System Architecture &amp; Workflows","text":"<p>The framework is built upon a set of abstract base classes (ABCs), each defining a specific role in the retrieval pipeline. The <code>Searcher</code> and <code>Benchmarker</code> classes orchestrate the interaction between these components.</p>"},{"location":"onboarding/#core-components-interfaces","title":"Core Components &amp; Interfaces","text":"<ul> <li><code>BaseLoader</code>: Responsible for loading data from a source (e.g., file, database) and converting it into a <code>List[SearchableItem]</code>.</li> <li><code>BaseUserQueryProcessor</code>: Transforms a batch of user queries into a format suitable for retrieval (e.g., decomposition, keyword extraction).</li> <li><code>BaseRetriever</code>: Manages the indexing of <code>SearchableItem</code>s and the retrieval of candidate results.</li> <li><code>BaseReranker</code>: Re-scores a list of candidate <code>RetrievalResult</code>s to improve ranking quality.</li> </ul>"},{"location":"onboarding/#indexing-workflow","title":"Indexing Workflow","text":"<p>The indexing process is initiated by the <code>Searcher.index()</code> method.</p> <ol> <li>Load: An instance of a <code>BaseLoader</code> (e.g., <code>JsonlLoader</code>) is used to load data from a source into a <code>List[SearchableItem]</code>.</li> <li>Index: The <code>Searcher</code> passes this list to the <code>index()</code> method of each configured <code>BaseRetriever</code> (e.g., <code>PyseriniRetriever</code>, <code>FaissRetriever</code>).</li> <li>Store: Each retriever builds its specific index (e.g., a Lucene index, a FAISS vector index) and saves it to a dedicated subdirectory.</li> </ol> <p> Figure 1: The indexing workflow showing data flow from source through loader to multiple retrievers creating their respective indexes.</p>"},{"location":"onboarding/#search-workflow","title":"Search Workflow","text":"<p>The search process is initiated by the <code>Searcher.search()</code> method.</p> <ol> <li>Process Query: The input queries are passed to the configured <code>BaseUserQueryProcessor</code>.</li> <li>Retrieve: The <code>Searcher</code> passes the processed queries to each configured <code>BaseRetriever</code>.</li> <li>Combine (Hybrid Search): The <code>Searcher</code> collects and deduplicates the results from all retrievers into a single pool of candidates.</li> <li>Rerank (Optional): If a <code>BaseReranker</code> is configured, this combined pool is passed to its <code>rerank()</code> method for re-scoring.</li> <li>Return: The final, sorted list of results is returned.</li> </ol> <p> Figure 2: The complete search pipeline from query input through processing, retrieval, combination, and optional reranking.</p>"},{"location":"onboarding/#3-part-1-adding-a-new-component","title":"3. Part 1: Adding a New Component","text":""},{"location":"onboarding/#general-steps","title":"General Steps","text":"<ol> <li>Identify the Component Type: Determine which category your new class falls into: <code>loaders</code>, <code>user_query_processors</code>, <code>retrievers</code>, or <code>rerankers</code>.</li> <li>Create the File: Add a new Python file in the appropriate directory (<code>cross_dataset_discovery/darelabdb/nlp_retrieval/retrievers/my_new_retriever.py</code>).</li> <li>Inherit from the ABC: Your new class must inherit from the correct Abstract Base Class (e.g., <code>BaseRetriever</code>).</li> <li>Implement All Abstract Methods: Your IDE or Python itself will require you to implement all methods marked with <code>@abstractmethod</code> in the parent ABC.</li> <li>Use Core Data Models: Your component must accept and/or return the Pydantic models (<code>SearchableItem</code>, <code>RetrievalResult</code>) as defined in the ABC contract.</li> </ol>"},{"location":"onboarding/#component-specific-guides","title":"Component-Specific Guides","text":""},{"location":"onboarding/#creating-a-new-loader","title":"Creating a New Loader","text":"<ul> <li>Inherit from: <code>BaseLoader</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Accept necessary parameters, like file paths.</li> <li><code>load(self) -&gt; List[SearchableItem]</code>: Read your data source and create a list of <code>SearchableItem</code> objects.</li> </ul>"},{"location":"onboarding/#creating-a-new-user-query-processor","title":"Creating a New User Query Processor","text":"<ul> <li>Inherit from: <code>BaseUserQueryProcessor</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Accept any configuration for your processing logic.</li> <li><code>process(self, nlqs: List[str]) -&gt; List[List[str]]</code>: Receives a list of raw queries and must return a list of lists, where each inner list contains the processed strings (e.g., sub-queries, keywords) for an input query.</li> </ul>"},{"location":"onboarding/#creating-a-new-retriever","title":"Creating a New Retriever","text":"<ul> <li>Inherit from: <code>BaseRetriever</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Initialize your retrieval model, parameters, etc.</li> <li><code>index(self, items: List[SearchableItem], output_path: str) -&gt; None</code>: Build your search index from the items and save all index files inside the provided <code>output_path</code>.</li> <li><code>retrieve(self, processed_queries_batch: List[List[str]], output_path: str, k: int) -&gt; List[List[RetrievalResult]]</code>: Load the index and process the batch of sub-queries, returning a single, aggregated, and deduplicated list of <code>RetrievalResult</code> objects for each original query.</li> </ul>"},{"location":"onboarding/#creating-a-new-reranker","title":"Creating a New Reranker","text":"<ul> <li>Inherit from: <code>BaseReranker</code></li> <li>Implement:</li> <li><code>__init__(self, ...)</code>: Initialize your reranking model.</li> <li><code>rerank(self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int) -&gt; List[List[RetrievalResult]]</code>: For each query and its candidates, compute new scores, sort the results, and truncate to the top <code>k</code>.</li> </ul>"},{"location":"onboarding/#assembling-a-pipeline","title":"Assembling a Pipeline","text":"<p>Once you have created your custom component, you can use it in a <code>Searcher</code> pipeline.</p> <pre><code>from darelabdb.nlp_retrieval.searcher import Searcher\nfrom darelabdb.nlp_retrieval.loaders.jsonl_loader import JsonlLoader\nfrom darelabdb.nlp_retrieval.retrievers.dense_retriever import FaissRetriever\n# Import your new custom component\nfrom darelabdb.nlp_retrieval.rerankers.my_reranker import MyReranker\n\n# 1. Initialize the components\nmy_loader = JsonlLoader(file_path=\"path/to/data.jsonl\", content_field=\"text\")\nmy_retriever = FaissRetriever(model_name_or_path=\"BAAI/bge-m3\")\nmy_custom_reranker = MyReranker(model_name=\"some-model\") # Your new reranker\n\n# 2. Assemble the Searcher\nsearcher = Searcher(\n    retrievers=[my_retriever],\n    reranker=my_custom_reranker\n)\n\n# 3. Run the pipeline\nINDEX_DIR = \"./my_index\"\nsearcher.index(loader=my_loader, output_path=INDEX_DIR)\n\n# 4. Search\nqueries = [\"what is the capital of france?\", \"what is the best gpu for gaming?\"]\nresults = searcher.search(nlqs=queries, output_path=INDEX_DIR, k=5)\n\nprint(results)\n</code></pre>"},{"location":"onboarding/#4-part-2-evaluating-your-new-component","title":"4. Part 2: Evaluating Your New Component","text":"<p>After creating a new component, the next step is to measure its performance. The framework provides tools to run automated benchmarks and calculate standard information retrieval metrics.</p> <p> Figure 3: The benchmarking pipeline showing the complete flow from configuration through evaluation and logging.</p>"},{"location":"onboarding/#key-evaluation-classes","title":"Key Evaluation Classes","text":"<ul> <li><code>RetrievalEvaluator</code>: The core engine for calculating metrics like Precision, Recall, and F1-Score.</li> <li><code>Benchmarker</code>: The high-level orchestrator that runs different Searcher configurations, calls the RetrievalEvaluator, and logs results to the console and Weights &amp; Biases.</li> </ul>"},{"location":"onboarding/#the-matching-logic","title":"The Matching Logic","text":"<p>This is the most important concept in our evaluation system. A predicted item is considered a \"correct hit\" if its metadata is a superset of a gold standard item's metadata.</p> <p>Example:</p> <ul> <li>Gold Standard Metadata: <code>{'page_title': 'Mashable'}</code></li> <li>Prediction Metadata: <code>{'page_title': 'Mashable', 'source': 'some_sentence'}</code> \u2192 MATCH</li> </ul> <p>The evaluator also automatically handles granularity. It inspects the metadata keys in your gold standard to determine the level of uniqueness (e.g., by <code>page_title</code>, or by <code>page_title</code> and <code>source</code>). It then deduplicates predictions at that level before calculating metrics.</p>"},{"location":"onboarding/#how-to-run-a-benchmark","title":"How to Run a Benchmark","text":"<p>The <code>run_benchmark.py</code> script provides a template for setting up and running an evaluation.</p>"},{"location":"onboarding/#step-1-prepare-the-corpus-loader","title":"Step 1: Prepare the Corpus Loader","text":"<p>The Benchmarker requires an initialized loader object that returns a <code>List[SearchableItem]</code>.</p> <pre><code>from darelabdb.nlp_retrieval.loaders.jsonl_loader import JsonlLoader\n\nloader = JsonlLoader(file_path=\"path/to/corpus.jsonl\", ...)\n</code></pre>"},{"location":"onboarding/#step-2-prepare-queries-and-gold-standard","title":"Step 2: Prepare Queries and Gold Standard","text":"<p>You must create two lists in memory:</p> <ul> <li><code>queries: List[str]</code>: A list of query strings.</li> <li><code>gold_standard: List[List[RetrievalResult]]</code>: A parallel list where each inner list contains the correct <code>RetrievalResult</code> objects for the corresponding query. For the gold standard, only the metadata field is used for matching.</li> </ul>"},{"location":"onboarding/#step-3-configure-your-searcher-pipelines","title":"Step 3: Configure Your Searcher Pipelines","text":"<p>Create an initialized <code>Searcher</code> instance for each configuration you want to test, including your new component.</p> <pre><code># Configuration 1: Baseline\nbm25_searcher = Searcher(retrievers=[PyseriniRetriever()])\n\n# Configuration 2: Your new component\nmy_new_searcher = Searcher(retrievers=[MyNewRetriever()])\n\n# Create a list of named configurations for the Benchmarker\nsearcher_configs = [\n    (\"BM25_Baseline\", bm25_searcher),\n    (\"My_New_Retriever\", my_new_searcher),\n]\n</code></pre>"},{"location":"onboarding/#step-4-initialize-and-run-the-benchmarker","title":"Step 4: Initialize and Run the Benchmarker","text":"<p>Pass all the prepared objects to the Benchmarker and call <code>.run()</code>.</p> <pre><code>from darelabdb.nlp_retrieval.benchmarking.benchmarker import Benchmarker\nfrom darelabdb.nlp_retrieval.evaluation.evaluator import RetrievalEvaluator\n\nevaluator = RetrievalEvaluator()\nbenchmarker = Benchmarker(\n    searcher_configs=searcher_configs,\n    evaluator=evaluator,\n    loader=loader,\n    queries=queries,\n    gold_standard=gold_standard,\n    k_values=[1, 5, 10],\n    output_path=\"./benchmark_results\",\n    use_wandb=True,\n    wandb_project=\"my-retrieval-project\",\n    wandb_entity=\"my-username\",\n)\n\nbenchmarker.run()\n</code></pre>"},{"location":"onboarding/#interpreting-the-output","title":"Interpreting the Output","text":"<ul> <li>Console Output: The script will print progress, search speed (QPS), and a final summary table comparing all runs.</li> <li>Weights &amp; Biases (W&amp;B) Dashboard: If <code>use_wandb=True</code>, the Benchmarker logs detailed configurations and metrics for each run, along with an aggregated summary table, making it easy to compare pipeline performance.</li> </ul>"},{"location":"openapi/","title":"OpenAPI Specification","text":"<p>This service provides an OpenAPI 3.0 specification for its API, which can be used to generate clients or for integration with other tools.</p>"},{"location":"openapi/#specification-file","title":"Specification File","text":"<p>The OpenAPI specification is located in the documentation source directory.</p> <p>View openapi.json</p>"},{"location":"openapi/#postman-collection","title":"Postman Collection","text":"<p>Download Postman Collection</p> <p>After importing the collection, you will need to configure the <code>BASE_URL</code> variable and add your JWT access token to the \"Authorization\" tab of the \"Perform</p>"},{"location":"openapi/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>The following is an interactive documentation panel generated directly from the <code>openapi.json</code> specification.</p>"},{"location":"qa/","title":"Quality Assurance","text":"<p>Quality assurance (QA) combines automated static analysis, vulnerability scanning, and API-level integration testing to ensure the service functions as expected.</p>"},{"location":"qa/#code-analysis","title":"Code Analysis","text":"<p>We use GitHub CodeQL for static analysis of the source code. This process helps identify potential bugs, security vulnerabilities, and quality issues before they impact production.</p> <ul> <li>Technology: GitHub CodeQL</li> <li>Targets: The analysis runs on both the Python source code and the GitHub Actions workflow files themselves.</li> <li>Checks: It uses a set of queries to find common vulnerabilities (e.g., injection flaws, insecure data handling), bugs, and code quality anti-patterns.</li> <li>Execution: The analysis is performed by the on-demand <code>code-scan-on-demand.yml</code> workflow. Results and alerts are available directly in the repository's \"Security\" tab.</li> </ul>"},{"location":"qa/#code-metrics","title":"Code Metrics","text":"<p>To ensure the long-term maintainability and readability of the code, we use the Radon library to generate code metrics.</p> <ul> <li>Technology: Radon</li> <li>Metrics:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain (higher is better).</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code. A lower score indicates simpler, less complex code.</li> <li>Lines of Code (LOC): Provides raw metrics on the size of the codebase.</li> </ul> </li> <li>Execution: The metrics are generated by the on-demand <code>code-metrics-on-demand.yml</code> workflow, which produces a downloadable <code>code-metrics-report.txt</code> artifact.</li> </ul>"},{"location":"qa/#vulnerability-checks","title":"Vulnerability Checks","text":"<p>We use Trivy to scan for known vulnerabilities in our dependencies and container image, ensuring the service is secure from common threats.</p> <ul> <li>Technology: Trivy</li> <li>Scans:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository files for security misconfigurations.</li> <li>Image Scan: Scans the final Docker image for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity in its OS packages and Python libraries.</li> </ol> </li> <li>Execution: The scan is performed by the on-demand <code>vulnerability-scan-on-demand.yml</code> workflow. It requires an image tag as input and uploads detailed JSON reports as build artifacts.</li> </ul>"},{"location":"qa/#testing","title":"Testing","text":"<p>The service's functionality is validated through API-level integration tests using Postman and its command-line runner, Newman.</p> <ul> <li> <p>Test Suite: The test cases are defined in the Postman collection located at <code>tests/cross-dataset-discovery-api-tests.postman_collection.json</code>.</p> </li> <li> <p>Test Cases: The suite includes the following tests:</p> <ul> <li>Health Check: Verifies that the <code>/health</code> endpoint is available and returns a <code>200 OK</code> status, indicating the service and its dependencies are healthy.</li> <li>Get User Access Token: Simulates a user login against the OIDC provider to acquire a valid JWT, which is required for all protected endpoints.</li> <li>Perform Search - Valid Request: Executes a valid search against the <code>/search/</code> endpoint and asserts that the response is successful (<code>200 OK</code>) and has the correct structure.</li> <li>Perform Search - Bad Request: Sends a request with an invalid payload (an empty <code>dataset_ids</code> list) and asserts that the API correctly rejects it with a <code>400 Bad Request</code> status.</li> </ul> </li> </ul>"},{"location":"qa/#how-to-run-tests","title":"How to Run Tests","text":"<p>The API tests are designed to be run automatically via a GitHub Actions workflow.</p> <ul> <li>Workflow File: <code>.github/workflows/test-on-demand.yml</code></li> <li>Trigger: The workflow is triggered manually (<code>workflow_dispatch</code>) from the Actions tab in the GitHub repository.</li> <li>Process:<ol> <li>Navigate to the \"Actions\" tab and select the \"Test Scenario (On-Demand)\" workflow.</li> <li>Click \"Run workflow\". You will be prompted to enter a <code>tag</code> (e.g., <code>main</code> or a specific version like <code>v1.0.0</code>) to test against.</li> <li>The workflow checks out the specified version of the code.</li> <li>It then uses the official <code>postman/newman</code> Docker image to execute the test collection.</li> <li>All necessary environment variables (API base URL, credentials, etc.) are securely injected into the test run from the repository's secrets.</li> </ol> </li> </ul>"},{"location":"qa/#expected-output","title":"Expected Output","text":"<p>A successful test run will produce the following summary table in the GitHub Actions log:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                         \u2502            executed \u2502             failed \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              iterations \u2502                   1 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                requests \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502            test-scripts \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      prerequest-scripts \u2502                   0 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              assertions \u2502                   4 \u2502                  0 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total run duration: 1674ms                                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 total data received: 6.9kB (approx)                                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 average response time: 396ms [min: 147ms, max: 645ms, s.d.: 205ms] \u2502\n</code></pre>"},{"location":"security/","title":"Security","text":""},{"location":"security/#authentication","title":"Authentication","text":"<p>Authentication is handled via the OAuth 2.0 and OpenID Connect (OIDC) protocols. Every request to a protected endpoint must include a valid JSON Web Token (JWT) in the <code>Authorization</code> header as a Bearer token.</p> <p>The service validates incoming JWTs against the OIDC provider's public keys. The token's signature, issuer, and audience are all verified to ensure its authenticity.</p>"},{"location":"security/#authorization","title":"Authorization","text":"<p>Authorization is role-based. The service checks for specific roles within the validated JWT's claims.</p> <ul> <li>Required Roles: To access the <code>/search/</code> endpoint, the user's token must contain at least one of the following roles in its <code>realm_access.roles</code> claim:<ul> <li><code>user</code></li> <li><code>dg_user</code></li> </ul> </li> </ul> <p>If a user attempts to access an endpoint without the required role, the API will respond with a <code>403 Forbidden</code> error.</p>"},{"location":"security/#dataset-level-permissions","title":"Dataset-Level Permissions","text":"<p>In addition to role-based access, the service enforces dataset-level permissions.</p> <p>Before executing a search, the API uses the user's token to call the DataGEMS Gateway's <code>/api/principal/me/context-grants</code> endpoint. This returns a list of all dataset IDs the user is permitted to access.</p> <ul> <li>Search Filtering:<ul> <li>If a user's search request includes <code>dataset_ids</code>, the service intersects this list with the user's authorized datasets. The search is only performed on the datasets present in both lists.</li> <li>All search results, regardless of the input filter, are always filtered against the user's authorized dataset list before being returned. This ensures a user can never see data from a dataset they are not permitted to access.</li> </ul> </li> </ul>"}]}