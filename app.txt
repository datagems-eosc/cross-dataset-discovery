# Combined content of /home/tolis/Desktop/datagems/cross-dataset-discovery/app.zip

================================================================================

### FILE: cross_dataset_discovery/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/searcher.py
# EXT: .py, SIZE: 6257 bytes, SHA256: c88a7bbbb292baa5222030eb346430bae857aed60a927ae166774b58530e6ef6

import os
from typing import Dict, List, Optional

from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.loaders.loader_abc import BaseLoader
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from nlp_retrieval.retrievers.retriever_abc import BaseRetriever
from nlp_retrieval.user_query_processors.passthrough_processor import (
    PassthroughQueryProcessor,
)
from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)


class Searcher:
    """
    The main orchestrator for the retrieval pipeline.

    This class wires together the different components (loader, query processor,
    retrievers, reranker) to perform end-to-end indexing and searching. It supports
    hybrid search by aggregating results from multiple retrievers.
    """

    def __init__(
        self,
        retrievers: List[BaseRetriever],
        query_processor: Optional[BaseUserQueryProcessor] = None,
        reranker: Optional[BaseReranker] = None,
        reranker_multiplier: int = 3,
    ):
        """
        Initializes the search pipeline.

        Args:
            retrievers: A list of one or more initialized retriever instances.
            query_processor: An optional, initialized user query processor instance.
                             If None, a `PassthroughQueryProcessor` is used.
            reranker: An optional, initialized reranker instance.
            reranker_multiplier: The multiplier for the number of candidates to retrieve
                                 before reranking. This allows the reranker to work with a
                                 larger set of candidates.
        """
        if not retrievers:
            raise ValueError("At least one retriever must be provided.")
        self.retrievers = retrievers
        self.query_processor = query_processor or PassthroughQueryProcessor()
        self.reranker = reranker
        self.reranker_multiplier = reranker_multiplier

    def index(self, loader: BaseLoader, output_path: str) -> None:
        """
        Loads data using the provided loader and indexes it with all configured retrievers.

        Each retriever's index will be stored in a subdirectory named after its class.

        Args:
            loader: An initialized data loader instance.
            output_path: The root directory to save all index artifacts.
        """
        items = loader.load()
        if not items:
            print("Warning: No items loaded. Indexing will be skipped.")
            return
        for retriever in self.retrievers:
            # Use the retriever's class name for a unique, descriptive folder
            retriever_name = retriever.__class__.__name__
            retriever_path = os.path.join(output_path, retriever_name)
            os.makedirs(retriever_path, exist_ok=True)
            retriever.index(items, retriever_path)

    def search(
        self,
        nlqs: List[str],
        output_path: str,
        k: int,
        **kwargs,
    ) -> List[List[RetrievalResult]]:
        """
        Executes the full search pipeline for a batch of natural language queries.

        The pipeline is as follows:
        1. Process queries using the `UserQueryProcessor`.
        2. Retrieve candidates from all `Retrievers`.
        3. Combine and deduplicate the results from different retrievers.
        4. (Optional) Rerank the combined candidates using the `Reranker`.
        5. Return the final top-k results for each query.

        Args:
            nlqs: A list of natural language queries.
            output_path: The root directory where indexes are stored.
            k: The final number of results to return for each query.

        Returns:
            A list of lists of `RetrievalResult` objects, one list per input query.
            The results might be more than `k` if multiple retrievers return different results
        """
        if not nlqs:
            return []

        # 1. Process all NLQs in a batch
        processed_queries_batch = self.query_processor.process(nlqs)

        # 2. Retrieve from all retrievers and combine results
        # This pool will collect unique results for each query.
        # The dictionary structure handles deduplication by item_id automatically.
        hybrid_results_pool: List[Dict[str, RetrievalResult]] = [{} for _ in nlqs]

        for retriever in self.retrievers:
            retriever_name = retriever.__class__.__name__
            retriever_path = os.path.join(output_path, retriever_name)

            # The retriever handles its own sub-query logic and returns one list per NLQ
            # We retrieve more candidates to create a larger pool for the reranker.
            retriever_results_batch = retriever.retrieve(
                processed_queries_batch,
                retriever_path,
                k=k * self.reranker_multiplier,
                **kwargs,
            )

            # Combine results from this retriever into the hybrid pool
            for i, result_list in enumerate(retriever_results_batch):
                for result in result_list:
                    item_id = result.item.item_id
                    # Add the result if this item hasn't been seen yet for this query
                    if item_id not in hybrid_results_pool[i]:
                        hybrid_results_pool[i][item_id] = result

        # Convert the dictionary of unique results back into a list for each query
        candidate_results_batch: List[List[RetrievalResult]] = [
            list(pool.values()) for pool in hybrid_results_pool
        ]

        # 3. Rerank if a reranker is provided
        if self.reranker:
            # The reranker will handle sorting and truncating to k.
            final_results_batch = self.reranker.rerank(nlqs, candidate_results_batch, k)
        else:
            # If no reranker, sort the combined results by their original scores and truncate.
            final_results_batch = []
            for result_list in candidate_results_batch:
                sorted_results = sorted(
                    result_list, key=lambda r: r.score, reverse=True
                )
                final_results_batch.append(sorted_results[:k])
        return final_results_batch

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/benchmarking/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/benchmarking/benchmarker.py
# EXT: .py, SIZE: 13424 bytes, SHA256: 9298a35ca1a6bb1b4a0cde75e3f48425111415fc2f99953cfa3daf97ff6b120a

import os
import time
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd
from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.evaluation.evaluator import RetrievalEvaluator
from nlp_retrieval.loaders.loader_abc import BaseLoader
from nlp_retrieval.searcher import Searcher

import wandb


class Benchmarker:
    """
    A class to run, evaluate, and log benchmarks for multiple
    retrieval pipeline configurations.
    """

    def __init__(
        self,
        searcher_configs: List[Tuple[str, Searcher]],
        evaluator: RetrievalEvaluator,
        loader: BaseLoader,
        queries: List[str],
        gold_standard: List[List[RetrievalResult]],
        k_values: List[int],
        output_path: str,
        use_wandb: bool = True,
        wandb_project: Optional[str] = None,
        wandb_entity: Optional[str] = None,
        retrieval_depth_multiplier: int = 3,
    ):
        """
        Initializes the Benchmarker.

        Args:
            searcher_configs: A list of tuples, where each tuple contains a unique
                              run name and an initialized `Searcher` instance.
                              Run names are just used to log with a specific name.
            evaluator: An initialized `RetrievalEvaluator` instance.
            loader: An initialized `BaseLoader` for loading data for indexing.
            queries: A list of natural language queries to run.
            gold_standard: A parallel list of lists of gold standard `RetrievalResult`s.
            k_values: A list of `k` values (e.g., [1, 5, 10]) to evaluate metrics at.
            output_path: The root directory to store all generated indexes.
            use_wandb: If True, logs results to Weights & Biases.
            wandb_project: The W&B project name. Required if `use_wandb` is True.
            wandb_entity: The W&B entity (username or team). Required if `use_wandb` is True.
            retrieval_depth_multiplier: Multiplier for the maximum retrieval depth.
                                        Default is 3, meaning if max_k is 10, it retrieves up to 30 items
                                        ensuring enough results for evaluation after deduplication.
        """
        self.searcher_configs = searcher_configs
        self.evaluator = evaluator
        self.loader = loader
        self.queries = queries
        self.gold_standard = gold_standard
        self.k_values = sorted(
            k_values, reverse=True
        )  # Sort descending to get max_k first
        self.max_k = self.k_values[0] if self.k_values else 10
        self.output_path = output_path

        self.use_wandb = use_wandb
        if use_wandb and (not wandb_project or not wandb_entity):
            raise ValueError(
                "`wandb_project` and `wandb_entity` must be provided when `use_wandb` is True."
            )
        self.wandb_project = wandb_project
        self.wandb_entity = wandb_entity
        self.retrieval_depth_multiplier = retrieval_depth_multiplier
        os.makedirs(self.output_path, exist_ok=True)
        self.summary_data = []

    def _get_config_dict(self, searcher: Searcher) -> Dict[str, Any]:
        """Creates a JSON-serializable dictionary of the Searcher's configuration."""
        config = {
            "searcher_class": searcher.__class__.__name__,
            "query_processor": {
                "class": searcher.query_processor.__class__.__name__,
                "params": searcher.query_processor.__dict__,
            },
            "retrievers": [
                {
                    "class": retriever.__class__.__name__,
                    "params": retriever.__dict__,
                }
                for retriever in searcher.retrievers
            ],
            "reranker": None,
        }
        if searcher.reranker:
            config["reranker"] = {
                "class": searcher.reranker.__class__.__name__,
                "params": searcher.reranker.__dict__,
            }

        # Clean up non-serializable objects from the params
        def cleanup_dict(d):
            if isinstance(d, dict):
                return {
                    k: cleanup_dict(v)
                    for k, v in d.items()
                    if not callable(v) and not k.startswith("_")
                }
            if isinstance(d, list):
                return [cleanup_dict(i) for i in d]
            if hasattr(d, "__dict__"):
                # Fallback for complex objects, just get their class name
                return d.__class__.__name__
            return d

        return cleanup_dict(config)

    def run(self):
        """
        Executes the full benchmarking pipeline for all configured searchers.
        """
        for run_name, searcher in self.searcher_configs:
            print(f"\n{'='*20} Running Benchmark for: {run_name} {'='*20}")

            # --- 1. Indexing ---
            run_output_path = os.path.join(self.output_path, run_name)
            os.makedirs(run_output_path, exist_ok=True)
            searcher.index(self.loader, run_output_path)

            retrieval_depth = self.max_k * self.retrieval_depth_multiplier
            print(
                f"Retrieving up to {retrieval_depth} items to evaluate at k={self.k_values}..."
            )

            # --- 2. Searching ---
            start_time = time.time()
            # Retrieve once with the largest k value for efficiency
            predicted_results = searcher.search(
                self.queries, run_output_path, retrieval_depth
            )
            end_time = time.time()

            search_duration = end_time - start_time
            qps = (
                len(self.queries) / search_duration
                if search_duration > 0
                else float("inf")
            )
            print(f"Search completed in {search_duration:.2f}s ({qps:.2f} QPS)")

            # --- 3. W&B Initialization ---
            if self.use_wandb:
                wandb.init(
                    project=self.wandb_project,
                    entity=self.wandb_entity,
                    name=run_name,
                    config=self._get_config_dict(searcher),
                    reinit=True,
                )
                wandb.summary["queries_per_second"] = qps
                wandb.summary["total_search_time_s"] = search_duration

            print(
                f"--- Evaluating on FULL retrieved set (up to k={retrieval_depth}) ---"
            )
            full_summary = self.evaluator.evaluate(
                predicted_results, self.gold_standard
            )
            failures_path = os.path.join(run_output_path, "evaluation_failures.json")
            full_summary.save_failures_to_json(failures_path)
            if os.path.exists(failures_path):
                print(f"Saved failure analysis to {failures_path}")

            print(f"  Full Precision: {full_summary.overall_precision:.4f}")
            print(f"  Full Recall: {full_summary.overall_recall:.4f}")
            print(f"  Full F1 Score: {full_summary.overall_f1_score:.4f}")
            print(f"  Full Perfect Recall Rate: {full_summary.perfect_recall_rate:.4f}")
            print(
                f"  Full Recall (Non-Numerical): {full_summary.overall_recall_non_numerical:.4f}"
            )
            print(
                f"  Full Perfect Recall Rate (Non-Numerical): {full_summary.perfect_recall_rate_non_numerical:.4f}"
            )

            if self.use_wandb:
                wandb.summary["FullResults_Precision"] = full_summary.overall_precision
                wandb.summary["FullResults_Recall"] = full_summary.overall_recall
                wandb.summary["FullResults_F1_Score"] = full_summary.overall_f1_score
                wandb.summary["FullResults_Perfect_Recall_Rate"] = (
                    full_summary.perfect_recall_rate
                )
                wandb.summary["FullResults_Recall_Non_Numerical"] = (
                    full_summary.overall_recall_non_numerical
                )
                wandb.summary["FullResults_Perfect_Recall_Rate_Non_Numerical"] = (
                    full_summary.perfect_recall_rate_non_numerical
                )

            # --- 4. Evaluation at different k values ---
            metrics_table_data = []
            for k in sorted(self.k_values):
                print(f"--- Evaluating @k={k} ---")
                correctly_prepared_predictions_at_k = []
                for i, pred_list_for_query in enumerate(predicted_results):
                    # Get the corresponding gold standard to determine granularity
                    gold_list_for_query = self.gold_standard[i]

                    # Step A: Determine the granularity for this specific query.
                    granularity_fields = (
                        self.evaluator._get_granularity_fields_for_query(
                            gold_list_for_query
                        )
                    )
                    # Step B: Deduplicate the *entire* list of predictions for this query.
                    deduplicated_list = self.evaluator.deduplicate_by_granularity(
                        pred_list_for_query, granularity_fields
                    )
                    # Step C: slice the *deduplicated* list to k.
                    sliced_list = deduplicated_list[:k]
                    correctly_prepared_predictions_at_k.append(sliced_list)
                summary = self.evaluator.evaluate(
                    correctly_prepared_predictions_at_k, self.gold_standard
                )

                failures_path_k = os.path.join(
                    run_output_path, f"evaluation_failures_at_k_{k}.json"
                )
                summary.save_failures_to_json(failures_path_k)

                metrics_at_k = {
                    "@k": k,
                    "Precision": summary.overall_precision,
                    "Recall": summary.overall_recall,
                    "F1 Score": summary.overall_f1_score,
                    "Perfect Recall Rate": summary.perfect_recall_rate,
                    "Recall (Non-Numerical)": summary.overall_recall_non_numerical,
                    "Perfect Recall Rate (Non-Numerical)": summary.perfect_recall_rate_non_numerical,
                }
                metrics_table_data.append(list(metrics_at_k.values()))

                if self.use_wandb:
                    wandb.summary[f"Precision@{k}"] = summary.overall_precision
                    wandb.summary[f"Recall@{k}"] = summary.overall_recall
                    wandb.summary[f"F1_Score@{k}"] = summary.overall_f1_score
                    wandb.summary[f"Perfect_Recall_Rate@{k}"] = (
                        summary.perfect_recall_rate
                    )
                    wandb.summary[f"Recall_Non_Numerical@{k}"] = (
                        summary.overall_recall_non_numerical
                    )
                    wandb.summary[f"Perfect_Recall_Rate_Non_Numerical@{k}"] = (
                        summary.perfect_recall_rate_non_numerical
                    )

            if self.use_wandb:
                columns = [
                    "@k",
                    "Precision",
                    "Recall",
                    "F1 Score",
                    "Perfect Recall Rate",
                    "Recall (Non-Numerical)",
                    "Perfect Recall Rate (Non-Numerical)",
                ]
                metrics_table = wandb.Table(columns=columns, data=metrics_table_data)
                wandb.log({"performance_metrics_at_k": metrics_table})

                self.summary_data.append(
                    [
                        run_name,
                        qps,
                        metrics_table_data[-1][1],  # Precision at max_k
                        metrics_table_data[-1][2],  # Recall at max_k
                        metrics_table_data[-1][3],  # F1 at max_k
                        metrics_table_data[-1][4],  # PRR at max_k
                        metrics_table_data[-1][5],  # Recall (Non-Num) at max_k
                        metrics_table_data[-1][6],  # PRR (Non-Num) at max_k
                    ]
                )

                wandb.finish()

        # --- 5. Final Aggregated Report ---
        if self.use_wandb and self.summary_data:
            print(f"\n{'='*20} Logging Aggregated Benchmark Summary {'='*20}")
            wandb.init(
                project=self.wandb_project,
                entity=self.wandb_entity,
                name="Benchmark_Summary_Report",
                reinit=True,
            )

            summary_columns = [
                "Run Name",
                "QPS",
                f"Precision@{self.max_k}",
                f"Recall@{self.max_k}",
                f"F1_Score@{self.max_k}",
                f"Perfect_Recall_Rate@{self.max_k}",
                f"Recall_Non_Numerical@{self.max_k}",
                f"Perfect_Recall_Rate_Non_Numerical@{self.max_k}",
            ]
            summary_table = wandb.Table(columns=summary_columns, data=self.summary_data)
            wandb.log({"benchmark_summary_table": summary_table})

            summary_df = pd.DataFrame(self.summary_data, columns=summary_columns)
            print("\nBenchmark Summary:")
            print(summary_df.to_markdown(index=False))

            wandb.finish()

        print("\nBenchmarking finished.")

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/bm25_retriever.py
# EXT: .py, SIZE: 7405 bytes, SHA256: 1bc3de3d269a45c3e0d139ccc8fcf031e1cf407697972b843ed97439f4f45975

import json
import os
import subprocess
import tempfile
from typing import Dict, List, Optional

from nlp_retrieval.core.models import RetrievalResult, SearchableItem
from nlp_retrieval.retrievers.retriever_abc import BaseRetriever
from pyserini.analysis import Analyzer, get_lucene_analyzer
from pyserini.search.lucene import LuceneSearcher, querybuilder
from tqdm import tqdm


class PyseriniRetriever(BaseRetriever):
    """
    A sparse retriever implementation using the BM25 algorithm via Pyserini.
    """

    def __init__(self, k1: float = 0.9, b: float = 0.4, enable_tqdm: bool = True):
        """
        Initializes the Pyserini-based BM25 retriever.

        Args:
            k1: The BM25 k1 parameter. Controls term frequency saturation.
            b: The BM25 b parameter. Controls document length normalization.
            enable_tqdm: If True, displays tqdm progress bars during operations.
        """
        self.k1 = k1
        self.b = b
        self.enable_tqdm = enable_tqdm
        self._searcher_cache: Dict[str, LuceneSearcher] = {}

    def index(self, items: List[SearchableItem], output_path: str) -> None:
        """
        Builds a Pyserini/Lucene index from a list of SearchableItem objects.
        """
        if os.path.exists(output_path) and os.listdir(output_path):
            print(f"Index already exists in '{output_path}'. Skipping indexing.")
            return

        # Pyserini's JsonCollection requires a directory of .jsonl files.
        # We create a temporary directory to stage the data.
        with tempfile.TemporaryDirectory() as temp_dir:
            prepared_jsonl_path = os.path.join(temp_dir, "corpus.jsonl")
            with open(prepared_jsonl_path, "w", encoding="utf-8") as outfile:
                for item in items:
                    pyserini_doc = {
                        "id": item.item_id,
                        "contents": item.content,
                        **item.metadata,
                    }
                    outfile.write(json.dumps(pyserini_doc, ensure_ascii=False) + "\n")
            num_threads = os.cpu_count() or 1
            cmd = [
                "python",
                "-m",
                "pyserini.index.lucene",
                "--collection",
                "JsonCollection",
                "--input",
                temp_dir,
                "--index",
                output_path,
                "--generator",
                "DefaultLuceneDocumentGenerator",
                "--threads",
                str(num_threads),
                "--storePositions",
                "--storeDocvectors",
                "--storeRaw",
            ]
            subprocess.run(
                cmd, check=True, capture_output=True, text=True, encoding="utf-8"
            )  # nosec B603

    def retrieve(
        self,
        processed_queries_batch: List[List[str]],
        output_path: str,
        k: int,
        **kwargs,
    ) -> List[List[RetrievalResult]]:
        """
        Retrieves items using Pyserini. If filters are provided, it builds complex
        queries; otherwise, it uses the default high-performance batch search.
        """
        analyzer = Analyzer(get_lucene_analyzer())
        if not os.path.exists(output_path):
            print(f"BM25 index not found in {output_path}. Skipping.")
            return [[] for _ in processed_queries_batch]

        filters: Optional[Dict] = kwargs.get("filters")
        if output_path not in self._searcher_cache:
            searcher = LuceneSearcher(output_path)
            searcher.set_bm25(self.k1, self.b)
            self._searcher_cache[output_path] = searcher
        searcher = self._searcher_cache[output_path]
        flat_queries = []
        query_to_original_idx_map = []
        for i, sub_queries in enumerate(processed_queries_batch):
            for sub_query in sub_queries:
                if sub_query and isinstance(sub_query, str):
                    flat_queries.append(sub_query)
                    query_to_original_idx_map.append(i)

        if not flat_queries:
            return [[] for _ in processed_queries_batch]

        q_ids = [f"q{i}" for i in range(len(flat_queries))]
        if filters and filters.get("source"):
            query_objects = []
            for query_text in flat_queries:
                analyzed_terms = analyzer.analyze(query_text)

                should = querybuilder.JBooleanClauseOccur["should"].value
                must = querybuilder.JBooleanClauseOccur["must"].value
                filter_clause = querybuilder.JBooleanClauseOccur["filter"].value

                text_query_builder = querybuilder.get_boolean_query_builder()
                for term in analyzed_terms:
                    term_query = querybuilder.get_term_query(term, field="contents")
                    text_query_builder.add(term_query, should)
                text_query = text_query_builder.build()

                dataset_ids = filters["source"]
                filter_builder = querybuilder.get_boolean_query_builder()
                for did in dataset_ids:
                    term_query = querybuilder.get_term_query(did, field="source")
                    filter_builder.add(term_query, should)

                combined_builder = querybuilder.get_boolean_query_builder()
                combined_builder.add(text_query, must)
                combined_builder.add(filter_builder.build(), filter_clause)
                query_objects.append(combined_builder.build())

            batch_hits = searcher.batch_search(
                queries=query_objects, qids=q_ids, k=k, threads=os.cpu_count() or 1
            )
        else:
            batch_hits = searcher.batch_search(
                queries=flat_queries, qids=q_ids, k=k, threads=os.cpu_count() or 1
            )

        aggregated_results: List[Dict[str, RetrievalResult]] = [
            {} for _ in processed_queries_batch
        ]

        pbar_desc = "Processing BM25 Results"
        for i in tqdm(
            range(len(flat_queries)), desc=pbar_desc, disable=not self.enable_tqdm
        ):
            original_nlq_idx = query_to_original_idx_map[i]
            qid = q_ids[i]
            hits = batch_hits.get(qid, [])

            for hit in hits:
                raw_doc = json.loads(hit.lucene_document.get("raw"))
                item_id = raw_doc.get("id")
                if not item_id:
                    continue

                item = SearchableItem(
                    item_id=item_id,
                    content=raw_doc.get("contents", ""),
                    metadata={
                        k: v for k, v in raw_doc.items() if k not in ["id", "contents"]
                    },
                )
                result = RetrievalResult(item=item, score=hit.score)

                # If item is new or has a better score, add/update it
                if (
                    item_id not in aggregated_results[original_nlq_idx]
                    or result.score
                    > aggregated_results[original_nlq_idx][item_id].score
                ):
                    aggregated_results[original_nlq_idx][item_id] = result

        # Convert aggregated dicts to sorted lists
        final_batches = [
            sorted(res_dict.values(), key=lambda r: r.score, reverse=True)
            for res_dict in aggregated_results
        ]
        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/colbert_retriever.py
# EXT: .py, SIZE: 7859 bytes, SHA256: 3b7574cdfe5aeb09c9c049cd6f0717d64b7d20c19e5213cac2381e3a0bf97dcc

import os
import pickle
from typing import Dict, List

import torch
from nlp_retrieval.core.models import RetrievalResult, SearchableItem
from nlp_retrieval.retrievers.retriever_abc import BaseRetriever
from pylate import indexes as pylate_indexes
from pylate import models as pylate_models
from pylate import retrieve as pylate_retrieve
from tqdm.auto import tqdm


class PylateColbertRetriever(BaseRetriever):
    """
    A retriever for ColBERT models using the `pylate` library with PLAID indexing.
    Optimized for maximum accuracy regardless of latency.
    Note that `pylate` is not installed in the monorepo due to its dependency
    on `sentence-transformers == 4.0.2`. This is an open issue that must be
    addressed when the dependency changes.
    """

    ITEMS_FILENAME = "items.pkl"
    PLAID_INDEX_DIR_NAME = "plaid_index"

    def __init__(
        self,
        model_name_or_path: str = "lightonai/Reason-ModernColBERT",
        plaid_nbits: int = 8,
        plaid_kmeans_niters: int = 256,
        plaid_index_bsize: int = 64,
        plaid_ncells: int = 1,
        encode_batch_size: int = 8,
        enable_tqdm: bool = True,
        use_fp16: bool = False,
        compile_model: bool = False,
        max_length: int = 8192,
    ):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.pylate_model = pylate_models.ColBERT(
            model_name_or_path=model_name_or_path,
            device=self.device,
        )

        if not compile_model:
            if not use_fp16:
                self.pylate_model.float()
        else:
            self.pylate_model = torch.compile(self.pylate_model)

        self.embedding_size = self.pylate_model.get_sentence_embedding_dimension()
        self.use_fp16 = use_fp16
        self.max_length = max_length

        self.plaid_config_params = {
            "embedding_size": self.embedding_size,
            "nbits": plaid_nbits,
            "kmeans_niters": plaid_kmeans_niters,
            "index_bsize": plaid_index_bsize,
            "ncells": plaid_ncells,
        }
        self.encode_batch_size = encode_batch_size
        self.enable_tqdm = enable_tqdm

        self._items_cache: Dict[str, Dict[str, SearchableItem]] = {}
        self._retriever_cache: Dict[str, pylate_retrieve.ColBERT] = {}

    def _get_plaid_index_path(self, output_path: str) -> str:
        return os.path.join(output_path, self.PLAID_INDEX_DIR_NAME)

    def _get_items_path(self, output_path: str) -> str:
        return os.path.join(output_path, self.ITEMS_FILENAME)

    def index(self, items: List[SearchableItem], output_path: str) -> None:
        """Builds a PLAID index and saves the corresponding item data."""
        items_path = self._get_items_path(output_path)
        plaid_index_path = self._get_plaid_index_path(output_path)
        plaid_metadata_file = os.path.join(plaid_index_path, "metadata.json")

        if os.path.exists(items_path) and os.path.exists(plaid_metadata_file):
            print(f"Index already exists in '{output_path}'. Skipping indexing.")
            return

        if not items:
            return

        os.makedirs(output_path, exist_ok=True)

        contents_to_encode = [item.content for item in items]

        self.pylate_model.eval()

        with torch.no_grad():
            doc_embeddings = self.pylate_model.encode(
                contents_to_encode,
                batch_size=self.encode_batch_size,
                is_query=False,  # Mark as document encoding
                show_progress_bar=self.enable_tqdm,
                convert_to_numpy=False,  # Keep as tensors for better precision
            )

        plaid_index = pylate_indexes.PLAID(
            index_folder=output_path,
            index_name=self.PLAID_INDEX_DIR_NAME,
            override=True,
            **self.plaid_config_params,
        )
        plaid_index.add_documents(
            documents_ids=[item.item_id for item in items],
            documents_embeddings=doc_embeddings,
        )

        with open(items_path, "wb") as f:
            pickle.dump(items, f, protocol=pickle.HIGHEST_PROTOCOL)

    def _load_retriever_and_items(
        self, output_path: str
    ) -> tuple[pylate_retrieve.ColBERT, Dict[str, SearchableItem]]:
        """Loads the PLAID retriever and item map, caching them in memory."""
        if output_path in self._retriever_cache:
            return self._retriever_cache[output_path], self._items_cache[output_path]

        items_path = self._get_items_path(output_path)
        plaid_index_path = self._get_plaid_index_path(output_path)

        if not os.path.exists(items_path) or not os.path.exists(plaid_index_path):
            raise FileNotFoundError(f"Index or item file not found in '{output_path}'")

        plaid_index = pylate_indexes.PLAID(
            index_folder=output_path,
            index_name=self.PLAID_INDEX_DIR_NAME,
            override=False,
            **self.plaid_config_params,
        )
        retriever = pylate_retrieve.ColBERT(index=plaid_index)

        with open(items_path, "rb") as f:
            items_list = pickle.load(f)
        items_map = {item.item_id: item for item in items_list}

        self._retriever_cache[output_path] = retriever
        self._items_cache[output_path] = items_map
        return retriever, items_map

    def retrieve(
        self, processed_queries_batch: List[List[str]], output_path: str, k: int
    ) -> List[List[RetrievalResult]]:
        """Retrieves items using the Pylate/PLAID index."""
        if not os.path.exists(output_path):
            print(f"ColBERT index not found for in {output_path}. Skipping.")
            return [[] for _ in processed_queries_batch]
        retriever, items_map = self._load_retriever_and_items(output_path)

        flat_queries = []
        query_to_original_idx_map = []
        for i, sub_queries in enumerate(processed_queries_batch):
            for sub_query in sub_queries:
                flat_queries.append(sub_query)
                query_to_original_idx_map.append(i)

        if not flat_queries:
            return [[] for _ in processed_queries_batch]

        self.pylate_model.eval()

        with torch.no_grad():
            query_embeddings = self.pylate_model.encode(
                flat_queries,
                batch_size=self.encode_batch_size,
                is_query=True,
                show_progress_bar=self.enable_tqdm,
                convert_to_numpy=False,
            )

        pylate_results_batch = retriever.retrieve(
            queries_embeddings=query_embeddings, k=k
        )

        aggregated_results: List[Dict[str, RetrievalResult]] = [
            {} for _ in processed_queries_batch
        ]

        pbar_desc = "Processing ColBERT Results"
        for i in tqdm(
            range(len(flat_queries)), desc=pbar_desc, disable=not self.enable_tqdm
        ):
            original_nlq_idx = query_to_original_idx_map[i]
            pylate_results = pylate_results_batch[i]

            for res_dict in pylate_results:
                item_id = res_dict["id"]
                score = res_dict["score"]

                if item_id in items_map:
                    item = items_map[item_id]
                    result = RetrievalResult(item=item, score=float(score))

                    if (
                        item_id not in aggregated_results[original_nlq_idx]
                        or result.score
                        > aggregated_results[original_nlq_idx][item_id].score
                    ):
                        aggregated_results[original_nlq_idx][item_id] = result

        final_batches = []
        for res_dict in aggregated_results:
            sorted_res = sorted(res_dict.values(), key=lambda r: r.score, reverse=True)
            final_batches.append(sorted_res[:k])

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/dense_retriever.py
# EXT: .py, SIZE: 10505 bytes, SHA256: a3590f902fd59abbe5f9ac7436548ff2c9beb229af8493fa2cfc73011d3ecd77

import asyncio
import os
import pickle
from typing import Any, Dict, List, Optional

import faiss
import numpy as np
import torch
from nlp_retrieval.core.models import RetrievalResult, SearchableItem
from nlp_retrieval.retrievers.retriever_abc import BaseRetriever

# from infinity_emb import AsyncEngineArray, EngineArgs
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm

from vllm import LLM


class FaissRetriever(BaseRetriever):
    """
    A powerful dense retriever using FAISS for indexing and search.

    This class supports multiple embedding backends:
    - 'sentence-transformers': Standard, easy-to-use library (default).
    - 'vllm': High-throughput embedding generation using VLLM.
    - 'infinity': High-throughput embedding generation using Infinity-Embed.
    """

    INDEX_FILENAME = "faiss.index"
    ITEMS_FILENAME = "items.pkl"

    def __init__(
        self,
        model_name_or_path: str = "BAAI/bge-m3",
        embedding_backend: str = "sentence-transformers",
        batch_size: int = 128,
        normalize_embeddings: bool = True,
        enable_tqdm: bool = True,
    ):
        """
        Initializes the FaissRetriever.

        Args:
            model_name_or_path: The name or path of the embedding model.
            embedding_backend: The backend to use for generating embeddings.
                               Options: 'sentence-transformers', 'vllm', 'infinity'.
            batch_size: The batch size for encoding operations.
            normalize_embeddings: Whether to L2-normalize embeddings. Essential for
                                  Inner Product (IP) search.
            enable_tqdm: If True, displays tqdm progress bars.
        """
        self.model_name_or_path = model_name_or_path
        self.embedding_backend = embedding_backend
        self.batch_size = batch_size
        self.normalize_embeddings = normalize_embeddings
        self.enable_tqdm = enable_tqdm
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.num_gpus = torch.cuda.device_count()

        self._model: Optional[Any] = None
        self._embedding_dim: Optional[int] = None
        self._items_cache: Dict[str, List[SearchableItem]] = {}
        self._faiss_index_cache: Dict[str, faiss.Index] = {}

        self._initialize_model()

    def _initialize_model(self):
        """Lazy initialization of the embedding model based on the selected backend."""
        if self._model:
            return

        if self.embedding_backend == "sentence-transformers":
            self._model = SentenceTransformer(
                self.model_name_or_path, device=self.device
            )
            self._embedding_dim = self._model.get_sentence_embedding_dimension()

        elif self.embedding_backend == "vllm":
            self._model = LLM(
                model=self.model_name_or_path,
                tensor_parallel_size=self.num_gpus or 1,
                trust_remote_code=True,
                enforce_eager=True,  # Required for embed mode
            )
            self._embedding_dim = self._model.llm_engine.model_config.get_hidden_size()

        # elif self.embedding_backend == "infinity":
        #    engine_args = EngineArgs(
        #        model_name_or_path=self.model_name_or_path,
        #        device=self.device,
        #        batch_size=self.batch_size,
        #    )
        #    self._model = AsyncEngineArray.from_args([engine_args])

        else:
            raise ValueError(f"Unknown embedding_backend: '{self.embedding_backend}'")

    def _encode(self, texts: List[str]) -> np.ndarray:
        """Encodes a list of texts into embeddings using the configured backend."""
        self._initialize_model()

        if self.embedding_backend == "sentence-transformers":
            return self._model.encode(
                texts,
                batch_size=self.batch_size,
                show_progress_bar=self.enable_tqdm,
                convert_to_numpy=True,
                normalize_embeddings=self.normalize_embeddings,
            )

        if self.embedding_backend == "vllm":
            outputs = self._model.embed(texts)
            embeddings = np.array([out.outputs.embedding for out in outputs])
            if self.normalize_embeddings:
                faiss.normalize_L2(embeddings)
            return embeddings

        if self.embedding_backend == "infinity":

            async def _embed_async():
                engine = self._model[0]
                await engine.astart()
                embeds, _ = await engine.embed(sentences=texts)
                await engine.astop()
                return np.array(embeds)

            embeddings = asyncio.run(_embed_async())
            if self._embedding_dim is None:  # Infer embedding dim on first run
                self._embedding_dim = embeddings.shape[1]
            if self.normalize_embeddings:
                faiss.normalize_L2(embeddings)
            return embeddings

        raise RuntimeError("Encoding failed due to uninitialized or unknown backend.")

    def index(self, items: List[SearchableItem], output_path: str) -> None:
        """Builds and saves a FAISS index and the corresponding item data."""
        index_file = os.path.join(output_path, self.INDEX_FILENAME)
        items_file = os.path.join(output_path, self.ITEMS_FILENAME)

        if os.path.exists(index_file) and os.path.exists(items_file):
            print(f"Index already exists in '{output_path}'. Skipping indexing.")
            return

        if not items:
            return

        contents = [item.content for item in items]
        embeddings = self._encode(contents)

        if self._embedding_dim is None:
            raise RuntimeError("Embedding dimension could not be determined.")

        # Using IndexFlatIP for normalized embeddings (dot product is equivalent to cosine similarity)
        index = faiss.IndexFlatIP(self._embedding_dim)
        index.add(embeddings.astype(np.float32))

        faiss.write_index(index, index_file)
        with open(items_file, "wb") as f:
            pickle.dump(items, f)

    def _load_index_and_items(
        self, output_path: str
    ) -> tuple[faiss.Index, List[SearchableItem]]:
        """Loads the FAISS index and item list, caching them in memory."""
        if output_path in self._faiss_index_cache:
            return self._faiss_index_cache[output_path], self._items_cache[output_path]

        index_file = os.path.join(output_path, self.INDEX_FILENAME)
        items_file = os.path.join(output_path, self.ITEMS_FILENAME)

        if not os.path.exists(index_file) or not os.path.exists(items_file):
            raise FileNotFoundError(f"Index or item file not found in '{output_path}'")

        index = faiss.read_index(index_file)
        with open(items_file, "rb") as f:
            items = pickle.load(f)

        # Move index to GPU if available
        if self.device == "cuda":
            try:
                res = faiss.StandardGpuResources()
                index = faiss.index_cpu_to_gpu(res, 0, index)
            except Exception:
                print("Failed to move FAISS index to GPU. Using CPU instead.")
                pass  # Keep on CPU if GPU transfer fails

        self._faiss_index_cache[output_path] = index
        self._items_cache[output_path] = items
        return index, items

    def retrieve(
        self, processed_queries_batch: List[List[str]], output_path: str, k: int
    ) -> List[List[RetrievalResult]]:
        """
        Retrieves items for a batch of processed queries, handling sub-query aggregation.

        This method retrieves k items for each sub-query, combines them into a
        single, deduplicated list for each original query, and annotates each
        result with the keyword that retrieved it.
        """
        if not os.path.exists(output_path):
            print(f"Dense index not found for in {output_path}. Skipping.")
            return [[] for _ in processed_queries_batch]
        index, items = self._load_index_and_items(output_path)

        # 1. Flatten all sub-queries for a single, efficient batch encoding call
        flat_queries = []
        query_to_original_idx_map = []
        for i, sub_queries in enumerate(processed_queries_batch):
            for sub_query in sub_queries:
                flat_queries.append(sub_query)
                query_to_original_idx_map.append(i)

        if not flat_queries:
            return [[] for _ in processed_queries_batch]

        # 2. Encode all queries in one batch
        query_embeddings = self._encode(flat_queries).astype(np.float32)

        # 3. Perform FAISS search
        scores_batch, indices_batch = index.search(query_embeddings, k)

        # 4. Aggregate results, keeping the highest score and tracking the keyword
        aggregated_results: List[Dict[str, RetrievalResult]] = [
            {} for _ in processed_queries_batch
        ]

        pbar_desc = f"Processing FAISS Results ({self.embedding_backend})"
        for i in tqdm(
            range(len(flat_queries)), desc=pbar_desc, disable=not self.enable_tqdm
        ):
            original_nlq_idx = query_to_original_idx_map[i]
            keyword_used = flat_queries[i]  #
            scores = scores_batch[i]
            indices = indices_batch[i]

            for score, doc_idx in zip(scores, indices):
                if doc_idx == -1:  # FAISS uses -1 for no result
                    continue

                retrieved_item = items[doc_idx].copy(deep=True)
                item_id = retrieved_item.item_id

                # If item is new or has a better score, add/update it
                if (
                    item_id not in aggregated_results[original_nlq_idx]
                    or score > aggregated_results[original_nlq_idx][item_id].score
                ):
                    retrieved_item.metadata["retrieved_by_keyword"] = keyword_used

                    aggregated_results[original_nlq_idx][item_id] = RetrievalResult(
                        item=retrieved_item, score=float(score)
                    )

        # 5. Convert the aggregated dictionaries to sorted lists
        final_batches = []
        for res_dict in aggregated_results:
            # Sort final results by score
            sorted_by_score = sorted(
                res_dict.values(), key=lambda r: r.score, reverse=True
            )
            final_batches.append(sorted_by_score)
        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/minhash_lsh_retriever.py
# EXT: .py, SIZE: 7049 bytes, SHA256: 6aeaba5bfe7ce68722e5bd02de328002f12653b06b40e5d390436bf19e307a31

import os
import pickle
from typing import Dict, List, Set

from nlp_retrieval.core.models import RetrievalResult, SearchableItem
from nlp_retrieval.retrievers.retriever_abc import BaseRetriever
from datasketch import MinHash, MinHashLSHForest
from tqdm import tqdm


class MinHashLshRetriever(BaseRetriever):
    """
    A retriever based on MinHash and Locality Sensitive Hashing (LSH).

    This retriever is effective at finding items with high Jaccard similarity
    (i.e., high token overlap) to the query in a scalable manner. It is not a
    semantic retriever but excels at near-duplicate detection and set-based matching.
    """

    LSH_INDEX_FILENAME = "minhash_lsh.pkl"
    ITEMS_FILENAME = "items.pkl"

    def __init__(self, num_perm: int = 128, enable_tqdm: bool = True):
        """
        Initializes the MinHashLshRetriever.

        Args:
            num_perm: The number of permutation functions to use for the MinHash
                      signatures. A larger number increases accuracy but also
                      memory usage and processing time.
            enable_tqdm: If True, displays tqdm progress bars.
        """
        self.num_perm = num_perm
        self.enable_tqdm = enable_tqdm

        # Caches to avoid reloading from disk for the same index path
        self._lsh_cache: Dict[str, tuple[MinHashLSHForest, Dict[str, MinHash]]] = {}
        self._items_cache: Dict[str, Dict[str, SearchableItem]] = {}

    def _create_minhash(self, text: str) -> MinHash:
        """Creates a MinHash signature for a given text."""
        minhash = MinHash(num_perm=self.num_perm)
        # Simple whitespace tokenization
        for token in text.split():
            minhash.update(token.encode("utf8"))
        return minhash

    def index(self, items: List[SearchableItem], output_path: str) -> None:
        """
        Builds a MinHash LSH Forest index from a list of items.
        """
        lsh_index_path = os.path.join(output_path, self.LSH_INDEX_FILENAME)
        items_path = os.path.join(output_path, self.ITEMS_FILENAME)

        if os.path.exists(lsh_index_path) and os.path.exists(items_path):
            print(f"Index already exists in '{output_path}'. Skipping indexing.")
            return

        if not items:
            return

        forest = MinHashLSHForest(num_perm=self.num_perm)
        item_id_to_minhash_map: Dict[str, MinHash] = {}

        seen_item_ids: Set[str] = set()

        pbar_desc = "Creating MinHash Signatures"
        for item in tqdm(items, desc=pbar_desc, disable=not self.enable_tqdm):
            if item.item_id in seen_item_ids:
                # Skip if we've already processed this item
                continue

            minhash = self._create_minhash(item.content)
            item_id_to_minhash_map[item.item_id] = minhash
            forest.add(item.item_id, minhash)

            # Add the item ID to the seen set
            seen_item_ids.add(item.item_id)

        forest.index()

        with open(lsh_index_path, "wb") as f:
            pickle.dump((forest, item_id_to_minhash_map), f)

        with open(items_path, "wb") as f:
            pickle.dump(items, f)

    def _load_index_and_items(
        self, output_path: str
    ) -> tuple[MinHashLSHForest, Dict[str, MinHash], Dict[str, SearchableItem]]:
        """Loads the LSH Forest, MinHash map, and item map, caching them."""
        if output_path in self._lsh_cache:
            lsh_forest, minhash_map = self._lsh_cache[output_path]
            items_map = self._items_cache[output_path]
            return lsh_forest, minhash_map, items_map

        lsh_index_path = os.path.join(output_path, self.LSH_INDEX_FILENAME)
        items_path = os.path.join(output_path, self.ITEMS_FILENAME)

        if not os.path.exists(lsh_index_path) or not os.path.exists(items_path):
            raise FileNotFoundError(f"Index or item file not found in '{output_path}'")

        with open(lsh_index_path, "rb") as f:
            lsh_forest, minhash_map = pickle.load(f)

        with open(items_path, "rb") as f:
            items_list = pickle.load(f)

        items_map = {item.item_id: item for item in items_list}

        self._lsh_cache[output_path] = (lsh_forest, minhash_map)
        self._items_cache[output_path] = items_map
        return lsh_forest, minhash_map, items_map

    def retrieve(
        self, processed_queries_batch: List[List[str]], output_path: str, k: int
    ) -> List[List[RetrievalResult]]:
        """
        Retrieves items by finding nearest neighbors in the MinHash LSH Forest.
        """
        if not os.path.exists(output_path):
            print(f"MinHash LSH index not found for in {output_path}. Skipping.")
            return [[] for _ in processed_queries_batch]
        lsh_forest, minhash_map, items_map = self._load_index_and_items(output_path)

        # 1. Flatten queries for processing
        flat_queries = []
        query_to_original_idx_map = []
        for i, sub_queries in enumerate(processed_queries_batch):
            for sub_query in sub_queries:
                flat_queries.append(sub_query)
                query_to_original_idx_map.append(i)

        if not flat_queries:
            return [[] for _ in processed_queries_batch]

        # 2. Create MinHash for all queries
        query_minhashes = [self._create_minhash(q) for q in flat_queries]

        # 3. Query the LSH Forest and aggregate results
        aggregated_results: List[Dict[str, RetrievalResult]] = [
            {} for _ in processed_queries_batch
        ]

        pbar_desc = "Processing MinHash LSH Results"
        for i in tqdm(
            range(len(flat_queries)), desc=pbar_desc, disable=not self.enable_tqdm
        ):
            original_nlq_idx = query_to_original_idx_map[i]
            query_minhash = query_minhashes[i]

            # Find candidate item IDs from the LSH Forest
            retrieved_ids = lsh_forest.query(query_minhash, k)

            for item_id in retrieved_ids:
                if item_id in minhash_map and item_id in items_map:
                    # Calculate the Jaccard similarity as the score
                    item_minhash = minhash_map[item_id]
                    score = query_minhash.jaccard(item_minhash)

                    item = items_map[item_id]
                    result = RetrievalResult(item=item, score=score)

                    # Add to results, keeping the one with the highest score
                    if (
                        item_id not in aggregated_results[original_nlq_idx]
                        or result.score
                        > aggregated_results[original_nlq_idx][item_id].score
                    ):
                        aggregated_results[original_nlq_idx][item_id] = result

        # 4. Convert dictionaries to sorted lists
        final_batches = []
        for res_dict in aggregated_results:
            sorted_res = sorted(res_dict.values(), key=lambda r: r.score, reverse=True)
            final_batches.append(sorted_res)

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/ragatouille_colbert_retriever.py
# EXT: .py, SIZE: 7099 bytes, SHA256: 6692f15d9a9046d5b33f37913a6b2b7406e38a39b6baac9feb7a7a797d4c3a86

import os
from typing import Dict, List, Union

from nlp_retrieval.core.models import RetrievalResult, SearchableItem
from nlp_retrieval.retrievers.retriever_abc import BaseRetriever
from ragatouille import RAGPretrainedModel
from tqdm.auto import tqdm


class RagatouilleColbertRetriever(BaseRetriever):
    """
    A late-interaction retriever using ColBERT models via the RAGatouille library.
    It is preferable to use the ColBERT implementation using pylate, using
    the `PylateColbertRetriever` class found in `colbert_retriever.py`
    """

    def __init__(
        self,
        model_name_or_path: str = "jinaai/jina-colbert-v2",
        enable_tqdm: bool = True,
    ):
        """
        Initializes the RagatouilleColbertRetriever.

        Args:
            model_name_or_path: The name or path of the pretrained ColBERT model
            enable_tqdm: If True, displays tqdm progress bars during operations.
        """
        self.model_name_or_path = model_name_or_path
        self.enable_tqdm = enable_tqdm
        self._rag_model_cache: Dict[str, RAGPretrainedModel] = {}

    def index(self, items: List[SearchableItem], output_path: str) -> None:
        """
        Builds and saves a RAGatouille (ColBERT) index.
        """
        # RAGatouille manages index paths internally relative to an `index_root`.
        # The `output_path` from our framework will serve as this root.
        index_name = f"ragatouille_index_{os.path.basename(output_path)}"
        full_index_path = os.path.join(
            output_path, ".ragatouille", "colbert", "indexes", index_name
        )

        if os.path.exists(full_index_path):
            print(
                f"Index '{index_name}' appears to exist in '{output_path}'. Skipping indexing."
            )
            return

        if not items:
            print("Warning: No items provided to index.")
            return

        # Initialize the RAG model, setting the root for all its indexes.
        rag_model = RAGPretrainedModel.from_pretrained(
            self.model_name_or_path, index_root=output_path
        )

        collection = [item.content for item in items]
        document_ids = [item.item_id for item in items]
        document_metadatas = [item.metadata for item in items]

        rag_model.index(
            collection=collection,
            document_ids=document_ids,
            document_metadatas=document_metadatas,
            index_name=index_name,
            overwrite_index=True,
            use_faiss=True,
        )

    def _load_rag_model(self, output_path: str) -> RAGPretrainedModel:
        """Loads a RAGatouille model from an index path, using an in-memory cache."""
        # We use the output_path as the key, as it's the root for all indexes within.
        if output_path in self._rag_model_cache:
            return self._rag_model_cache[output_path]

        # RAGatouille needs the path to the specific index directory to load.
        # We assume there's only one index per retriever output_path.
        index_root_path = os.path.join(
            output_path, ".ragatouille", "colbert", "indexes"
        )
        if not os.path.exists(index_root_path):
            raise FileNotFoundError(
                f"RAGatouille index directory not found at: {index_root_path}. "
                "Please ensure `index()` has been run."
            )

        try:
            # Get the first (and likely only) index name in the directory
            index_name = os.listdir(index_root_path)[0]
            full_index_path = os.path.join(index_root_path, index_name)
        except (FileNotFoundError, IndexError):
            raise FileNotFoundError(
                f"No RAGatouille index found under the root: {output_path}. "
                "Please ensure `index()` has been run."
            )

        # from_index is the correct way to load a model for searching
        rag_model = RAGPretrainedModel.from_index(full_index_path)
        self._rag_model_cache[output_path] = rag_model
        return rag_model

    def retrieve(
        self, processed_queries_batch: List[List[str]], output_path: str, k: int
    ) -> List[List[RetrievalResult]]:
        """
        Retrieves items for a batch of queries using the RAGatouille index.
        """
        rag_model = self._load_rag_model(output_path)

        # 1. Flatten all sub-queries for a single, efficient batch search call
        flat_queries = []
        query_to_original_idx_map = []
        for i, sub_queries in enumerate(processed_queries_batch):
            for sub_query in sub_queries:
                flat_queries.append(sub_query)
                query_to_original_idx_map.append(i)

        if not flat_queries:
            return [[] for _ in processed_queries_batch]

        # 2. Perform one large batch search using RAGatouille
        ragatouille_results_batch: Union[List[Dict], List[List[Dict]]]
        ragatouille_results_batch = rag_model.search(query=flat_queries, k=k)

        # RAGatouille returns a single list for a single query, but a list of lists for multiple.
        # We normalize to always be a list of lists for consistent processing.
        if flat_queries and len(flat_queries) == 1:
            ragatouille_results_batch = [ragatouille_results_batch]

        # 3. Aggregate results, keeping the highest score for each unique item
        aggregated_results: List[Dict[str, RetrievalResult]] = [
            {} for _ in processed_queries_batch
        ]

        pbar_desc = "Processing RAGatouille Results"
        for i in tqdm(
            range(len(flat_queries)), desc=pbar_desc, disable=not self.enable_tqdm
        ):
            original_nlq_idx = query_to_original_idx_map[i]
            results_for_sub_query = ragatouille_results_batch[i]

            for res_dict in results_for_sub_query:
                item_id = res_dict.get("document_id")
                if not item_id:
                    continue

                # Reconstruct our SearchableItem and RetrievalResult models
                item = SearchableItem(
                    item_id=item_id,
                    content=res_dict.get("content", ""),
                    metadata=res_dict.get("document_metadata", {}),
                )
                result = RetrievalResult(
                    item=item, score=float(res_dict.get("score", 0.0))
                )

                # If item is new for this query or has a better score, add/update it
                if (
                    item_id not in aggregated_results[original_nlq_idx]
                    or result.score
                    > aggregated_results[original_nlq_idx][item_id].score
                ):
                    aggregated_results[original_nlq_idx][item_id] = result

        # 4. Convert the aggregated dictionaries to sorted lists
        final_batches = []
        for res_dict in aggregated_results:
            sorted_by_score = sorted(
                res_dict.values(), key=lambda r: r.score, reverse=True
            )
            final_batches.append(sorted_by_score)

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/retrievers/retriever_abc.py
# EXT: .py, SIZE: 2318 bytes, SHA256: 51e2bb3390a269b749fbed449b636932ffb4938337168e7d8e1ab0c08555a831

from abc import ABC, abstractmethod
from typing import List

from nlp_retrieval.core.models import RetrievalResult, SearchableItem


class BaseRetriever(ABC):
    """
    Abstract Base Class for retrieval models.

    Retrievers are responsible for two main tasks:
    1. `index`: Building an efficient search index from a collection of `SearchableItem`s.
    2. `retrieve`: Using the index to find the most relevant items for a given set of queries.
    """

    @abstractmethod
    def index(self, items: List[SearchableItem], output_path: str) -> None:
        """
        Builds and saves an index from a list of items.

        Args:
            items: A list of `SearchableItem` objects to be indexed.
            output_path: The directory path where the index and any related
                         artifacts should be stored.
        """
        pass

    @abstractmethod
    def retrieve(
        self,
        processed_queries_batch: List[List[str]],
        output_path: str,
        k: int,
        **kwargs,
    ) -> List[List[RetrievalResult]]:
        """
        Retrieves relevant items for a batch of processed queries.

        Each element in `processed_queries_batch` is a list of strings representing
        all the queries (e.g., original + sub-queries) for a single original NLQ.
        The implementation should retrieve candidates for all these sub-queries and
        return a single, aggregated list of results for that original NLQ.

        Args:
            processed_queries_batch: A list where each inner list contains the query strings
                                     for one original NLQ.
            output_path: The path to the directory containing the pre-built index.
            k: The number of candidate results to retrieve. Note that this `k` might
               be applied per sub-query, so the implementation should handle
               aggregation and deduplication to produce a final list for each original NLQ.

        Returns:
            A list of lists of `RetrievalResult` objects. Each outer list corresponds
            to one original NLQ, and the inner list contains the aggregated,
            deduplicated results from all its sub-queries.
            The structure must be `len(processed_queries_batch) == len(output)`.
        """
        pass

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/evaluation/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/evaluation/eval_models.py
# EXT: .py, SIZE: 3258 bytes, SHA256: ca3a1b2ac76b88ef48cc6e80222aa013090175feb3f5de74cce89a2c49125f39

import json
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class PerQueryMetrics(BaseModel):
    """
    Holds the detailed evaluation metrics for a single query.
    """

    query_index: int
    true_positives: int
    false_positives: int
    false_negatives: int
    precision: float
    recall: float
    f1_score: float
    perfect_recall: float = Field(
        description="1.0 if all gold items were found for this query, 0.0 otherwise."
    )
    recall_non_numerical: float = Field(
        description="Recall calculated after filtering out purely numerical gold items."
    )
    perfect_recall_non_numerical: float = Field(
        description="1.0 if all non-numerical gold items were found, 0.0 otherwise."
    )
    retrieved_items: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description="Details of all de-duplicated items that were retrieved for this query. Only populated if non-numerical recall is not perfect.",
    )
    missed_items: Optional[List[Dict[str, Any]]] = Field(
        default=None,
        description="Details of the gold items that were missed. Only populated if non-numerical recall is not perfect.",
    )


class EvaluationSummary(BaseModel):
    """
    A summary of the evaluation results across all queries.
    """

    num_queries: int
    overall_precision: float = Field(
        description="Precision calculated from the sum of all TPs and FPs (micro-average)."
    )
    overall_recall: float = Field(
        description="Recall calculated from the sum of all TPs and FNs (micro-average)."
    )
    overall_f1_score: float = Field(
        description="F1-score calculated from the overall precision and recall."
    )
    perfect_recall_rate: float = Field(
        description="The percentage of queries for which all gold standard items were found."
    )
    overall_recall_non_numerical: float = Field(
        description="Overall recall calculated after filtering out purely numerical gold items."
    )
    perfect_recall_rate_non_numerical: float = Field(
        description="The percentage of queries for which all non-numerical gold items were found."
    )
    per_query_details: List[PerQueryMetrics] = Field(
        description="A list containing detailed metrics for each individual query."
    )

    def save_failures_to_json(self, output_path: str):
        """
        Saves the details of queries with imperfect non-numerical recall to a JSON file.

        Args:
            output_path: The path to the JSON file where failures will be saved.
        """
        failure_cases = [
            {
                "query_index": query_metrics.query_index,
                "retrieved_items": query_metrics.retrieved_items,
                "missed_items": query_metrics.missed_items,
            }
            for query_metrics in self.per_query_details
            if query_metrics.perfect_recall_non_numerical == 0.0
        ]

        if failure_cases:
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(
                    {"evaluation_failures": failure_cases},
                    f,
                    indent=4,
                    ensure_ascii=False,
                )

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/evaluation/evaluator.py
# EXT: .py, SIZE: 10582 bytes, SHA256: 773523eb79cf6ff3a1b7ab86740fc2a54230681556ec5c00b2941e827b820a26

import re
from typing import List, Set

import pandas as pd
from nlp_retrieval.core.models import RetrievalResult, SearchableItem
from nlp_retrieval.evaluation.eval_models import (
    EvaluationSummary,
    PerQueryMetrics,
)


class RetrievalEvaluator:
    """
    Calculates and visualizes retrieval evaluation metrics.

    This class compares a list of retrieved results against a gold standard
    list. The core matching logic is based on checking if the metadata of a gold
    item is a subset of the metadata of a retrieved item.

    It automatically handles evaluation granularity by inspecting the gold standard.
    For each query, it determines the granularity based on the metadata keys of the
    gold items and deduplicates the retrieved items accordingly before calculating metrics.
    """

    def _is_match(
        self, retrieved_item: SearchableItem, gold_item: SearchableItem
    ) -> bool:
        """
        Checks if a retrieved item is a match for a gold standard item.

        A match occurs if all key-value pairs in the gold item's metadata
        are present in the retrieved item's metadata. The comparison is
        case-insensitive for both keys and string values.
        """
        retrieved_meta_lower = {
            k.lower(): (v.lower() if isinstance(v, str) else v)
            for k, v in retrieved_item.metadata.items()
        }
        gold_meta_lower = {
            k.lower(): (v.lower() if isinstance(v, str) else v)
            for k, v in gold_item.metadata.items()
        }
        return gold_meta_lower.items() <= retrieved_meta_lower.items()

    def deduplicate_by_granularity(
        self, results: List[RetrievalResult], granularity_fields: List[str]
    ) -> List[RetrievalResult]:
        """
        Deduplicates a list of results based on a set of metadata fields.

        It preserves the original order and keeps the first occurrence of each
        unique entity defined by the granularity fields. The comparison is
        case-insensitive for both the metadata keys and their string values.
        """
        if not granularity_fields:
            return results

        seen_keys: Set[tuple] = set()
        deduplicated_results: List[RetrievalResult] = []
        for res in results:
            meta_lower_keys = {k.lower(): v for k, v in res.item.metadata.items()}
            key = tuple(
                (lambda v: v.lower() if isinstance(v, str) else v)(
                    meta_lower_keys.get(field.lower())
                )
                for field in granularity_fields
            )
            if key not in seen_keys:
                seen_keys.add(key)
                deduplicated_results.append(res)
        return deduplicated_results

    def _get_granularity_fields_for_query(
        self, gold_list: List[RetrievalResult]
    ) -> List[str]:
        """
        Determines evaluation granularity from the gold standard for a single query.
        """
        if not gold_list:
            return []
        return list(gold_list[0].item.metadata.keys())

    def _contains_alpha(self, text: str) -> bool:
        """Checks if a string contains any alphabetic characters."""
        return bool(re.search("[a-zA-Z]", text))

    def evaluate(
        self,
        retrieved_results: List[List[RetrievalResult]],
        gold_standard: List[List[RetrievalResult]],
    ) -> EvaluationSummary:
        """
        Calculates comprehensive evaluation metrics for a batch of queries.
        """
        if len(retrieved_results) != len(gold_standard):
            raise ValueError(
                f"Mismatch in number of queries. Retrieved results have {len(retrieved_results)}, "
                f"but gold standard has {len(gold_standard)}."
            )

        total_tp, total_fp, total_fn = 0, 0, 0
        total_perfect_recalls = 0.0
        total_tp_non_numerical, total_fn_non_numerical = 0, 0
        total_perfect_recalls_non_numerical = 0.0
        all_query_metrics: List[PerQueryMetrics] = []

        for i, (retrieved_list, gold_list) in enumerate(
            zip(retrieved_results, gold_standard)
        ):
            # --- Automatic Granularity Handling ---
            granularity_fields = self._get_granularity_fields_for_query(gold_list)
            effective_retrieved_list = self.deduplicate_by_granularity(
                retrieved_list, granularity_fields
            )

            # --- Per-Query Calculation (Standard) ---
            matched_gold_indices: Set[int] = set()
            matched_retrieved_indices: Set[int] = set()

            # Find all matches between the (deduplicated) retrieved list and the gold list
            for retrieved_idx, retrieved_res in enumerate(effective_retrieved_list):
                for gold_idx, gold_res in enumerate(gold_list):
                    if gold_idx in matched_gold_indices:
                        continue

                    if self._is_match(retrieved_res.item, gold_res.item):
                        matched_retrieved_indices.add(retrieved_idx)
                        matched_gold_indices.add(gold_idx)
                        break

            # Calculate TP, FP, FN using the length of the effective (deduplicated) list
            tp = len(matched_retrieved_indices)
            fp = len(effective_retrieved_list) - tp
            fn = len(gold_list) - len(matched_gold_indices)

            # Calculate metrics for this query
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = (
                2 * (precision * recall) / (precision + recall)
                if (precision + recall) > 0
                else 0.0
            )
            perfect_recall = 1.0 if fn == 0 else 0.0

            # --- Per-Query Calculation (Non-Numerical) ---
            gold_list_non_numerical = [
                (idx, gold_res)
                for idx, gold_res in enumerate(gold_list)
                if self._contains_alpha(str(gold_res.item.metadata.get("value", "")))
            ]

            matched_gold_indices_non_numerical: Set[int] = set()
            if gold_list_non_numerical:
                for _, retrieved_res in enumerate(effective_retrieved_list):
                    for gold_idx, gold_res in gold_list_non_numerical:
                        if gold_idx in matched_gold_indices_non_numerical:
                            continue
                        if self._is_match(retrieved_res.item, gold_res.item):
                            matched_gold_indices_non_numerical.add(gold_idx)
                            break

            tp_non_numerical = len(matched_gold_indices_non_numerical)
            fn_non_numerical = len(gold_list_non_numerical) - tp_non_numerical

            recall_non_numerical = (
                tp_non_numerical / len(gold_list_non_numerical)
                if gold_list_non_numerical
                else 1.0
            )
            perfect_recall_non_numerical = 1.0 if fn_non_numerical == 0 else 0.0

            # --- Logging Failure Cases ---
            retrieved_items_for_log = None
            missed_items_for_log = None
            if perfect_recall_non_numerical == 0.0:
                missed_items_for_log = [
                    gold_list[i].item.model_dump()
                    for i in range(len(gold_list))
                    if i not in matched_gold_indices
                ]
                retrieved_items_for_log = [
                    res.model_dump() for res in effective_retrieved_list
                ]

            all_query_metrics.append(
                PerQueryMetrics(
                    query_index=i,
                    true_positives=tp,
                    false_positives=fp,
                    false_negatives=fn,
                    precision=precision,
                    recall=recall,
                    f1_score=f1,
                    perfect_recall=perfect_recall,
                    recall_non_numerical=recall_non_numerical,
                    perfect_recall_non_numerical=perfect_recall_non_numerical,
                    retrieved_items=retrieved_items_for_log,
                    missed_items=missed_items_for_log,
                )
            )

            # Update total counts
            total_tp += tp
            total_fp += fp
            total_fn += fn
            total_perfect_recalls += perfect_recall

            # Update new total counts
            total_tp_non_numerical += tp_non_numerical
            total_fn_non_numerical += fn_non_numerical
            total_perfect_recalls_non_numerical += perfect_recall_non_numerical

        # --- Overall Aggregation ---
        num_queries = len(retrieved_results)
        overall_precision = (
            total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        )
        overall_recall = (
            total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        )
        overall_f1 = (
            2
            * (overall_precision * overall_recall)
            / (overall_precision + overall_recall)
            if (overall_precision + overall_recall) > 0
            else 0.0
        )
        perfect_recall_rate = (
            total_perfect_recalls / num_queries if num_queries > 0 else 0.0
        )

        overall_recall_non_numerical = (
            total_tp_non_numerical / (total_tp_non_numerical + total_fn_non_numerical)
            if (total_tp_non_numerical + total_fn_non_numerical) > 0
            else 0.0
        )
        perfect_recall_rate_non_numerical = (
            total_perfect_recalls_non_numerical / num_queries
            if num_queries > 0
            else 0.0
        )

        return EvaluationSummary(
            num_queries=num_queries,
            overall_precision=overall_precision,
            overall_recall=overall_recall,
            overall_f1_score=overall_f1,
            perfect_recall_rate=perfect_recall_rate,
            overall_recall_non_numerical=overall_recall_non_numerical,
            perfect_recall_rate_non_numerical=perfect_recall_rate_non_numerical,
            per_query_details=all_query_metrics,
        )

    def to_dataframe(self, summary: EvaluationSummary) -> pd.DataFrame:
        """
        Converts the EvaluationSummary object into a pandas DataFrame for easy viewing.
        """
        records = [
            query_metrics.model_dump() for query_metrics in summary.per_query_details
        ]
        df = pd.DataFrame.from_records(records)
        df = df.set_index("query_index")
        return df

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/keybert_query_processor.py
# EXT: .py, SIZE: 2299 bytes, SHA256: 04e594e1e92135ad6b822dd6d6470e4b473a54974704084c5fb47e38f5461ed3

from typing import List, Optional

from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)
from keybert import KeyBERT


class KeyBERTProcessor(BaseUserQueryProcessor):
    """
    A query processor that uses KeyBERT to extract keywords and keyphrases.

    This method leverages sentence-transformer models to find the most relevant
    phrases in a query by comparing phrase embeddings to the full query embedding.
    """

    def __init__(
        self,
        model: str = "BAAI/bge-m3",
        keyphrase_ngram_range: tuple = (1, 2),
        stop_words: Optional[str] = "english",
        top_n: int = 5,
        use_mmr: bool = True,
        diversity: float = 0.5,
        **kwargs,
    ):
        """
        Initializes the KeyBERT-based processor.

        Args:
            model (str): The sentence-transformer model to use.
            keyphrase_ngram_range (tuple): The length of n-grams to consider for keyphrases.
            stop_words (Optional[str]): The language for stop words, e.g., 'english' or None.
            top_n (int): The number of keywords to extract.
            use_mmr (bool): Whether to use Maximal Marginal Relevance (MMR) to diversify results.
            diversity (float): The diversity factor for MMR (0 for no diversity, 1 for max).
            **kwargs: Additional arguments passed to `kw_model.extract_keywords`.
        """
        self.kw_model = KeyBERT(model)
        self.extract_kwargs = {
            "keyphrase_ngram_range": keyphrase_ngram_range,
            "stop_words": stop_words,
            "top_n": top_n,
            "use_mmr": use_mmr,
            "diversity": diversity,
            **kwargs,
        }

    def process(self, nlqs: List[str]) -> List[List[str]]:
        """
        Extracts keywords from a batch of queries using KeyBERT.
        """
        extracted_keywords_with_scores = self.kw_model.extract_keywords(
            nlqs, **self.extract_kwargs
        )
        if isinstance(extracted_keywords_with_scores[0], list):
            return [
                [keyword for keyword, score in kw_list]
                for kw_list in extracted_keywords_with_scores
            ]
        else:
            return [[keyword for keyword, score in extracted_keywords_with_scores]]

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/llm_decomposition_query_processor.py
# EXT: .py, SIZE: 8847 bytes, SHA256: 56f6f6e179408d1d51ff1df7a14c9b4104dfdc5cae03343caa33899f7721bfa6

import json
import os
import re
from typing import Dict, List, Optional

from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)
from tqdm.auto import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams


class DecompositionProcessor(BaseUserQueryProcessor):
    """
    A query processor that decomposes complex queries into simpler, self-contained
    sub-queries using a VLLM-powered large language model.

    This processor is optimized for high-throughput decomposition, using batching
    and an optional file-based cache to avoid redundant computations.
    """

    def __init__(
        self,
        model_name_or_path: str,
        cache_folder: Optional[str] = None,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.85,
        cache_dir="/data/hdd1/vllm_models/",
        **kwargs,
    ):
        """
        Initializes the VLLM-based decomposition processor.

        Args:
            model_name_or_path (str): The name or path of the model for VLLM.
            cache_folder (Optional[str]): A path to a folder for caching results.
                                          If None, caching is disabled.
            tensor_parallel_size (int): The number of GPUs for tensor parallelism.
            gpu_memory_utilization (float): Fraction of GPU memory for VLLM.
            **kwargs: Additional arguments for VLLM's LLM class or SamplingParams.
        """
        self.model_name_or_path = model_name_or_path
        self.cache_folder = cache_folder
        self.decompositions_cache: Optional[Dict[str, List[str]]] = None

        if self.cache_folder:
            os.makedirs(self.cache_folder, exist_ok=True)
            self.cache_file = os.path.join(
                self.cache_folder, "decompositions_cache.json"
            )
            self.decompositions_cache = self._load_cache()

        # Pop VLLM and SamplingParams specific args from kwargs
        vllm_args = {
            "quantization": kwargs.pop("quantization", None),
            "enable_prefix_caching": kwargs.pop("enable_prefix_caching", True),
            "trust_remote_code": kwargs.pop("trust_remote_code", True),
            "download_dir": kwargs.pop("download_dir", cache_dir),
            "max_model_len": kwargs.pop("max_model_len", 2048),
            "max_seq_len_to_capture": kwargs.pop("max_seq_len_to_capture", 1024),
        }
        sampling_args = {
            "temperature": kwargs.pop("temperature", 0.0),
            "top_p": kwargs.pop("top_p", 0.95),
            "top_k": kwargs.pop("top_k", -1),
            "max_tokens": kwargs.pop("max_tokens", 512),
        }

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=vllm_args["trust_remote_code"],
            cache_dir=cache_dir,
        )
        self.llm = LLM(
            model=self.model_name_or_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            **vllm_args,
        )
        self.sampling_params = SamplingParams(**sampling_args)

        self._build_prompt_template()

    def _build_prompt_template(self):
        """Constructs the few-shot prompt for the language model."""
        examples_data = [
            {
                "input": "What's the difference between web-based search and reflection agents? Do they use similar graph-based approaches?",
                "output": "What is the difference between web-based search and reflection agents\nDo web-based search and reflection agents use graph-based approaches\nWhat are web-based search agents\nWhat are reflection agents",
            },
            {
                "input": "How can I build a multi-agent system and stream intermediate steps from it?",
                "output": "How to build a multi-agent system\nHow to stream intermediate steps from a multi-agent system",
            },
        ]

        self.formatted_examples = []
        for ex in examples_data:
            self.formatted_examples.extend(
                [
                    {"role": "user", "content": ex["input"]},
                    {"role": "assistant", "content": ex["output"]},
                ]
            )

        system_prompt = """You are an expert at query decomposition. Your goal is to break down a user's question into a set of specific, answerable sub-questions that are all necessary to fully answer the original question.

Follow these rules:
1.  **Single Focus:** Each sub-question must target only one specific fact, concept, or entity.
2.  **Necessity:** Only generate sub-questions whose answers are strictly required. Do not add questions for general context if not asked.
3.  **Completeness:** The set of sub-questions must collectively cover all parts of the original question.
4.  **Preserve Terms:** Retain all acronyms, technical terms, and proper nouns from the original question.

Respond ONLY with the list of sub-questions, each on a new line. Do NOT include any introduction, explanation, numbering, or bullet points."""

        self.system_message = {"role": "system", "content": system_prompt}

    def _load_cache(self) -> Dict[str, List[str]]:
        if self.cache_file and os.path.exists(self.cache_file):
            with open(self.cache_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _save_cache(self):
        if self.cache_file and self.decompositions_cache is not None:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.decompositions_cache, f, indent=2, ensure_ascii=False)

    def _parse_llm_output(self, raw_output: str) -> List[str]:
        """Cleans and parses the raw text output from the LLM."""
        decomposed_queries = []
        if raw_output:
            lines = raw_output.strip().split("\n")
            for line in lines:
                # Clean up potential markdown, numbering, or extra whitespace
                cleaned_line = re.sub(r"^\s*[-*]?\s*\d*\.\s*", "", line).strip()
                if cleaned_line:
                    decomposed_queries.append(cleaned_line)
        return decomposed_queries

    def process(self, nlqs: List[str]) -> List[List[str]]:
        """Decomposes a batch of natural language queries into sub-queries."""
        if not nlqs:
            return []

        final_results: List[Optional[List[str]]] = [None] * len(nlqs)
        prompts_to_generate, indices_to_generate, nlqs_to_generate = [], [], []

        # First pass: check cache and prepare prompts for non-cached queries.
        for i, nlq in enumerate(tqdm(nlqs, desc="Preparing decomposition prompts")):
            if (
                self.decompositions_cache is not None
                and nlq in self.decompositions_cache
            ):
                final_results[i] = self.decompositions_cache[nlq]
                continue

            messages = (
                [self.system_message]
                + self.formatted_examples
                + [{"role": "user", "content": nlq}]
            )
            prompt_text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            prompts_to_generate.append(prompt_text)
            indices_to_generate.append(i)
            nlqs_to_generate.append(nlq)

        # Second pass: generate decompositions for non-cached queries in a single batch.
        if prompts_to_generate:
            vllm_outputs = self.llm.generate(prompts_to_generate, self.sampling_params)

            for i, output in enumerate(
                tqdm(vllm_outputs, desc="Generating decompositions")
            ):
                original_nlq_index = indices_to_generate[i]
                current_nlq = nlqs_to_generate[i]
                raw_output = output.outputs[0].text
                decomposed_queries = self._parse_llm_output(raw_output)

                # Include the original query as one of the queries to search for.
                final_decomposed_list = [current_nlq] + decomposed_queries
                final_results[original_nlq_index] = list(
                    dict.fromkeys(final_decomposed_list)
                )  # Deduplicate

                if self.decompositions_cache is not None:
                    self.decompositions_cache[current_nlq] = final_results[
                        original_nlq_index
                    ]

        # Save the cache if it was modified.
        if self.decompositions_cache is not None and prompts_to_generate:
            self._save_cache()

        # Ensure the final output matches the required format.
        return [
            res if res is not None else [nlqs[i]] for i, res in enumerate(final_results)
        ]

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/llm_keyword_extractor_query_processor.py
# EXT: .py, SIZE: 8474 bytes, SHA256: f78acb3d8043887d445deac89e4c7a5450defa0800774701c11b0b85e18e09f3

import json
import os
import re
from typing import Dict, List, Optional

from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)
from tqdm.auto import tqdm
from transformers import AutoTokenizer

from vllm import LLM, SamplingParams


class KeywordExtractorProcessor(BaseUserQueryProcessor):
    """
    A query processor that extracts key terms and entities from queries
    using a VLLM-powered large language model.

    This processor is optimized for high-throughput keyword extraction, using batching
    and an optional file-based cache to avoid redundant computations.
    """

    def __init__(
        self,
        model_name_or_path: str,
        cache_folder: Optional[str] = None,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.85,
        cache_dir="/data/hdd1/vllm_models/",
        **kwargs,
    ):
        """
        Initializes the VLLM-based keyword extraction processor.

        Args:
            model_name_or_path (str): The name or path of the model for VLLM.
            cache_folder (Optional[str]): A path to a folder for caching results.
            tensor_parallel_size (int): The number of GPUs for tensor parallelism.
            gpu_memory_utilization (float): Fraction of GPU memory for VLLM.
            **kwargs: Additional arguments for VLLM's LLM class or SamplingParams.
        """
        self.model_name_or_path = model_name_or_path
        self.cache_folder = cache_folder
        self.keywords_cache: Optional[Dict[str, List[str]]] = None

        if self.cache_folder:
            os.makedirs(self.cache_folder, exist_ok=True)
            self.cache_file = os.path.join(self.cache_folder, "keywords_cache.json")
            self.keywords_cache = self._load_cache()

        self.vllm_args = {
            "quantization": kwargs.pop("quantization", None),
            "trust_remote_code": kwargs.pop("trust_remote_code", True),
            "max_model_len": kwargs.pop("max_model_len", 4096),
        }
        sampling_args = {
            "temperature": kwargs.pop("temperature", 0.0),
            "top_p": kwargs.pop("top_p", 1.0),
            "max_tokens": kwargs.pop("max_tokens", 256),
        }
        self.cache_dir = cache_dir
        self.tensor_parallel_size = tensor_parallel_size
        self.gpu_memory_utilization = gpu_memory_utilization
        self.tokenizer = None
        self.llm = None
        self.sampling_params = SamplingParams(**sampling_args)

        self._build_prompt_template()

    def _build_prompt_template(self):
        """Constructs the few-shot prompt for the language model."""
        examples_data = [
            {
                "input": "Compare the performance of the GeForce RTX 4090 and the Radeon RX 7900 XTX in Cyberpunk 2077.",
                "output": "GeForce RTX 4090\nRadeon RX 7900 XTX\nCyberpunk 2077\nperformance",
            },
            {
                "input": "Who was the prime minister of the United Kingdom during the Falklands War?",
                "output": "prime minister\nUnited Kingdom\nFalklands War",
            },
            {
                "input": "How do I implement a thread-safe singleton pattern in Java?",
                "output": "thread-safe\nsingleton pattern\nJava",
            },
        ]

        self.formatted_examples = []
        for ex in examples_data:
            self.formatted_examples.extend(
                [
                    {"role": "user", "content": ex["input"]},
                    {"role": "assistant", "content": ex["output"]},
                ]
            )

        system_prompt = """You are an expert at keyword extraction. Your goal is to identify and list the most important terms, entities, and concepts from the user's question. These keywords should be suitable for a search engine.

Follow these rules:
1.  **Extract Core Concepts:** Identify the main subjects of the query.
2.  **Identify Named Entities:** Extract all specific names of people, organizations, products, locations, etc.
3.  **Preserve Multi-Word Terms:** Keep phrases like "machine learning" or "GeForce RTX 4090" together.
4.  **Be Concise:** Do not include stopwords (like 'the', 'is', 'a') or generic verbs (like 'compare', 'find') unless they are part of a key phrase.
5.  ** This is the most inportant part, since the keywords you provide will be used for retriveing values from a database, make sure that they are exhaustive, meaning that you provide all the keywords from the query that might actually be a database value, so perhaps for a keyword with multiuple tokens you could also return the ngrams of it if you think that these could be database values on their own.

Respond ONLY with the list of keywords, each on a new line. Do NOT include any introduction, explanation, numbering, or bullet points."""

        self.system_message = {"role": "system", "content": system_prompt}

    def _load_cache(self) -> Dict[str, List[str]]:
        if self.cache_file and os.path.exists(self.cache_file):
            with open(self.cache_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return {}

    def _save_cache(self):
        if self.cache_file and self.keywords_cache is not None:
            with open(self.cache_file, "w", encoding="utf-8") as f:
                json.dump(self.keywords_cache, f, indent=2, ensure_ascii=False)

    def _parse_llm_output(self, raw_output: str) -> List[str]:
        """Cleans and parses the raw text output from the LLM into a list of keywords."""
        keywords = []
        if raw_output:
            lines = raw_output.strip().split("\n")
            for line in lines:
                cleaned_line = re.sub(r"^\s*[-*]?\s*\d*\.\s*", "", line).strip()
                if cleaned_line:
                    keywords.append(cleaned_line)
        return keywords

    def process(self, nlqs: List[str]) -> List[List[str]]:
        """Extracts keywords from a batch of natural language queries."""
        if not nlqs:
            return []

        final_results: List[Optional[List[str]]] = [None] * len(nlqs)
        prompts_to_generate, indices_to_generate, nlqs_to_generate = [], [], []

        for i, nlq in enumerate(
            tqdm(nlqs, desc="Preparing keyword extraction prompts")
        ):
            if self.keywords_cache is not None and nlq in self.keywords_cache:
                final_results[i] = self.keywords_cache[nlq]
                continue
            if self.tokenizer is None:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name_or_path,
                    trust_remote_code=self.vllm_args["trust_remote_code"],
                    cache_dir=self.cache_dir,
                )
            if self.llm is None:
                self.llm = LLM(
                    model=self.model_name_or_path,
                    tensor_parallel_size=self.tensor_parallel_size,
                    gpu_memory_utilization=self.gpu_memory_utilization,
                    download_dir=self.cache_dir,
                    **self.vllm_args,
                )
            messages = (
                [self.system_message]
                + self.formatted_examples
                + [{"role": "user", "content": nlq}]
            )
            prompt_text = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            prompts_to_generate.append(prompt_text)
            indices_to_generate.append(i)
            nlqs_to_generate.append(nlq)

        if prompts_to_generate:
            vllm_outputs = self.llm.generate(prompts_to_generate, self.sampling_params)

            for i, output in enumerate(tqdm(vllm_outputs, desc="Extracting keywords")):
                original_nlq_index = indices_to_generate[i]
                current_nlq = nlqs_to_generate[i]
                raw_output = output.outputs[0].text
                extracted_keywords = self._parse_llm_output(raw_output)

                final_results[original_nlq_index] = extracted_keywords

                if self.keywords_cache is not None:
                    self.keywords_cache[current_nlq] = extracted_keywords

        if self.keywords_cache is not None and prompts_to_generate:
            self._save_cache()

        # Ensure the final output is a list of lists, with empty lists for failures.
        return [res if res is not None else [] for res in final_results]

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/ner_query_processor.py
# EXT: .py, SIZE: 1935 bytes, SHA256: 9e8922faf78d1d2b716c1d9287be4274d8f7a283e29e505c3574a2e02615ba23

import string
from typing import List

import spacy
from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)
from nltk import pos_tag, word_tokenize
from nltk.corpus import stopwords
from tqdm.auto import tqdm


class NERQueryProcessor(BaseUserQueryProcessor):
    """
    A query processor using Named Entity Recognition (NER) and Part-of-Speech (POS) tagging.

    This processor combines spaCy's NER with NLTK's POS tagging to identify
    potential keywords, filtering for nouns, proper nouns, and adjectives while
    removing common stopwords.
    """

    def __init__(self, spacy_model: str = "en_core_web_sm"):
        """
        Args:
            spacy_model (str): The name of the spaCy model to use for NER.
        """
        self.nlp = spacy.load(spacy_model)
        self.stop_words = set(stopwords.words("english"))
        self.punctuation = set(string.punctuation)

    def _extract_ner_and_pos(self, text: str) -> List[str]:
        """Extracts entities and POS-filtered keywords for a single text."""
        # NER extraction
        doc = self.nlp(text)
        entities = [ent.text for ent in doc.ents]

        # POS-based keyword extraction
        tokens = word_tokenize(text)
        pos_tags = pos_tag(tokens)
        keywords = [
            word
            for word, pos in pos_tags
            if pos
            in ["NN", "NNS", "NNP", "NNPS", "JJ"]  # Nouns, Proper Nouns, Adjectives
            and word.lower() not in self.stop_words
            and word not in self.punctuation
        ]
        return list(set(entities + keywords))

    def process(self, nlqs: List[str]) -> List[List[str]]:
        """
        Processes a batch of queries using NER and POS-tagging.
        """
        results = []
        for nlq in tqdm(nlqs, desc="Extracting NER & POS Keywords"):
            results.append(self._extract_ner_and_pos(nlq))
        return results

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/ngram_processor.py
# EXT: .py, SIZE: 1357 bytes, SHA256: ceb4c9c23b22f6a8772f74c44c958b5ca83948a05b9801545b29d2e32042987f

from typing import List

from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)
from nltk import word_tokenize
from nltk.util import ngrams
from tqdm.auto import tqdm


class NgramQueryProcessor(BaseUserQueryProcessor):
    """
    A query processor that breaks down queries into n-grams.

    This processor generates all possible n-grams (from 1 to n) for each query,
    which can be useful for capturing multi-word entities in a simple,
    non-semantic way.
    """

    def __init__(self, n: int = 4):
        """
        Args:
            n (int): The maximum size of the n-grams to generate.
        """
        self.n = n

    def _extract_ngrams(self, text: str) -> List[str]:
        """Extracts n-grams for a single text string."""
        tokens = word_tokenize(text)
        all_ngrams = []
        for i in range(1, self.n + 1):
            n_grams_for_i = [" ".join(ngram) for ngram in ngrams(tokens, i)]
            all_ngrams.extend(n_grams_for_i)
        return list(set(all_ngrams))

    def process(self, nlqs: List[str]) -> List[List[str]]:
        """
        Processes a batch of queries by generating n-grams for each.
        """
        results = []
        for nlq in tqdm(nlqs, desc="Generating N-grams"):
            results.append(self._extract_ngrams(nlq))
        return results

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/passthrough_processor.py
# EXT: .py, SIZE: 776 bytes, SHA256: 75344d9b9d16ea7341939aa82affa7737832deaaa01e173042045f5485b8eecf

from typing import List

from nlp_retrieval.user_query_processors.query_processor_abc import (
    BaseUserQueryProcessor,
)


class PassthroughQueryProcessor(BaseUserQueryProcessor):
    """

    A simple query processor that does not modify the original queries.

    It simply wraps each query in a list, conforming to the required
    `List[List[str]]` output format.
    """

    def process(self, nlqs: List[str]) -> List[List[str]]:
        """
        Wraps each query in a list.
        Args:
            nlqs: A list of raw natural language query strings.

        Returns:
            A list of lists, where each inner list contains only the original query.
            Example: `["q1", "q2"] -> [["q1"], ["q2"]]`
        """
        return [[nlq] for nlq in nlqs]

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/user_query_processors/query_processor_abc.py
# EXT: .py, SIZE: 746 bytes, SHA256: c62b7ee96346efd1aaa577f1ea81a7d5b50daa09831eb460ec0c901b9e7b900f

from abc import ABC, abstractmethod
from typing import List


class BaseUserQueryProcessor(ABC):
    """
    Abstract Base Class for user query processors.

    Take a batch of raw natural language queries and transform them
    into a format suitable for the retriever
    """

    @abstractmethod
    def process(self, nlqs: List[str]) -> List[List[str]]:
        """
        Processes a batch of natural language queries.

        Args:
            nlqs: A list of raw natural language query strings.

        Returns:
            A list of lists, where each inner list contains the processed
            query strings (e.g., one or more sub-queries or extracted keywords)
            for the corresponding input NLQ.
        """
        pass

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/bge_cross_encoder_reranker.py
# EXT: .py, SIZE: 3768 bytes, SHA256: 14d974a3363f14136cd9b6b30b7ec4f14bb1d58dad3f1b11e5867ea102e0ae1e

from typing import List, Union

import torch
from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from FlagEmbedding import FlagLLMReranker, FlagReranker
from tqdm.auto import tqdm


class BgeReranker(BaseReranker):
    """
    A reranker that uses the BGE (BAAI General Embedding) reranker models.

    This class is a wrapper around the `FlagEmbedding` library, supporting both
    standard cross-encoders (like `bge-reranker-v2-m3`) and LLM-based rerankers
    (like `bge-reranker-v2-gemma`).
    """

    def __init__(
        self,
        model_name: str = "BAAI/bge-reranker-v2-m3",
        use_fp16: bool = True,
        normalize: bool = True,
        device: Union[str, int] = "auto",
    ):
        """
        Initializes the BgeReranker.

        Args:
            model_name: The name of the BGE reranker model from Hugging Face.
            use_fp16: Whether to use float16 precision for faster inference.
                      Defaults to True.
            normalize: Whether to normalize scores to a [0, 1] range using a
                       sigmoid function. Defaults to True.
            device: The device to run the model on (e.g., "auto", "cpu", "cuda:0").
        """
        self.model_name = model_name
        self.normalize = normalize

        # The 'gemma' model requires the LLM-specific reranker class
        if "gemma" in model_name.lower() or "minicpm" in model_name.lower():
            self.reranker = FlagLLMReranker(model_name, use_fp16=use_fp16)
        else:
            self.reranker = FlagReranker(model_name, use_fp16=use_fp16)
        if device != "auto" and hasattr(self.reranker, "model"):
            self.reranker.model.to(device)

    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        """
        Reranks candidate results for a batch of queries using the BGE model.
        """
        final_batches = []
        progress_bar_desc = f"Reranking with {self.model_name}"

        for nlq, candidate_list in tqdm(
            zip(nlqs, results_batch),
            total=len(nlqs),
            desc=progress_bar_desc,
            disable=len(nlqs) < 5,
        ):
            if not candidate_list:
                final_batches.append([])
                continue

            # Prepare pairs of [query, document_content] for the reranker
            pairs_to_score = [[nlq, res.item.content] for res in candidate_list]

            try:
                # The reranker computes scores for all pairs in a single batch
                scores = self.reranker.compute_score(
                    pairs_to_score, normalize=self.normalize
                )

                # Create new RetrievalResult objects with the updated scores
                rescored_results = [
                    RetrievalResult(item=orig_res.item, score=float(score))
                    for orig_res, score in zip(candidate_list, scores)
                ]

                # Sort by the new scores and truncate to the top-k
                sorted_results = sorted(
                    rescored_results, key=lambda r: r.score, reverse=True
                )
                final_batches.append(sorted_results[:k])

            except Exception as e:
                print(
                    f"ERROR during BGE reranking for query '{nlq[:50]}...': {e}. "
                    "Falling back to initial results."
                )

                sorted_candidates = sorted(
                    candidate_list, key=lambda r: r.score, reverse=True
                )
                final_batches.append(sorted_candidates[:k])

            torch.cuda.empty_cache()

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/bi_encoder_reranker.py
# EXT: .py, SIZE: 2946 bytes, SHA256: a33ddbc69261f31f1cdbec881a9d7b43c115d7ebb75b4959106ee7af95876be5

from typing import List

import torch
import torch.nn.functional as F
from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from sentence_transformers import SentenceTransformer
from tqdm.auto import tqdm


class BiEncoderReranker(BaseReranker):
    """
    A reranker that uses a bi-encoder model to re-score candidates.

    This component independently encodes the query and each candidate document,
    then computes their cosine similarity
    """

    def __init__(
        self,
        model_name: str = "BAAI/bge-m3",
        batch_size: int = 32,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
    ):
        """
        Initializes the BiEncoderReranker.

        Args:
            model_name: The name of the sentence-transformer model to use.
            batch_size: The batch size for encoding documents.
            device: The device to run the model on (e.g., "cuda" or "cpu").
        """
        self.batch_size = batch_size
        self.model = SentenceTransformer(model_name, device=device)

    def _cosine_similarity(
        self, query_emb: torch.Tensor, doc_embs: torch.Tensor
    ) -> torch.Tensor:
        """Computes cosine similarity between a single query and multiple docs."""
        query_emb = F.normalize(query_emb, p=2, dim=-1)
        doc_embs = F.normalize(doc_embs, p=2, dim=-1)
        # Squeeze to convert shape [1, N] to [N]
        return torch.mm(query_emb, doc_embs.T).squeeze(0)

    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        final_batches = []
        for nlq, result_list in tqdm(
            zip(nlqs, results_batch),
            total=len(nlqs),
            desc="Reranking with BiEncoder",
            disable=len(nlqs) < 5,
        ):
            if not result_list:
                final_batches.append([])
                continue

            query_to_embed = [f"search_query: {nlq}"]
            docs_to_embed = [
                f"search_document: {res.item.content}" for res in result_list
            ]

            with torch.no_grad():
                query_emb = self.model.encode(query_to_embed, convert_to_tensor=True)
                doc_embs = self.model.encode(
                    docs_to_embed, batch_size=self.batch_size, convert_to_tensor=True
                )

            similarities = self._cosine_similarity(query_emb, doc_embs)

            rescored_results = [
                RetrievalResult(item=orig_res.item, score=float(score))
                for orig_res, score in zip(result_list, similarities)
            ]

            # Sort by the newly computed scores and truncate
            sorted_results = sorted(
                rescored_results, key=lambda r: r.score, reverse=True
            )
            final_batches.append(sorted_results[:k])

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/bridge_reranker.py
# EXT: .py, SIZE: 10849 bytes, SHA256: 45aecc0e90fff71033213525f120537606b23bf302053898fea1ec8ec1d65b51

import difflib
from typing import List, Optional, Tuple

from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from rapidfuzz import fuzz
from tqdm.auto import tqdm


class _Match:
    """A simple helper class to store the start and size of a string match."""

    def __init__(self, start: int, size: int) -> None:
        self.start = start
        self.size = size


class BridgeReranker(BaseReranker):
    """
    A reranker based on the string-matching logic from the BRIDGE system.

    This component re-scores candidate items by performing a complex, rule-based
    fuzzy string matching between the natural language query and the content of
    each candidate. It is not a neural reranker but is effective for cases
    where exact or near-exact string presence is a strong signal of relevance.
    """

    def __init__(
        self,
        m_theta: float = 0.85,
        s_theta: float = 0.85,
        include_substrings: bool = False,
    ):
        """
        Initializes the BridgeReranker.

        Args:
            m_theta: The matching threshold for the primary match.
            s_theta: The matching threshold for the secondary match.
            include_substrings: If True, grants a max score if the query is a
                                substring of the item content (case-insensitive).
        """
        self.include_substrings = include_substrings
        self.m_theta = m_theta
        self.s_theta = s_theta
        self._stopwords = {
            "a",
            "about",
            "above",
            "after",
            "again",
            "against",
            "all",
            "am",
            "an",
            "and",
            "any",
            "are",
            "aren't",
            "as",
            "at",
            "be",
            "because",
            "been",
            "before",
            "being",
            "below",
            "between",
            "both",
            "but",
            "by",
            "can't",
            "cannot",
            "could",
            "couldn't",
            "did",
            "didn't",
            "do",
            "does",
            "doesn't",
            "doing",
            "don't",
            "down",
            "during",
            "each",
            "few",
            "for",
            "from",
            "further",
            "had",
            "hadn't",
            "has",
            "hasn't",
            "have",
            "haven't",
            "having",
            "he",
            "he'd",
            "he'll",
            "he's",
            "her",
            "here",
            "here's",
            "hers",
            "herself",
            "him",
            "himself",
            "his",
            "how",
            "how's",
            "i",
            "i'd",
            "i'll",
            "i'm",
            "i've",
            "if",
            "in",
            "into",
            "is",
            "isn't",
            "it",
            "it's",
            "its",
            "itself",
            "let's",
            "me",
            "more",
            "most",
            "mustn't",
            "my",
            "myself",
            "no",
            "nor",
            "not",
            "of",
            "off",
            "on",
            "once",
            "only",
            "or",
            "other",
            "ought",
            "our",
            "ours",
            "ourselves",
            "out",
            "over",
            "own",
            "same",
            "shan't",
            "she",
            "she'd",
            "she'll",
            "she's",
            "should",
            "shouldn't",
            "so",
            "some",
            "such",
            "than",
            "that",
            "that's",
            "the",
            "their",
            "theirs",
            "them",
            "themselves",
            "then",
            "there",
            "there's",
            "these",
            "they",
            "they'd",
            "they'll",
            "they're",
            "they've",
            "this",
            "those",
            "through",
            "to",
            "too",
            "under",
            "until",
            "up",
            "very",
            "was",
            "wasn't",
            "we",
            "we'd",
            "we'll",
            "we're",
            "we've",
            "were",
            "weren't",
            "what",
            "what's",
            "when",
            "when's",
            "where",
            "where's",
            "which",
            "while",
            "who",
            "who's",
            "whom",
            "why",
            "why's",
            "with",
            "won't",
            "would",
            "wouldn't",
            "you",
            "you'd",
            "you'll",
            "you're",
            "you've",
            "your",
            "yours",
            "yourself",
            "yourselves",
        }
        self._commonwords = {"no", "yes", "many"}
        self._common_db_terms = {"id"}

    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        final_batches = []
        for nlq, result_list in tqdm(
            zip(nlqs, results_batch),
            total=len(nlqs),
            desc="Reranking with BridgeReranker",
            disable=len(nlqs) < 5,
        ):
            if not result_list:
                final_batches.append([])
                continue

            rescored_results = []
            for res in result_list:
                item_content = res.item.content
                new_score = 0.0

                if self.include_substrings and nlq.lower() in item_content.lower():
                    new_score = 1.0
                else:
                    matched_entries = self._get_matched_entries(nlq, [item_content])
                    if matched_entries:
                        # Extract the match_score from the first (best) match
                        _, (_, _, match_score, _, _) = matched_entries[0]
                        new_score = match_score

                rescored_results.append(RetrievalResult(item=res.item, score=new_score))

            # Sort by the newly computed scores and truncate
            sorted_results = sorted(
                rescored_results, key=lambda r: r.score, reverse=True
            )
            final_batches.append(sorted_results[:k])

        return final_batches

    def _is_span_separator(self, c: str) -> bool:
        return c in "'\"()`,.?! "

    def _split_to_chars(self, s: str) -> List[str]:
        return [c.lower() for c in s.strip()]

    def _prefix_match(self, s1: str, s2: str) -> bool:
        i, j = 0, 0
        while i < len(s1) and self._is_span_separator(s1[i]):
            i += 1
        while j < len(s2) and self._is_span_separator(s2[j]):
            j += 1

        if i < len(s1) and j < len(s2):
            return s1[i] == s2[j]
        return i >= len(s1) and j >= len(s2)

    def _get_effective_match_source(
        self, s: str, start: int, end: int
    ) -> Optional[_Match]:
        _start = -1
        for i in range(start, start - 2, -1):
            if i < 0 or self._is_span_separator(s[i]):
                _start = i + 1 if i < 0 else i
                break
        if _start < 0:
            return None

        _end = -1
        for i in range(end - 1, end + 3):
            if i >= len(s) or self._is_span_separator(s[i]):
                _end = i - 1 if i >= len(s) else i
                break
        if _end < 0:
            return None

        while _start < len(s) and self._is_span_separator(s[_start]):
            _start += 1
        while _end >= 0 and self._is_span_separator(s[_end]):
            _end -= 1

        return _Match(_start, _end - _start + 1)

    def _get_matched_entries(
        self, s: str, field_values: List[str]
    ) -> Optional[List[Tuple[str, Tuple[str, str, float, float, int]]]]:
        n_grams = self._split_to_chars(s)
        matched = {}

        for field_value in field_values:
            if not isinstance(field_value, str):
                continue

            fv_tokens = self._split_to_chars(field_value)
            sm = difflib.SequenceMatcher(None, n_grams, fv_tokens)
            match = sm.find_longest_match(0, len(n_grams), 0, len(fv_tokens))

            if match.size > 0:
                source_match = self._get_effective_match_source(
                    n_grams, match.a, match.a + match.size
                )
                if not source_match:
                    continue

                match_str = field_value[match.b : match.b + match.size]
                source_match_str = s[
                    source_match.start : source_match.start + source_match.size
                ]

                c_match_str = match_str.lower().strip()
                c_source_match_str = source_match_str.lower().strip()
                c_field_value = field_value.lower().strip()

                if not c_match_str or c_match_str in self._common_db_terms:
                    continue
                if (
                    c_match_str in self._stopwords
                    or c_source_match_str in self._stopwords
                    or c_field_value in self._stopwords
                ):
                    continue

                if c_source_match_str.endswith(c_match_str + "'s"):
                    match_score = 1.0
                elif self._prefix_match(c_field_value, c_source_match_str):
                    match_score = fuzz.ratio(c_field_value, c_source_match_str) / 100.0
                else:
                    match_score = 0.0

                is_common = (
                    c_match_str in self._commonwords
                    or c_source_match_str in self._commonwords
                    or c_field_value in self._commonwords
                )
                if is_common and match_score < 1.0:
                    continue

                s_match_score = match_score
                if match_score >= self.m_theta and s_match_score >= self.s_theta:
                    if field_value.isupper() and (match_score * s_match_score) < 1.0:
                        continue
                    matched[match_str] = (
                        field_value,
                        source_match_str,
                        match_score,
                        s_match_score,
                        match.size,
                    )

        if not matched:
            return None
        return sorted(
            matched.items(),
            key=lambda x: (1e16 * x[1][2] + 1e8 * x[1][3] + x[1][4]),
            reverse=True,
        )

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/llm_reranker.py
# EXT: .py, SIZE: 7872 bytes, SHA256: 7415c8f911917c867d9fa85d119c179799e3604bc73944cc9bb2efc6a071a727

import re
from typing import Dict, List, Optional

from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from tqdm.auto import tqdm
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams


class LLMReranker(BaseReranker):
    """
    A reranker with a Large Language Model (LLM) via VLLM for re-scoring.

    This reranker constructs a detailed prompt for each query, presenting the candidate
    documents with temporary integer IDs. It uses few-shot examples to instruct the
    LLM to return a sorted list of the IDs corresponding to the most relevant documents.
    """

    def __init__(
        self,
        model_name_or_path: str,
        tensor_parallel_size: int = 1,
        gpu_memory_utilization: float = 0.85,
        cache_dir="/data/hdd1/vllm_models/",
        **kwargs,
    ):
        """
        Initializes the VLLM-based reranker.

        Args:
            model_name_or_path: The name or path of the model to be loaded by VLLM.
            tensor_parallel_size: The number of GPUs to use for tensor parallelism.
            gpu_memory_utilization: The fraction of GPU memory to reserve for the model.
            **kwargs: Additional arguments for VLLM's LLM class (e.g., quantization)
                      or SamplingParams (e.g., temperature, max_tokens).
        """
        self.model_name_or_path = model_name_or_path

        vllm_args = {
            "trust_remote_code": kwargs.pop("trust_remote_code", True),
            "max_model_len": kwargs.pop("max_model_len", 32768),
            "quantization": kwargs.pop("quantization", None),
        }
        sampling_args = {
            "temperature": kwargs.pop("temperature", 0.0),
            "top_p": kwargs.pop("top_p", 1.0),
            "max_tokens": kwargs.pop("max_tokens", 1024),
        }

        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name_or_path,
            trust_remote_code=vllm_args["trust_remote_code"],
            cache_dir=cache_dir,
        )
        self.llm = LLM(
            model=self.model_name_or_path,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            download_dir=cache_dir,
            **vllm_args,
        )
        self.sampling_params = SamplingParams(**sampling_args)

        self._build_prompt_components()

    def _build_prompt_components(self):
        """Constructs the static parts of the prompt (system message and few-shot examples)."""
        self.system_message = {
            "role": "system",
            "content": "You are an expert search result reranker. Your task is to analyze a user's query and a list of retrieved documents. You must identify the most relevant documents that directly answer the query.\n\nRespond ONLY with a comma-separated list of the integer IDs, sorted from most relevant to least relevant. Do not include any explanation, preamble, or formatting.",
        }

        # Few-shot examples to guide the model's output format and reasoning
        few_shot_user_1 = """User Query: "What is the capital of France?"

Documents:
[1] Paris is the capital and most populous city of France.
[2] The Eiffel Tower is a famous landmark in Paris.
[3] Berlin is the capital of Germany.

Instructions:
Identify the top 2 most relevant documents and respond with their IDs."""

        few_shot_assistant_1 = "1,2"

        few_shot_user_2 = """User Query: "Which GPU is better for gaming, the RTX 4090 or the RX 7900 XTX?"

Documents:
[1] The Nvidia RTX 4090 offers top-tier performance in 4K gaming.
[2] A detailed review concludes that the RTX 4090 has a slight edge in most games, while the 7900 XTX offers better value.
[3] The AMD RX 7900 XTX provides excellent rasterization performance for modern titles.
[4] The RTX 3060 is a popular mid-range GPU from the previous generation.

Instructions:
Identify the top 3 most relevant documents and respond with their IDs."""

        few_shot_assistant_2 = "2,1,3"

        self.few_shot_examples = [
            {"role": "user", "content": few_shot_user_1},
            {"role": "assistant", "content": few_shot_assistant_1},
            {"role": "user", "content": few_shot_user_2},
            {"role": "assistant", "content": few_shot_assistant_2},
        ]

    def _parse_llm_output(self, raw_output: str, max_id: int) -> List[int]:
        """Cleans and parses the raw text output from the LLM into a list of integer IDs."""
        try:
            # Find all numbers in the string
            ids_str = re.findall(r"\d+", raw_output)
            # Convert to integers and filter out any invalid IDs
            parsed_ids = [
                int(id_str) for id_str in ids_str if 1 <= int(id_str) <= max_id
            ]
            return parsed_ids
        except (ValueError, TypeError):
            return []

    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        if not nlqs:
            return []

        prompts_to_generate: List[str] = []
        result_maps_for_batch: List[Dict[int, RetrievalResult]] = []
        indices_to_process: List[int] = []
        final_results_batch: List[Optional[List[RetrievalResult]]] = [None] * len(nlqs)

        # --- 1. Prepare prompts for all queries in the batch ---
        for i, (nlq, candidate_list) in enumerate(
            tqdm(
                zip(nlqs, results_batch),
                total=len(nlqs),
                desc="Preparing VLLM Reranking Prompts",
            )
        ):
            if not candidate_list:
                final_results_batch[i] = []
                continue

            id_to_result_map = {idx + 1: res for idx, res in enumerate(candidate_list)}
            formatted_docs = "\n".join(
                f"[{doc_id}] {res.item.content}"
                for doc_id, res in id_to_result_map.items()
            )

            # Construct the final user prompt for the current query
            final_user_prompt = f"""User Query: "{nlq}"

Documents:
{formatted_docs}

Instructions:
Identify the top {k} most relevant documents and respond with their IDs."""

            # Combine system message, few-shot examples, and the final user prompt
            messages = (
                [self.system_message]
                + self.few_shot_examples
                + [{"role": "user", "content": final_user_prompt}]
            )

            chat_prompt = self.tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )

            prompts_to_generate.append(chat_prompt)
            result_maps_for_batch.append(id_to_result_map)
            indices_to_process.append(i)

        # --- 2. Generate responses from VLLM in a single batch call ---
        if prompts_to_generate:
            vllm_outputs = self.llm.generate(prompts_to_generate, self.sampling_params)

            # --- 3. Process the batch results ---
            for i, output in enumerate(tqdm(vllm_outputs, desc="Reranking with VLLM")):
                original_batch_idx = indices_to_process[i]
                id_map = result_maps_for_batch[i]
                raw_output = output.outputs[0].text

                reranked_ids = self._parse_llm_output(raw_output, max_id=len(id_map))

                reranked_results = [
                    id_map[doc_id] for doc_id in reranked_ids if doc_id in id_map
                ]

                # Assign new scores based on the rank from the LLM for consistency
                for rank, res in enumerate(reranked_results):
                    res.score = 1.0 / (rank + 1)

                final_results_batch[original_batch_idx] = reranked_results[:k]

        return [res if res is not None else [] for res in final_results_batch]

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/mxbai_cross_encoder_reranker.py
# EXT: .py, SIZE: 3665 bytes, SHA256: 9ce3a5be460441f66e5b2f79a06b9d26de561f6fde5a533689b3150934242088

from typing import Dict, List

import torch
from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from mxbai_rerank import MxbaiRerankV2
from tqdm.auto import tqdm


class MxbaiCrossEncoderReranker(BaseReranker):
    """
    A reranker using a cross-encoder model from Mixedbread AI.

    This component uses a `MxbaiRerankV2` model to re-score a list of candidate
    documents against a query
    """

    def __init__(
        self,
        model_name: str = "mixedbread-ai/mxbai-rerank-large-v2",
        batch_size: int = 16,
        device_map: str | int = "cuda:0",
    ):
        """
        Initializes the MxbaiCrossEncoderReranker.

        Args:
            model_name: The name or path of the mixedbread-ai reranker model.
            batch_size: The batch size to use during the reranking process.
            device_map: The device to run the model on
        """
        self.reranker = MxbaiRerankV2(model_name, device_map=device_map)
        self.batch_size = batch_size
        self._model_name = model_name

    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        """
        Reranks candidate results for a batch of queries using the Mxbai model.
        """
        final_batches = []
        progress_bar_desc = f"Reranking with {self._model_name}"

        for nlq, candidate_list in tqdm(
            zip(nlqs, results_batch),
            total=len(nlqs),
            desc=progress_bar_desc,
            disable=len(nlqs) < 5,
        ):
            if not candidate_list:
                final_batches.append([])
                continue

            # Prepare documents and a map to retrieve original metadata later
            docs_to_rerank = [res.item.content for res in candidate_list]

            # This map allows us to recover the full SearchableItem after reranking,
            # as the reranker only returns the text content.
            original_results_map: Dict[str, RetrievalResult] = {
                res.item.content: res for res in candidate_list
            }

            try:
                reranked_output = self.reranker.rank(
                    query=nlq,
                    documents=docs_to_rerank,
                    top_k=k,
                    return_documents=True,
                    batch_size=self.batch_size,
                )

                reranked_results: List[RetrievalResult] = []
                for reranked_item in reranked_output:
                    # Look up the original result to preserve its full item and metadata
                    original_result = original_results_map.get(reranked_item.document)
                    if original_result:
                        reranked_results.append(
                            RetrievalResult(
                                item=original_result.item,
                                score=float(reranked_item.score),
                            )
                        )
                final_batches.append(reranked_results)

            except Exception as e:
                # In case of an error, fall back to the original retriever's ranking
                print(
                    f"ERROR during reranking for query '{nlq[:50]}...': {e}. "
                    "Falling back to initial results."
                )

                sorted_candidates = sorted(
                    candidate_list, key=lambda r: r.score, reverse=True
                )
                final_batches.append(sorted_candidates[:k])

            torch.cuda.empty_cache()

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/reranker_abc.py
# EXT: .py, SIZE: 1043 bytes, SHA256: 63e3898def8811b764579ed5980c72a87a030a0dc379f355ee73bb37f4d790b2

from abc import ABC, abstractmethod
from typing import List

from nlp_retrieval.core.models import RetrievalResult


class BaseReranker(ABC):
    """
    Abstract Base Class for reranking models.

    Rerankers take an initial list of candidate results from a retriever and
    re-score them
    """

    @abstractmethod
    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        """
        Reranks a batch of retrieval results against their original queries.

        Args:
            nlqs: The list of original natural language queries.
            results_batch: A list where each inner list contains the candidate
                           `RetrievalResult` objects for the corresponding NLQ.
            k: The final number of results to return for each query after reranking.

        Returns:
            A list of lists, where each inner list contains the final, reranked
            and sorted `RetrievalResult` objects.
        """
        pass

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/rerankers/sentence_transformer_reranker.py
# EXT: .py, SIZE: 3560 bytes, SHA256: 0ec711829c6bafc8accba1fcc84867d992f1cbda7cb187d4e5c4aa5df291c86e

from typing import List

import torch
from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.rerankers.reranker_abc import BaseReranker
from sentence_transformers import CrossEncoder
from tqdm.auto import tqdm


class SentenceTransformerCrossEncoderReranker(BaseReranker):
    """
    A reranker that uses a Cross-Encoder model from the sentence-transformers library.

    This component re-scores candidate documents by passing the query and document
    text through the model simultaneously
    """

    def __init__(
        self,
        model_name: str = "zeroentropy/zerank-1",
        batch_size: int = 1,
        device: str = "cuda",
        trust_remote_code: bool = False,
    ):
        """
        Initializes the SentenceTransformerCrossEncoderReranker.

        Args:
            model_name: The name of the Cross-Encoder model from Hugging Face.
            batch_size: The batch size for the prediction call.
            device: The device to run the model on (e.g., 'cpu', 'cuda').
                    If None, sentence-transformers will auto-detect.
            trust_remote_code: Whether to trust remote code when loading the model.
                               Set to True for models like 'zerank-1'.
        """
        self.model_name = model_name
        self.batch_size = batch_size
        self.model = CrossEncoder(
            model_name, device=device, trust_remote_code=trust_remote_code
        )
        self.device = device

    def rerank(
        self, nlqs: List[str], results_batch: List[List[RetrievalResult]], k: int
    ) -> List[List[RetrievalResult]]:
        """
        Reranks candidate results for a batch of queries using a Cross-Encoder.
        """
        final_batches = []
        progress_bar_desc = f"Reranking with {self.model_name}"

        for nlq, candidate_list in tqdm(
            zip(nlqs, results_batch),
            total=len(nlqs),
            desc=progress_bar_desc,
            disable=len(nlqs) < 5,
        ):
            if not candidate_list:
                final_batches.append([])
                continue

            pairs_to_score = [(nlq, res.item.content) for res in candidate_list]

            try:
                # The `predict` method handles batching internally
                scores = self.model.predict(
                    pairs_to_score,
                    batch_size=self.batch_size,
                    show_progress_bar=False,  # tqdm handles the outer loop
                )

                # Create new RetrievalResult objects with the updated scores
                rescored_results = [
                    RetrievalResult(item=orig_res.item, score=float(score))
                    for orig_res, score in zip(candidate_list, scores)
                ]

                # Sort by the new scores and truncate to the top-k
                sorted_results = sorted(
                    rescored_results, key=lambda r: r.score, reverse=True
                )
                final_batches.append(sorted_results[:k])

            except Exception as e:
                print(
                    f"ERROR during Cross-Encoder reranking for query '{nlq[:50]}...': {e}. "
                    "Falling back to initial results."
                )

                sorted_candidates = sorted(
                    candidate_list, key=lambda r: r.score, reverse=True
                )
                final_batches.append(sorted_candidates[:k])

            if self.device == "cuda":
                torch.cuda.empty_cache()

        return final_batches

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/core/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/core/models.py
# EXT: .py, SIZE: 1325 bytes, SHA256: 71ddcdeed12eeb87d68b260aa29c914c254a80ddb5f97a7e6b5ed3a698374bf5

import uuid
from typing import Any, Dict

from pydantic import BaseModel, Field


def _generate_uuid_str() -> str:
    return str(uuid.uuid4())


class SearchableItem(BaseModel):
    """
    A standard representation of a single item to be indexed and retrieved.

    This is the core data object that flows into the indexing part of the pipeline.
    """

    item_id: str = Field(
        default_factory=_generate_uuid_str,
        description="A unique identifier for the item. If not provided, a UUID will be generated.",
    )
    content: str = Field(
        description="The main text content of the item that will be used for searching."
    )
    metadata: Dict[str, Any] = Field(
        default_factory=dict,
        description="A dictionary to hold any additional metadata associated with the item.",
    )


class RetrievalResult(BaseModel):
    """
    A standard representation of a single retrieval result.

    Consists of a retrieved item (SearchableItem) and its
    associated relevance score.
    """

    item: SearchableItem
    score: float

    def __hash__(self) -> int:
        return hash(self.item.item_id)

    def __eq__(self, other: object) -> bool:
        if not isinstance(other, RetrievalResult):
            return NotImplemented
        return self.item.item_id == other.item.item_id

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/loaders/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/loaders/database_loader.py
# EXT: .py, SIZE: 6328 bytes, SHA256: 08a24ceaf0d24e6043c1d09aaa476fb88cc99b0447e4eb8439aa3ba216f2ffed

import hashlib
from enum import Enum
from typing import List, Union

from nlp_retrieval.core.models import SearchableItem
from nlp_retrieval.loaders.loader_abc import BaseLoader
from utils_database_connector.core import Database
from utils_database_connector.sqlite_db import DatabaseSqlite
from tqdm import tqdm


class SerializationStrategy(Enum):
    """Defines the different ways a database can be serialized into text items."""

    SCHEMA_LEVEL = "schema_level"
    ROW_LEVEL = "row_level"
    VALUE_LEVEL_SCHEMA_AWARE = "value_level_schema_aware"  # values with schema context
    VALUE_LEVEL = "value_level"


class DatabaseLoader(BaseLoader):
    """
    Loads data from a database connection and serializes it into `SearchableItem`s.

    Supports multiple serialization strategies to structure the database content
    for different retrieval tasks.
    """

    def __init__(
        self,
        db: Union[Database, DatabaseSqlite],
        strategy: SerializationStrategy,
        table_separator: str = " <table> ",
        column_separator: str = " <col> ",
        value_separator: str = " <val> ",
    ):
        """
        Initializes the DatabaseLoader.

        Args:
            db: An active database connection object.
            strategy: The serialization strategy to use.
            table_separator: The token to use for separating table names.
            column_separator: The token to use for separating column names.
            value_separator: The token to use for separating cell values.
        """
        self.db = db
        self.strategy = strategy
        self.table_sep = table_separator
        self.col_sep = column_separator
        self.val_sep = value_separator

    def load(self) -> List[SearchableItem]:
        """
        Executes the data loading and serialization based on the chosen strategy.

        Returns:
            A list of `SearchableItem` objects.
        """
        all_items: List[SearchableItem] = []
        schema = self.db.get_tables_and_columns()
        tables = [
            table for table in schema.get("tables", []) if table != "sqlite_sequence"
        ]

        for table_name in tqdm(tables, desc=f"Loading from DB ({self.strategy.value})"):
            columns_for_table = [
                col.split(".")[1]
                for col in schema.get("columns", [])
                if col.startswith(f"{table_name}.")
            ]
            if not columns_for_table:
                continue

            if self.strategy == SerializationStrategy.SCHEMA_LEVEL:
                items = self._load_schema_level(table_name, columns_for_table)
            elif self.strategy == SerializationStrategy.ROW_LEVEL:
                items = self._load_row_level(table_name, columns_for_table)
            elif self.strategy == SerializationStrategy.VALUE_LEVEL_SCHEMA_AWARE:
                items = self._load_value_level(
                    table_name, columns_for_table, schema_aware=True
                )
            elif self.strategy == SerializationStrategy.VALUE_LEVEL:
                items = self._load_value_level(
                    table_name, columns_for_table, schema_aware=False
                )
            else:
                items = []

            all_items.extend(items)

        return all_items

    def _load_schema_level(
        self, table_name: str, columns: List[str]
    ) -> List[SearchableItem]:
        """Serializes an entire table schema into a single item."""
        content = (
            f"{self.table_sep}{table_name}{self.col_sep}"
            f"{self.col_sep.join(columns)}"
        )
        metadata = {"table": table_name, "columns": columns}
        return [SearchableItem(item_id=table_name, content=content, metadata=metadata)]

    def _load_row_level(
        self, table_name: str, columns: List[str]
    ) -> List[SearchableItem]:
        """Serializes each row of a table into a separate item."""
        items: List[SearchableItem] = []
        df = self.db.execute(f'SELECT * FROM "{table_name}"', limit=-1)  # nosec B608

        for index, row in df.iterrows():
            row_parts = []
            row_metadata = []
            for col in columns:
                value = row.get(col)
                if value is not None:
                    value_str = str(value)
                    row_parts.append(f"{self.col_sep}{col}{self.val_sep}{value_str}")
                    row_metadata.append({"column": col, "value": value})

            if not row_parts:
                continue

            content = f"{self.table_sep}{table_name}{''.join(row_parts)}"
            item_id = f"{table_name}_row_{index}"
            metadata = {"table": table_name, "row_values": row_metadata}
            items.append(
                SearchableItem(item_id=item_id, content=content, metadata=metadata)
            )

        return items

    def _load_value_level(
        self, table_name: str, columns: List[str], schema_aware: bool
    ) -> List[SearchableItem]:
        """Serializes each unique cell value into a separate item."""
        items: List[SearchableItem] = []
        for col_name in columns:
            # Get distinct, non-null values for the column
            df = self.db.execute(
                f'SELECT DISTINCT "{col_name}" FROM "{table_name}" WHERE "{col_name}" IS NOT NULL;',  # nosec B608
                limit=-1,
            )
            values = df[col_name].tolist()

            for value in values:
                value_str = str(value)
                metadata = {"table": table_name, "column": col_name, "value": value}

                # Use a hash of the value for a stable, unique ID
                value_hash = hashlib.md5(
                    value_str.encode(), usedforsecurity=False
                ).hexdigest()

                item_id = f"{table_name}.{col_name}.{value_hash}"

                if schema_aware:
                    content = (
                        f"{self.table_sep}{table_name}"
                        f"{self.col_sep}{col_name}"
                        f"{self.val_sep}{value_str}"
                    )
                else:
                    content = value_str

                items.append(
                    SearchableItem(item_id=item_id, content=content, metadata=metadata)
                )

        return items

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/loaders/jsonl_loader.py
# EXT: .py, SIZE: 3123 bytes, SHA256: a4844920febcea463f7d14805694136ce27e5b9c233f5956a122a11704f6f322

import json
from typing import List, Optional

from nlp_retrieval.core.models import SearchableItem
from nlp_retrieval.loaders.loader_abc import BaseLoader
from tqdm import tqdm


class JsonlLoader(BaseLoader):
    """
    Loads data from a JSON Lines (.jsonl) file.

    Each line in the file is expected to be a valid JSON object. This loader
    extracts a specified content field and a selection of metadata fields
    to create a list of `SearchableItem` objects.
    """

    def __init__(
        self,
        file_path: str,
        content_field: str,
        metadata_fields: Optional[List[str]] = None,
        item_id_field: Optional[str] = None,
    ):
        """
        Initializes the JsonlLoader.

        Args:
            file_path: The path to the .jsonl file.
            content_field: The key in the JSON object to be used as the main `content`.
            metadata_fields: A list of keys to be included in the `metadata` dictionary.
                             If None, all fields other than the content and ID fields
                             will be included.
            item_id_field: An optional key in the JSON object to use as the `item_id`.
                           If None, a new UUID will be generated for each item.
        """
        self.file_path = file_path
        self.content_field = content_field
        self.metadata_fields = metadata_fields
        self.item_id_field = item_id_field

    def load(self) -> List[SearchableItem]:
        """
        Reads the .jsonl file and converts each line into a `SearchableItem`.

        A progress bar will be displayed. Lines that are not valid JSON or
        are missing the specified `content_field` will be skipped.

        Returns:
            A list of `SearchableItem` objects.
        """
        items: List[SearchableItem] = []

        with open(self.file_path, "r", encoding="utf-8") as f:
            for line in tqdm(f, desc=f"Loading from {self.file_path}"):
                try:
                    data = json.loads(line)
                except json.JSONDecodeError:
                    # Skip malformed lines
                    continue

                content = data.pop(self.content_field, None)
                if not content or not isinstance(content, str):
                    continue

                item_id = (
                    data.pop(self.item_id_field, None) if self.item_id_field else None
                )

                metadata: dict
                if self.metadata_fields is None:
                    # If no specific fields are requested, use all remaining data as metadata
                    metadata = data
                else:
                    metadata = {
                        key: data[key] for key in self.metadata_fields if key in data
                    }

                items.append(
                    SearchableItem(
                        item_id=item_id,  # Pydantic will generate a UUID if item_id is None
                        content=content,
                        metadata=metadata,
                    )
                )
        return items

================================================================================

### FILE: cross_dataset_discovery/nlp_retrieval/loaders/loader_abc.py
# EXT: .py, SIZE: 641 bytes, SHA256: a80415663cfbca482375d0e31265a063e17f6f90bd43545090d1a5ffdfcb6af1

from abc import ABC, abstractmethod
from typing import List

from nlp_retrieval.core.models import SearchableItem


class BaseLoader(ABC):
    """
    Abstract Base Class for data loaders.

    Loaders are responsible for reading data from a specific source (e.g., a JSONL file,
    a database, a directory of text files) and converting it into a standardized list
    of `SearchableItem` objects, ready for indexing.
    """

    @abstractmethod
    def load(self) -> List[SearchableItem]:
        """
        Loads data from the configured source.

        Returns:
            A list of `SearchableItem` objects.
        """
        pass

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/config.py
# EXT: .py, SIZE: 1055 bytes, SHA256: 58a292bad1ee260300213a6ff084dc6a17e1d92c542c50fefdf6d26b70fe0dcb

from pydantic_settings import BaseSettings, SettingsConfigDict
import os


class Settings(BaseSettings):
    # Model configuration to load from .env files
    model_config = SettingsConfigDict(
        env_file=".env", env_file_encoding="utf-8", extra="ignore"
    )

    # OIDC Authentication
    OIDC_ISSUER_URL: str = os.getenv(
        "OIDC_ISSUER_URL", "https://datagems-dev.scayle.es/oauth/realms/dev"
    )
    OIDC_AUDIENCE: str = os.getenv("OIDC_AUDIENCE", "cross-dataset-discovery-api")
    GATEWAY_API_URL: str = os.getenv(
        "GATEWAY_API_URL", "https://datagems-dev.scayle.es/dg-app-api"
    )

    @property
    def OIDC_CONFIG_URL(self) -> str:
        return f"{self.OIDC_ISSUER_URL}/.well-known/openid-configuration"

    # Database
    DB_CONNECTION_STRING: str
    TABLE_NAME: str = "your_table_name"  # Provide a default or load from env

    IdpClientSecret: str

    # Application & Search Index
    ROOT_PATH: str = ""
    INDEX_PATH: str = "./search_index"  # Path for the component's index artifacts


settings = Settings()

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/database.py
# EXT: .py, SIZE: 2297 bytes, SHA256: d2b98069d2d771d4aface3b2bf18a7ff5eb608e90f0a2d3285c913fb1ccb51db

import psycopg2
import structlog
from api_datagems_cross_dataset_discovery.app.config import settings
from api_datagems_cross_dataset_discovery.app.exceptions import (
    FailedDependencyException,
)
from api_datagems_cross_dataset_discovery.app.logging_config import (
    get_correlation_id,
)
from psycopg2 import sql
from psycopg2.pool import SimpleConnectionPool

logger = structlog.get_logger(__name__)

connection_pool: SimpleConnectionPool | None = None


def get_db_connection():
    """Dependency to get a database connection from the pool."""
    if connection_pool is None:
        logger.error("Database connection requested but pool is not available.")
        raise FailedDependencyException(
            source="Database",
            status_code=503,
            detail="Database connection pool is not available.",
            correlation_id=get_correlation_id(),
        )

    conn = None
    try:
        conn = connection_pool.getconn()
        yield conn
    finally:
        if conn:
            connection_pool.putconn(conn)


def check_database_schema(conn):
    """
    Checks if the database is connected, the required table exists,
    and all necessary columns are present in the table.
    Raises an exception if any check fails.
    """
    required_columns = {
        "content",
        "use_case",
        "source",
        "source_id",
        "chunk_id",
        "language",
        "ts_content",
        "embedding",
    }

    try:
        with conn.cursor() as cur:
            query = sql.SQL("SELECT 1 FROM {table} LIMIT 1;").format(
                table=sql.Identifier(settings.TABLE_NAME)
            )
            cur.execute(query)

            cur.execute(
                "SELECT column_name FROM information_schema.columns WHERE table_name = %s;",
                (settings.TABLE_NAME,),
            )
            existing_columns = {row[0] for row in cur.fetchall()}
            missing_columns = required_columns - existing_columns
            if missing_columns:
                raise ValueError(
                    f"Schema validation failed. Missing columns in table '{settings.TABLE_NAME}': {', '.join(missing_columns)}"
                )
    except psycopg2.Error as e:
        raise ConnectionError(f"Database check failed: {e}") from e

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/exceptions.py
# EXT: .py, SIZE: 1143 bytes, SHA256: 54d07782c59c2005a81dc0cd546b9b19cc535772a0de6f59d7668d4bda1ce343

# exceptions.py

from typing import Any, List, Optional

from fastapi import HTTPException, status
from pydantic import BaseModel


class ErrorResponse(BaseModel):
    code: int
    error: str


class ValidationErrorDetail(BaseModel):
    Key: str
    Value: List[str]


class ValidationErrorResponse(BaseModel):
    code: int
    error: str
    message: List[ValidationErrorDetail]


class FailedDependencyMessage(BaseModel):
    statusCode: int
    source: str
    correlationId: Optional[str] = None
    payload: Optional[Any] = None


class FailedDependencyResponse(BaseModel):
    code: int
    error: str
    message: FailedDependencyMessage


class FailedDependencyException(HTTPException):
    def __init__(
        self,
        source: str,
        status_code: int,
        detail: str,
        correlation_id: Optional[str] = None,
        payload: Optional[Any] = None,
    ):
        self.source = source
        self.downstream_status_code = status_code
        self.correlation_id = correlation_id
        self.downstream_payload = payload
        super().__init__(status_code=status.HTTP_424_FAILED_DEPENDENCY, detail=detail)

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/logging_config.py
# EXT: .py, SIZE: 4145 bytes, SHA256: 18ef6a29c45544c9314089d7e81aa790b1b1745029834804bd8cd5def6f3a9a2

import logging
import sys
import time
import uuid
from contextvars import ContextVar

import structlog
from fastapi import Request, Response
from structlog.types import EventDict, Processor

correlation_id_var: ContextVar[str] = ContextVar("correlation_id", default=None)


async def request_response_logging_middleware(request: Request, call_next):
    """
    FastAPI middleware to log the details of every incoming request and its response.
    """
    health_check_paths = ["/", "/health"]
    if request.url.path in health_check_paths:
        return await call_next(request)

    log = structlog.get_logger(__name__)
    start_time = time.time()

    try:
        response: Response = await call_next(request)
        process_time = (time.time() - start_time) * 1000
        status_code = response.status_code
        response_size = 0
        if "content-length" in response.headers:
            response_size = int(response.headers["content-length"])

        log_level = "info"
        if status_code >= 500:
            log_level = "error"
        elif status_code >= 400:
            log_level = "warning"

        log.log(
            getattr(logging, log_level.upper()),
            "http_request_finished",
            RequestMethod=request.method,
            RequestPath=str(request.url),
            StatusCode=status_code,
            ProcessTimeMS=round(process_time, 2),
            ResponseSize=response_size,
        )

    except Exception as e:
        process_time = (time.time() - start_time) * 1000
        log.error(
            "http_request_unhandled_exception",
            RequestMethod=request.method,
            RequestPath=str(request.url),
            ProcessTimeMS=round(process_time, 2),
            exc_info=e,
        )
        raise

    return response


def get_correlation_id() -> str | None:
    """Returns the current correlation ID."""
    return correlation_id_var.get()


async def correlation_id_middleware(request: Request, call_next):
    """
    FastAPI middleware to extract or generate a correlation ID
    and store it in a context variable for the request's lifetime.
    """
    correlation_id = request.headers.get("x-tracking-correlation")
    if not correlation_id:
        correlation_id = str(uuid.uuid4())

    token = correlation_id_var.set(correlation_id)

    response: Response = await call_next(request)
    response.headers["x-tracking-correlation"] = get_correlation_id()

    correlation_id_var.reset(token)
    return response


def datagems_log_formatter(_, __, event_dict: EventDict) -> EventDict:
    formatted_event = {}
    formatted_event["@t"] = event_dict.pop("timestamp", None)
    formatted_event["@mt"] = event_dict.pop("event", None)
    level = event_dict.pop("log_level", "info")
    formatted_event["@l"] = level.capitalize()
    correlation_id_obj = event_dict.pop("correlation_id", None)
    if callable(correlation_id_obj):
        formatted_event["DGCorrelationId"] = correlation_id_obj()
    else:
        formatted_event["DGCorrelationId"] = correlation_id_obj
    formatted_event["SourceContext"] = event_dict.pop("logger", None)
    formatted_event.update(event_dict)
    return formatted_event


def setup_logging():
    shared_processors: list[Processor] = [
        structlog.contextvars.merge_contextvars,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.processors.TimeStamper(fmt="iso"),
        datagems_log_formatter,
    ]
    structlog.configure(
        processors=shared_processors
        + [
            structlog.processors.JSONRenderer(),
        ],
        wrapper_class=structlog.stdlib.BoundLogger,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    root_logger = logging.getLogger()
    if root_logger.hasHandlers():
        root_logger.handlers.clear()
    handler = logging.StreamHandler(sys.stdout)
    root_logger.addHandler(handler)
    root_logger.setLevel(logging.INFO)
    structlog.contextvars.bind_contextvars(
        correlation_id=get_correlation_id,
        Application="cross-dataset-discovery-api",
    )

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/main.py
# EXT: .py, SIZE: 8528 bytes, SHA256: d812b0c3194ad6bc31a3f756dd51fd64e92b9b89f733232bd7a42b4b172017f7

import time
from contextlib import asynccontextmanager

import psycopg2
import structlog
from api_datagems_cross_dataset_discovery.app.config import settings
from api_datagems_cross_dataset_discovery.app.exceptions import (
    ErrorResponse,
    FailedDependencyException,
    FailedDependencyMessage,
    FailedDependencyResponse,
    ValidationErrorDetail,
    ValidationErrorResponse,
)
from api_datagems_cross_dataset_discovery.app.logging_config import (
    correlation_id_middleware,
    request_response_logging_middleware,
    setup_logging,
)
from api_datagems_cross_dataset_discovery.app.models import (
    API_SearchResult,
    SearchRequest,
    SearchResponse,
)
from nlp_retrieval.core.models import RetrievalResult
from nlp_retrieval.retrievers.bm25_retriever import PyseriniRetriever
from nlp_retrieval.searcher import Searcher
from fastapi import Depends, FastAPI, HTTPException, Request, status
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from psycopg2.pool import SimpleConnectionPool
from typing import List
from api_datagems_cross_dataset_discovery.app.database import get_db_connection
from api_datagems_cross_dataset_discovery.app import database, security

setup_logging()
logger = structlog.get_logger(__name__)
app_state = {}


@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("Application startup sequence initiated.")

    # 1. Initialize the Searcher Component
    try:
        logger.info("Initializing search components...", index_path=settings.INDEX_PATH)
        # Configure the retriever we want to use
        bm25_retriever = PyseriniRetriever(enable_tqdm=False)
        # Assemble the main searcher orchestrator
        searcher = Searcher(retrievers=[bm25_retriever])
        app_state["searcher"] = searcher
        logger.info("Search components initialized successfully.")
    except Exception as e:
        logger.fatal(
            "Failed to initialize search components", error=str(e), exc_info=True
        )
        app_state["searcher"] = None

    # 2. Create Database Connection Pool
    logger.info("Creating database connection pool...")
    try:
        database.connection_pool = SimpleConnectionPool(
            minconn=1, maxconn=2, dsn=settings.DB_CONNECTION_STRING
        )
        logger.info("Database connection pool created successfully.")
    except psycopg2.OperationalError as e:
        logger.fatal("Could not create database connection pool", error=str(e))
        database.connection_pool = None

    yield

    logger.info("Application shutdown sequence initiated.")
    if database.connection_pool:
        database.connection_pool.closeall()
        logger.info("Database connection pool closed.")
    app_state.clear()
    logger.info("Shutdown complete.")


app = FastAPI(
    lifespan=lifespan,
    title="Cross-Dataset Discovery API",
    version="1.0.0",
    root_path=settings.ROOT_PATH,
)


@app.exception_handler(FailedDependencyException)
async def failed_dependency_exception_handler(
    request: Request, exc: FailedDependencyException
):
    response_content = FailedDependencyResponse(
        code=104,
        error="error communicating with underpinning service",
        message=FailedDependencyMessage(
            statusCode=exc.downstream_status_code,
            source=exc.source,
            correlationId=exc.correlation_id,
            payload=exc.downstream_payload,
        ),
    )
    return JSONResponse(
        status_code=exc.status_code,
        content=response_content.model_dump(exclude_none=True),
    )


@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    details = [
        ValidationErrorDetail(
            Key=".".join(map(str, err.get("loc", []))), Value=[err.get("msg", "")]
        )
        for err in exc.errors()
    ]
    response_content = ValidationErrorResponse(
        code=102, error="Validation Error", message=details
    )
    return JSONResponse(
        status_code=status.HTTP_400_BAD_REQUEST, content=response_content.model_dump()
    )


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    return JSONResponse(
        status_code=exc.status_code,
        content=ErrorResponse(code=exc.status_code, error=exc.detail).model_dump(),
    )


# --- Middleware ---
app.middleware("http")(correlation_id_middleware)
app.middleware("http")(request_response_logging_middleware)


# --- Endpoints ---
@app.get("/")
def read_root():
    return {"status": "ok", "message": "Cross Dataset Discovery API is running."}


@app.post("/search/", response_model=SearchResponse)
async def perform_search(
    request: SearchRequest,
    claims: dict = Depends(security.require_role(["user", "dg_user"])),
    token: str = Depends(security.oauth2_scheme),  # Add dependency to get the raw token
):
    start_time = time.time()
    user_subject = claims.get("sub")
    log = logger.bind(query=request.query, k=request.k, UserId=user_subject)

    searcher: Searcher | None = app_state.get("searcher")
    if not searcher:
        log.error("Search request failed because searcher component is not available.")
        raise FailedDependencyException(
            source="SearchComponent",
            status_code=503,
            detail="Search service is not available.",
        )

    try:
        authorized_dataset_ids = await security.get_authorized_dataset_ids(token)

        search_filters = {}
        if request.dataset_ids:
            log = log.bind(dataset_ids=request.dataset_ids)
            allowed_to_search = set(request.dataset_ids).intersection(
                authorized_dataset_ids
            )
            if not allowed_to_search:
                log.warning(
                    "User requested datasets they are not authorized for. Returning empty results."
                )
                return SearchResponse(query_time=0, results=[])
            search_filters["source"] = list(allowed_to_search)

        search_results_batch: List[List[RetrievalResult]] = searcher.search(
            nlqs=[request.query],
            output_path=settings.INDEX_PATH,
            k=request.k,
            filters=search_filters,
        )

        component_results = search_results_batch[0]
        log.info(f"Component returned {len(component_results)} results.")

        authorized_results = []
        for result in component_results:
            dataset_id = result.item.metadata.get("source")
            if dataset_id in authorized_dataset_ids:
                api_result = API_SearchResult.model_validate(
                    {
                        **result.item.metadata,
                        "score": result.score,
                        "content": result.item.content,
                    }
                )
                authorized_results.append(api_result)
            else:
                log.warning(
                    "Result filtered due to missing permission",
                    dataset_id=dataset_id,
                    source_id=result.item.metadata.get("source_id"),
                )

        query_time = (time.time() - start_time) * 1000
        final_response = SearchResponse(
            query_time=round(query_time, 2),
            results=authorized_results,
        )
        log.info(
            "Search completed and filtered.",
            authorized_results_count=len(authorized_results),
        )
        return final_response

    except Exception as e:
        log.error(
            "An unexpected error occurred during search.", error=str(e), exc_info=True
        )
        raise HTTPException(
            status_code=500, detail=f"An unexpected error occurred: {e}"
        )


@app.get("/health")
def health_check(conn=Depends(get_db_connection)):
    # 1. Check if searcher component is loaded
    if not app_state.get("searcher"):
        raise FailedDependencyException(
            source="SearchComponent",
            status_code=503,
            detail="Searcher is not available.",
        )

    # 2. Check database connectivity and schema
    try:
        database.check_database_schema(conn)
        return {"status": "ok", "message": "All dependencies are healthy."}
    except (ConnectionError, ValueError) as e:
        logger.error("Health check failed", error=str(e))
        raise FailedDependencyException(
            source="Database", status_code=503, detail=str(e)
        )

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/models.py
# EXT: .py, SIZE: 925 bytes, SHA256: ef6fe1b7049751c9992a046d59cd5248eba4eeac07a4b93baff6b2c9a74d6ee6

from typing import List, Optional

from pydantic import BaseModel, Field, field_validator


class SearchRequest(BaseModel):
    query: str
    k: int = Field(default=5, gt=0, le=100)
    dataset_ids: Optional[List[str]] = Field(
        default=None,
        description="A list of dataset identifiers (UUIDs) to restrict the search to.",
    )

    @field_validator("dataset_ids")
    @classmethod
    def check_dataset_ids_not_empty(cls, v: Optional[List[str]]) -> Optional[List[str]]:
        if v is not None and not v:
            raise ValueError("dataset_ids cannot be an empty list.")
        return v


class API_SearchResult(BaseModel):
    content: str
    dataset_id: str = Field(validation_alias="source")
    object_id: str = Field(validation_alias="source_id")
    similarity: float = Field(validation_alias="score")


class SearchResponse(BaseModel):
    query_time: float
    results: List[API_SearchResult]

================================================================================

### FILE: cross_dataset_discovery/api_datagems_cross_dataset_discovery/app/security.py
# EXT: .py, SIZE: 9648 bytes, SHA256: e584758fffd42630e7eb541e2ae795793ceb6bdd50ead5b23122f44ffde09f26

from typing import List, Set

import httpx
import structlog
from api_datagems_cross_dataset_discovery.app.config import settings
from api_datagems_cross_dataset_discovery.app.exceptions import (
    FailedDependencyException,
)
from api_datagems_cross_dataset_discovery.app.logging_config import (
    get_correlation_id,
)
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer
from jose import JWTError, jwt

logger = structlog.get_logger(__name__)
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
_oidc_config = None
_jwks_keys = None


async def exchange_token(subject_token: str) -> str:
    """
    Exchanges the user's incoming token for a new token with the Gateway API audience.
    """
    token_url = f"{settings.OIDC_ISSUER_URL}/protocol/openid-connect/token"

    client_id = settings.OIDC_AUDIENCE  # This is 'cross-dataset-discovery-api'
    client_secret = settings.IdpClientSecret

    data = {
        "grant_type": "urn:ietf:params:oauth:grant-type:token-exchange",
        "client_id": client_id,
        "client_secret": client_secret,
        "subject_token": subject_token,
        "subject_token_type": "urn:ietf:params:oauth:token-type:access_token",
        "requested_token_type": "urn:ietf:params:oauth:token-type:access_token",
        "audience": "dg-app-api",
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(token_url, data=data)
            response.raise_for_status()
            new_token_data = response.json()
            return new_token_data["access_token"]
        except httpx.HTTPStatusError as e:
            logger.error(
                "Failed to exchange token for Gateway API",
                status_code=e.response.status_code,
                response=e.response.text,
            )
            raise FailedDependencyException(
                source="TokenExchange",
                status_code=e.response.status_code,
                detail="Could not acquire authorization token for downstream service.",
                correlation_id=get_correlation_id(),
            )


async def get_oidc_config():
    global _oidc_config
    if _oidc_config is None:
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(settings.OIDC_CONFIG_URL)
                response.raise_for_status()
                _oidc_config = response.json()
        except httpx.HTTPStatusError as e:
            logger.error(
                "Failed to fetch OIDC configuration due to HTTP error",
                url=str(e.request.url),
                status_code=e.response.status_code,
                response=e.response.text,
            )
            try:
                payload = e.response.json()
            except Exception:
                payload = e.response.text
            raise FailedDependencyException(
                source="OIDCProvider",
                status_code=e.response.status_code,
                correlation_id=get_correlation_id(),
                payload=payload,
                detail="Authentication service returned an error.",
            )
        except httpx.RequestError as e:
            logger.error(
                "Failed to fetch OIDC configuration due to network error",
                url=settings.OIDC_CONFIG_URL,
                error=str(e),
            )
            raise FailedDependencyException(
                source="OIDCProvider",
                status_code=503,
                correlation_id=get_correlation_id(),
                payload={"error": f"Network error: {type(e).__name__}"},
                detail="Authentication service is unavailable.",
            )
    return _oidc_config


async def get_jwks_keys():
    """Fetches and caches the JSON Web Key Set (JWKS) containing public keys."""
    global _jwks_keys
    if _jwks_keys is None:
        oidc_config = await get_oidc_config()
        jwks_uri = oidc_config.get("jwks_uri")
        if not jwks_uri:
            raise FailedDependencyException(
                source="OIDCProvider",
                status_code=500,
                detail="jwks_uri not found in OIDC config.",
                correlation_id=get_correlation_id(),
            )

        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(jwks_uri)
                response.raise_for_status()
                _jwks_keys = response.json()
        except httpx.HTTPStatusError as e:
            logger.error(
                "Failed to fetch JWKS keys due to HTTP error",
                url=str(e.request.url),
                status_code=e.response.status_code,
                response=e.response.text,
            )
            try:
                payload = e.response.json()
            except Exception:
                payload = e.response.text
            raise FailedDependencyException(
                source="OIDCProvider",
                status_code=e.response.status_code,
                correlation_id=get_correlation_id(),
                payload=payload,
                detail="Could not fetch public keys for token validation.",
            )
        except httpx.RequestError as e:
            logger.error(
                "Failed to fetch JWKS keys due to network error",
                url=jwks_uri,
                error=str(e),
            )
            raise FailedDependencyException(
                source="OIDCProvider",
                status_code=503,
                correlation_id=get_correlation_id(),
                payload={"error": f"Network error: {type(e).__name__}"},
                detail="Could not fetch public keys for token validation.",
            )
    return _jwks_keys


async def get_current_user_claims(token: str = Depends(oauth2_scheme)) -> dict:
    """
    A FastAPI dependency that validates the JWT and returns its claims.
    This will be applied to protected endpoints.
    """
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )

    try:
        keys = await get_jwks_keys()
        payload = jwt.decode(
            token,
            keys,
            algorithms=["RS256"],
            audience=settings.OIDC_AUDIENCE,
            issuer=settings.OIDC_ISSUER_URL,
        )
        return payload
    except JWTError as e:
        logger.warning("JWT validation failed", error=str(e))
        raise credentials_exception
    except Exception as e:
        logger.error(
            "An unexpected error occurred during token validation", error=str(e)
        )
        raise credentials_exception


def require_role(required_roles: List[str]):
    """
    A FastAPI dependency that checks if the user has at least one of the required roles.
    """

    def role_checker(claims: dict = Depends(get_current_user_claims)) -> dict:
        # this implementation  aims to check not only for user but for dg_user as well for the swagger
        user_roles = set(claims.get("realm_access", {}).get("roles", []))

        # Check for any intersection between the user's roles and the required roles
        if not user_roles.intersection(required_roles):
            log_context = {
                "required_roles": required_roles,
                "UserId": claims.get("sub"),
                "user_roles": list(user_roles),
            }
            client_id = claims.get("clientid")
            if client_id:
                log_context["ClientId"] = client_id

            logger.warning(
                "Authorization failed: User missing any of the required roles",
                **log_context,
            )
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail=f"User does not have any of the required roles: {', '.join(required_roles)}.",
            )

        # If the check passes, return the claims for use in the endpoint
        return claims

    return role_checker


async def get_authorized_dataset_ids(token: str) -> Set[str]:
    """
    Exchanges the token and calls the DataGEMS Gateway to get the dataset IDs the user can access.
    """
    log = logger.bind()
    try:
        gateway_token = await exchange_token(token)
        api_url = f"{settings.GATEWAY_API_URL}/api/principal/me/context-grants"
        headers = {"Authorization": f"Bearer {gateway_token}"}

        log = log.bind(gateway_url=api_url)

        async with httpx.AsyncClient() as client:
            response = await client.get(api_url, headers=headers)
            response.raise_for_status()

            context_grants = response.json()
            dataset_ids = {
                grant["targetId"]
                for grant in context_grants
                if grant.get("targetType") == 0
                and "targetId" in grant  # targetType 0 is 'dataset'
            }

            log.info(
                "Successfully fetched user permissions.",
                total_grants=len(context_grants),
                dataset_grants=len(dataset_ids),
            )
            return dataset_ids

    except httpx.HTTPStatusError as e:
        log.error(
            "Gateway returned an error when fetching dataset permissions.",
            status_code=e.response.status_code,
            response=e.response.text,
        )
        return set()
    except Exception as e:
        log.error(
            "An unexpected error occurred while fetching dataset permissions.",
            error=str(e),
        )
        return set()

================================================================================

### FILE: cross_dataset_discovery/utils_database_connector/__init__.py
# EXT: .py, SIZE: 0 bytes, SHA256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855


================================================================================

### FILE: cross_dataset_discovery/utils_database_connector/core.py
# EXT: .py, SIZE: 17486 bytes, SHA256: d7c3183d8e4ca54247d05f8b1e3702cce373034c6857d9f97c6f79b4171b2bd3

import os
from collections import defaultdict

import pandas as pd
from loguru import logger
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
from sqlglot import expressions, parse_one
from sqlglot.errors import ParseError


class Database:
    def __init__(
        self, database: str, max_execution_time: int = 180, specific_schema: str = None
    ):
        """
        Initialize the database connector. There are two types of databases supported: PostgreSQL, MySQL.
        The configuration of the database will be obtained from the utils_configs component.

        Args:
            database: the database name
            max_execution_time: the maximum execution time for a query in seconds. Default is 180s.
        """
        self.config = self._get_database_from_name(database)
        self.max_execution_time = max_execution_time
        self.specific_schema = specific_schema

        if "TEST" in os.environ:
            hostname = self.config.test_hostname
        elif "DEV" in os.environ:
            hostname = "localhost"
        else:  # pragma: no cover
            hostname = self.config.hostname

        self.connection_uri = (
            f"{self.config.type}+{self.config.driver}://{self.config.username}:"
            f"{self.config.password}@{hostname}:{self.config.port}/{self.config.name}"
        )

        if self.config.type == "postgresql":
            conn_args = {
                "options": f"-c statement_timeout={max_execution_time * 1000}"
                + (f" -c search_path={specific_schema}" if specific_schema else "")
            }
        elif self.config.type == "mysql":
            if specific_schema is not None:
                logger.warning(
                    "Query executor for MySQL does not support specific schema selection"
                )
            conn_args = {"read_timeout": max_execution_time}
        else:
            raise ValueError("Invalid database type")

        self.engine = create_engine(self.connection_uri, connect_args=conn_args)

        self.schemas = (
            ",".join(["'" + k + "'" for k in self.config.schemas])
            if specific_schema is None
            else f"'{specific_schema}'"
        )

    def _parse_query(self, query: str, limit: int, order_by_rand=False, only_read=True):
        pars = parse_one(query)
        if only_read and not isinstance(
            pars,
            (
                expressions.Select,
                expressions.Union,
                expressions.Intersect,
                expressions.Except,
            ),
        ):
            raise ValueError(
                "Database executor only accepts SELECT queries by default. "
                "If you want to execute other types of queries, set only_read=False."
            )

        if order_by_rand:
            if self.config.type != "mysql":
                pars = pars.order_by("random()")
            else:
                pars = pars.order_by("rand()")

        if limit not in (-1, 0):
            pars = pars.limit(limit)
        sql = pars.sql(dialect="mysql" if self.config.type == "mysql" else "postgres")
        # print(sql)
        return sql

    def execute(
        self,
        sql: str,
        limit: int = 500,
        order_by_rand: bool = False,
        fix_dates: bool = False,
        dates_format: str = "%d/%m/%Y",
        is_read: bool = True,
    ) -> pd.DataFrame | dict:
        """
        Execute a given SQL query.

        Note: By default the limit parameter is applied ignoring any limit set in the query from the user.
        To avoid applying a limit, set the limit parameter to -1.

        Args:
            sql: the sql query
            limit: the maximum number of rows to return. To ignore the limit parameter, set it to -1
            order_by_rand: whether to order the results randomly
            fix_dates: whether to fix the dates format
            dates_format: the dates format
            is_read: whether the query will read results from the database

        Returns:
            results: the results of the query or a dictionary with an error message
        """

        try:
            query = self._parse_query(sql, limit, order_by_rand, is_read)

            with self.engine.begin() as conn:
                df = pd.read_sql(text(query), con=conn)
            conn.close()
            self.engine.dispose()

            if fix_dates:
                mask = df.astype(str).apply(
                    lambda x: x.str.match(r"(\d{2,4}-\d{2}-\d{2,4})+").all()
                )
                df.loc[:, mask] = (  # type: ignore
                    df.loc[:, mask]  # type: ignore
                    .apply(pd.to_datetime)
                    .apply(lambda x: x.dt.strftime(dates_format))
                )
        except SQLAlchemyError as e:
            logger.error(f"sqlalchemy error {str(e.__dict__['orig'])}")
            return {"error": str(e.__dict__["orig"])}
        except ParseError as e:
            if len(e.errors) > 0:
                logger.error(f"parse error {e.errors[0]['description']}")
                return {"error": e.errors[0]["description"]}
            logger.error(f"parse error {str(e)}")
            return {"error": str(e)}
        except RuntimeError as e:
            logger.error(f"runtime error {str(e)}")
            return {"error": str(e)}
        except Exception as e:
            logger.error(f"other exception: {e} {sql}")
            return {"error": f"Something went wrong with your query. Error: {e}"}
        return df

    def executemany(self, sql: str, data: list):
        """
        Execute many SQL queries in a batch (for bulk INSERT, UPDATE, or DELETE operations)

        Args:
            sql: the SQL query template
            data: list of dictionaries containing the data to be used in the query
        """
        try:
            with self.engine.begin() as conn:
                conn.execute(text(sql), data)
            return {"status": "success"}
        except SQLAlchemyError as e:
            return {"error": str(e.__dict__["orig"])}
        except Exception as e:
            print("other exception", e)
            return {"error": "Something went wrong with your query."}

        # TODO: Parse the query for the specific database type (PostgreSQL or MySQL)

    def get_tables_and_columns(self, blacklist_tables: list = []) -> dict:
        """
        Return the schema of the database

        Args:
            blacklist_tables: the tables to exclude from the results


        Examples:
            ```
            {
                'tables': ['table1', 'table2'],
                'columns': ['table1.column1', 'table1.column2', 'table2.column1'],
                'table': {
                    'table1': [0, 1],
                    'table2': [2]
                }
            }
            ```
        """
        q = f"""
            SELECT table_name,column_name
            FROM information_schema.COLUMNS
            WHERE table_schema in ({self.schemas})
        """  # nosec B608
        if (
            len(blacklist_tables) == 0 and len(self.config.blacklist_tables) > 0
        ):  # blacklist not provided, use default
            blacklist_tables = self.config.blacklist_tables

        if len(blacklist_tables) > 0:
            blacklist = " AND ".join(
                ["table_name not like '" + k + "'" for k in blacklist_tables]
            )
            q += " AND " + blacklist

        results = self.execute(q, limit=0)
        return self._parse_tables_and_columns(results)

    @staticmethod
    def _parse_tables_and_columns(results) -> dict:
        column_id = 0
        parsed = {"tables": [], "columns": [], "table": {}}

        for _, row in results.iterrows():
            table, column = row

            if table not in parsed["tables"]:
                parsed["tables"].append(table)
                parsed["table"][table] = []

            parsed["columns"].append(table + "." + column)
            parsed["table"][table].append(column_id)

            column_id += 1

        return parsed

    def get_types_of_db(self) -> dict:
        """
        Return the types of the columns of the database
        """
        ret_types = defaultdict(dict)
        if self.config.type == "postgresql":
            query = f"""
                SELECT table_name, column_name, data_type
                FROM information_schema.COLUMNS
                WHERE table_schema='{self.specific_schema if self.specific_schema else 'public'}'
                  AND table_name NOT ILIKE 'pg_%'
                  AND table_name NOT ILIKE 'sql_%';
            """  # nosec B608
            results = self.execute(query, limit=0)
            for _, row in results.iterrows():
                table, column, data_type = row
                data_type = data_type.replace("character varying", "varchar").upper()
                ret_types[table][column] = data_type
        else:
            raise ValueError(
                f"This method is only available for PostgreSQL databases, not {self.config.type}"
            )
        return ret_types

    def get_primary_keys(self) -> dict:
        """
        Return the primary keys of the database
        """
        ret_pks = defaultdict(list)
        if self.config.type == "postgresql":
            query = f"""
                    SELECT
                        tc.table_name,
                        kcu.column_name
                    FROM
                        information_schema.table_constraints AS tc
                        JOIN information_schema.key_column_usage AS kcu
                        ON tc.constraint_name = kcu.constraint_name
                        AND tc.table_schema = kcu.table_schema
                    WHERE tc.constraint_type = 'PRIMARY KEY'
                        and tc.table_schema = '{self.specific_schema if self.specific_schema else 'public'}' ;
                """  # nosec B608

            results = self.execute(query, limit=0)
            for _, row in results.iterrows():
                table, column = row
                ret_pks[table].append(column)
        else:
            raise ValueError(
                f"This method is only available for PostgreSQL databases, not {self.config.type}"
            )

        return ret_pks

    def get_foreign_keys(self) -> dict:
        """
        Return the foreign keys of the database
        """
        ret_foreign_keys = {}
        if self.config.type == "postgresql":
            query = f"""
            SELECT
                tc.table_name,
                kcu.column_name,
                ccu.table_name AS foreign_table_name,
                ccu.column_name AS foreign_column_name
            FROM information_schema.table_constraints AS tc
            JOIN information_schema.key_column_usage AS kcu
                ON tc.constraint_name = kcu.constraint_name
                AND tc.table_schema = kcu.table_schema
            JOIN information_schema.constraint_column_usage AS ccu
                ON ccu.constraint_name = tc.constraint_name
                AND ccu.table_schema = tc.table_schema
            WHERE tc.constraint_type = 'FOREIGN KEY'
                AND tc.table_schema='{self.specific_schema if self.specific_schema else 'public'}'
            """  # nosec B608
            results = self.execute(query, limit=0)
            for _, row in results.iterrows():
                table, column, foreign_table, foreign_column = row
                if table in ret_foreign_keys and column in ret_foreign_keys[table]:
                    ret_foreign_keys[table][column].append(
                        {
                            "foreign_table": foreign_table,
                            "foreign_column": foreign_column,
                        }
                    )
                else:
                    ret_foreign_keys[table] = {
                        column: [
                            {
                                "foreign_table": foreign_table,
                                "foreign_column": foreign_column,
                            }
                        ]
                    }
        else:
            pass

        return ret_foreign_keys

    def get_joins(self) -> dict:
        """
        Return the joins for the database

        Examples:
            ```
            {
                'table1': {
                    'table2': 'table1.column1=table2.column1',
                    'table3': 'table1.column2=table3.column2'
                },
            }
            ```
        """
        if self.config.type != "mysql":
            query = f"""
            SELECT
                tc.table_name,
                kcu.column_name,
                ccu.table_name AS foreign_table_name,
                ccu.column_name AS foreign_column_name
            FROM
                information_schema.table_constraints AS tc
                JOIN information_schema.key_column_usage AS kcu
                ON tc.constraint_name = kcu.constraint_name
                AND tc.table_schema = kcu.table_schema
                JOIN information_schema.constraint_column_usage AS ccu
                ON ccu.constraint_name = tc.constraint_name
                AND ccu.table_schema = tc.table_schema
            WHERE tc.constraint_type = 'FOREIGN KEY' and tc.table_schema in ({self.schemas})
            """  # nosec B608
        else:
            # Note: THis probably does not work. Check the query in get_foreign_keys
            query = f"""
            SELECT
                TABLE_NAME,
                COLUMN_NAME,
                REFERENCED_TABLE_NAME,
                REFERENCED_COLUMN_NAME
            FROM
                INFORMATION_SCHEMA.KEY_COLUMN_USAGE
            WHERE REFERENCED_COLUMN_NAME is not null
            AND CONSTRAINT_SCHEMA in ({self.schemas})
            """  # nosec B608
        results = self.execute(query, limit=0)
        return self._parse_joins(results)

    @staticmethod
    def _parse_joins(results) -> dict:
        joins = {}
        for _, join in results.iterrows():
            for i in [0, 2]:
                thisTable = join[i]
                otherTable = join[0] if i == 2 else join[2]

                if thisTable not in joins:
                    joins[thisTable] = {}

                if otherTable not in joins[thisTable]:
                    joins[thisTable][otherTable] = []

                condition = join[0] + "." + join[1] + "=" + join[2] + "." + join[3]
                joins[thisTable][otherTable].append(condition)

        for tableA, valA in joins.items():
            for tableB, valB in valA.items():
                joins[tableA][tableB] = " AND ".join(valB)

        return joins

    def get_query_cost(self, query: str) -> float:
        """
        Gets the raw plan from EXPLAIN command of the input query and outputs the plan in a python dictionary

        Args:
            query (str): The SQL query

        Returns:
            Float: The execution estimated plan cost
        """

        explain_query = f"EXPLAIN (VERBOSE TRUE, SUMMARY TRUE, FORMAT json) {query}"
        plan = self.execute(sql=explain_query)

        # *** Explanation to extract the runtime estimation
        # 1. The EXPLAIN command has cardinality equal to 1, so the output is in the first row (first [0])
        # 2. The EXPLAIN returns a list encapsulating the whole plan, with one element, which is a JSON (second [0])
        # 3. Within the JSON there are two keys: a) the execution plan and b) the Planning Time:
        # 4. The structure of the execution plan is to nest the children of each node in a list, starting from the root.
        #   PostgresSQL computes the estimation in a bottom-up fashion, meaning that the first scan has starting
        #   estimate equal to 0.0 and the top node of the plan (root of the tree) has the cumulative estimates,
        #   thus for the whole plan.
        return float(plan[0][0]["Plan"]["Total Cost"])


if __name__ == "__main__":
    # q = """
    # SELECT
    #         a.fullname, a.orcid,
    #         COUNT(r.id) AS publication_num,
    #         STRING_AGG(r.id, '; ') AS publication_ids
    #     FROM author a
    #     LEFT JOIN result_author ra ON a.id = ra.author_id
    #     LEFT JOIN result r ON ra.result_id = r.id
    #     WHERE a.id = '00011ab1bc9af9fbfbabd4d8cca6fa76'
    #     GROUP BY a.id, a.fullname, a.orcid;
    # """
    # q = """
    # INSERT INTO table_name (column1, column2, column3)
    # VALUES (value1, value2, value3);
    # """
    q = """
    SELECT *
        FROM (
            SELECT r.title, r.publication_date
            FROM result r
            WHERE (r.keywords ILIKE '%recommender%' OR r.title ILIKE '%recommender%' OR r.description ILIKE '%recommender%') AND r.type = 'publication'
            ORDER BY r.publication_date DESC
         ) AS r
        UNION
        SELECT *
        FROM (
            SELECT r.title, r.publication_date
            FROM result r
            WHERE (r.keywords ILIKE '%recommender%' OR r.title ILIKE '%recommender%' OR r.description ILIKE '%recommender%') AND r.type = 'publication'
            ORDER BY r.publication_date
         ) AS r;
    """
    print(Database("fc4eosc").get_foreign_keys())
    # print(Database("fc4eosc", specific_schema="fc4eosc_subset").execute(q, limit=10))
    # db = Database("cordis", max_execution_time=0.0001)
    # print(db.execute("SELECT COUNT(*) FROM projects"))

================================================================================

### FILE: cross_dataset_discovery/utils_database_connector/sqlite_db.py
# EXT: .py, SIZE: 6007 bytes, SHA256: d6d78dcd8f461f0c0cfd2e766155268238592022f3009cad1bde4868f5cc683f

import os
from collections import defaultdict

import pandas as pd
from loguru import logger
from sqlalchemy import create_engine, text, event
from sqlalchemy.exc import SQLAlchemyError
from sqlglot import parse_one


class DatabaseSqlite:
    def __init__(self, database: str):
        """.
        Initialize the database connector for SQLite by providing the path to the database file.
        Note: SQLite does not have all the functionalities provided by the Database connector.

        Args:
            database: the database path i.e. "path/to/database.db"
        """
        if not os.path.exists(database):
            raise ValueError(f'Sqlite database file "{database}" does not exist.')
        self.connection_uri = f"sqlite:///{database}"
        self.engine = create_engine(self.connection_uri)

        @event.listens_for(self.engine, "connect")
        def connect(dbapi_connection, connection_record):
            dbapi_connection.text_factory = lambda b: b.decode("latin1")

    def execute(self, sql: str, limit: int = 500) -> pd.DataFrame | dict:
        """
        Execute a given SQL query

        Args:
            sql: the sql query
            limit: the limit of the number of rows to return

        Returns:
            results: the results of the query or a dictionary with an error message
        """
        pars = parse_one(sql, dialect="sqlite")
        if limit not in (-1, 0):
            pars = pars.limit(limit)
        sql = pars.sql(dialect="sqlite")

        try:
            with self.engine.begin() as conn:
                df = pd.read_sql(text(sql), con=conn)
            conn.close()
            self.engine.dispose()
        except SQLAlchemyError as e:
            logger.error(f"sqlalchemy error {str(e.__dict__['orig'])}")
            return {"error": str(e.__dict__["orig"])}
        except Exception as e:
            logger.error(f"General error: {e} for query {sql}")
            return {"error": "Something went wrong with your query."}
        return df

    def get_tables_and_columns(self):
        tables_cols_df = self.execute(
            """
            SELECT m.name as tableName,
                   p.name as columnName
            FROM sqlite_master m
            left outer join pragma_table_info((m.name)) p
                 on m.name <> p.name
            order by tableName, columnName
            ;
        """
        )

        res = {"tables": tables_cols_df["tableName"].unique(), "columns": []}
        for _, row in tables_cols_df.iterrows():
            if row["columnName"] and row["tableName"]:
                res["columns"].append(row["tableName"] + "." + row["columnName"])

        return res

    def get_types_of_db(self) -> dict:
        """
        Return the types of the columns of the database
        """
        types_table = self.execute(
            """
            SELECT m.name AS table_name,
              p.name AS column_name, p.type AS data_type
            FROM sqlite_master AS m
              INNER JOIN pragma_table_info(m.name) AS p
            WHERE m.name NOT IN ('sqlite_sequence')
            """
        )
        ret_types = defaultdict(dict)
        for _, row in types_table.iterrows():
            table, column, data_type = row
            ret_types[table][column] = data_type

        return ret_types

    def get_primary_keys(self) -> dict:
        """
        Return the primary keys of the database
        """
        pks_table = self.execute(
            """
            SELECT m.name AS table_name,
              p.name AS column_name
            FROM sqlite_master AS m
              INNER JOIN pragma_table_info(m.name) AS p
            WHERE m.name NOT IN ('sqlite_sequence')
                AND p.pk != 0
            ORDER BY m.name, p.cid
            """
        )

        ret_pks = defaultdict(list)
        for _, row in pks_table.iterrows():
            table, column = row
            ret_pks[table].append(column)

        return ret_pks

    def get_foreign_keys(self) -> dict:
        """
        Return the foreign keys of the database
        """
        foreign_keys = self.execute(
            """
            SELECT
                m.tbl_name AS table_name,
                p.'from' AS column_name,
                p.'table' AS foreign_table_name,
                p.'to' AS foreign_column_name
            FROM sqlite_master AS m
                  INNER JOIN pragma_foreign_key_list(m.name) AS p
            WHERE m.name NOT IN ('sqlite_sequence');
        """
        )
        ret_foreign_keys = {}
        for _, row in foreign_keys.iterrows():
            table, column, foreign_table, foreign_column = row

            if None in (table, column, foreign_table, foreign_column):
                # NOTE: Sometimes foreign_column is None, which cause errors later on
                continue

            if table in ret_foreign_keys and column in ret_foreign_keys[table]:
                ret_foreign_keys[table][column].append(
                    {
                        "foreign_table": foreign_table,
                        "foreign_column": foreign_column,
                    }
                )
            else:
                ret_foreign_keys[table] = {
                    column: [
                        {
                            "foreign_table": foreign_table,
                            "foreign_column": foreign_column,
                        }
                    ]
                }

        return ret_foreign_keys


if __name__ == "__main__":
    db = DatabaseSqlite("Car_Database.db")
    print(db.get_foreign_keys())
    # a = db.execute(
    #     """
    #     SELECT
    #         m.tbl_name AS table_name,
    #         p.'from' AS column_name,
    #         p.'table' AS foreign_table_name,
    #         p.'to' AS foreign_column_name
    #     FROM sqlite_master AS m
    #           INNER JOIN pragma_foreign_key_list(m.name) AS p
    #     WHERE m.name NOT IN ('sqlite_sequence');
    #     """
    # )
    # print(a)

================================================================================

### FILE: tests/test_foo.py
# EXT: .py, SIZE: 93 bytes, SHA256: 2f9c7ae1093d244bd40acf1c72d0ca19f2f461959c66b79907b651970f286fdd

from cross_dataset_discovery.foo import foo


def test_foo():
    assert foo("foo") == "foo"

================================================================================
