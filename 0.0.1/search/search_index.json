{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Welcome to the documentation for the Cross-Dataset Discovery Service.</p> <p>This service provides an API for performing natural language search queries across a pre-indexed collection of datasets.</p>"},{"location":"#how-to-use-this-documentation","title":"How to Use This Documentation","text":"<ul> <li>System: Learn about the high-level Architecture, Security model, and Datastores.</li> <li>API: Find detailed information on the API endpoints and error codes.</li> <li>Setup &amp; Monitoring: Get instructions on Deployment, Configuration, and Logging.</li> </ul>"},{"location":"api-overview/","title":"API Overview","text":"<p>The Cross-Dataset Discovery service exposes a RESTful API for performing search operations.</p>"},{"location":"api-overview/#base-url","title":"Base URL","text":"<p>The API is served from the root of the application. All endpoint paths are relative to the base URL where the service is deployed:</p> <p>Base URL: https://datagems-dev.scayle.es/cross-dataset-discovery/</p>"},{"location":"api-overview/#authentication","title":"Authentication","text":"<p>All endpoints are protected and require a valid JWT Bearer token. See the Security page for more details.</p>"},{"location":"api-overview/#endpoints","title":"Endpoints","text":""},{"location":"api-overview/#search","title":"Search","text":"<p>Performs a search query against the indexed datasets.</p> <ul> <li>Endpoint: <code>POST /search/</code></li> <li>Description: Submits a natural language query and returns a ranked list of relevant results. The search can be optionally filtered to a specific set of datasets. The results returned are automatically filtered based on the user's access permissions.</li> <li> <p>Request Body:</p> <pre><code>{\n  \"query\": \"string\",\n  \"k\": 5,\n  \"dataset_ids\": [\n    \"string\"\n  ]\n}\n</code></pre> </li> <li> <p>Response Body (Success):</p> <pre><code>{\n  \"query_time\": \"retrieval time in seconds\",\n  \"results\": [\n    {\n      \"content\": \"The main text content of the search result.\",\n      \"dataset_id\": \"uuid-of-the-source-dataset\",\n      \"object_id\": \"uuid-of-the-source-object\",\n      \"similarity\": \"simialrity-score (higher the better)\"\n    }\n  ]\n}\n</code></pre> </li> </ul>"},{"location":"api-overview/#health-check","title":"Health Check","text":"<p>Verifies the operational status of the API and its dependencies.</p> <ul> <li>Endpoint: <code>GET /health</code></li> <li>Description: Checks the availability of the search component and the existance of the index. Returns a <code>200 OK</code> if all systems are healthy.</li> <li> <p>Response Body (Success):</p> <pre><code>{\n  \"status\": \"ok\",\n  \"message\": \"All dependencies are healthy.\"\n}\n</code></pre> </li> </ul>"},{"location":"architecture/","title":"System Architecture","text":"<p>The Cross-Dataset Discovery Service is a self-contained application designed to act as a specialized search layer within the DataGEMS ecosystem.</p>"},{"location":"architecture/#core-components","title":"Core Components","text":"<ol> <li> <p>FastAPI Application: The heart of the service is a Python web application built with FastAPI. It exposes the REST API, handles incoming HTTP requests, and orchestrates all internal operations.</p> </li> <li> <p>Search Component: An internal module responsible for executing queries against the search index. It takes the user query and returns a ranked list of document IDs.</p> </li> <li> <p>Security Layer: This layer intercepts all incoming requests to perform authentication and authorization. It integrates with an external OIDC provider to validate JWTs and with the DataGEMS Gateway to fetch user-specific permissions.</p> </li> </ol>"},{"location":"architecture/#request-flow","title":"Request Flow","text":"<p>A typical search request follows this path: 1.  A user sends a <code>POST /search/</code> request with a valid JWT. 2.  The Security Layer validates the token and checks for the required user roles. 3.  The service calls the DataGEMS Gateway to get the list of datasets the user is authorized to see. 4.  The user's query is passed to the Search Component, which queries the pre-built index returning an authorized dataset list. 5.  The final, authorized results are returned to the user.</p>"},{"location":"automations/","title":"Automations","text":"<p>This project leverages GitHub Actions to automate builds, code analysis, security scanning, and documentation deployment. The following sections detail the specific workflows configured for this repository.</p>"},{"location":"automations/#dockerfile","title":"Dockerfile","text":"<p>The creation of a production-ready container image is fully automated by the <code>Dockerfile</code> in the root of the repository. This build process ensures a consistent environment for the application.</p> <ul> <li>Stage 1 (Builder): This stage prepares the environment by installing all necessary build-time dependencies, including OpenJDK (for the search library) and the full set of Python packages.</li> <li>Stage 2 (Final): This stage constructs the final, lean image by copying only the essential artifacts from the builder stage. It creates a non-root <code>appuser</code> for security, copies the application code, and sets the <code>gunicorn</code> server as the entry point.</li> </ul>"},{"location":"automations/#docker-image-publishing","title":"Docker image publishing","text":"<p>Workflow: <code>.github/workflows/docker-publish.yml</code></p> <p>This workflow automates the process of building and publishing the service's Docker image to the GitHub Container Registry (ghcr.io).</p> <ul> <li>Trigger: This workflow runs automatically whenever a new Git tag matching the pattern <code>v*</code> (e.g., <code>v1.0.0</code>, <code>v1.1.0</code>) is pushed to the repository.</li> <li>Process:<ol> <li>Logs into the GitHub Container Registry using a temporary <code>GITHUB_TOKEN</code>.</li> <li>Uses the <code>docker/metadata-action</code> to automatically generate appropriate Docker tags based on the Git tag.</li> <li>Builds the Docker image using the <code>Dockerfile</code>.</li> <li>Pushes the newly built and tagged image to the <code>ghcr.io/datagems-eosc/cross-dataset-discovery-api</code> repository, making it available for deployment.</li> </ol> </li> </ul>"},{"location":"automations/#vulnerability-scanning","title":"Vulnerability Scanning","text":"<p>Workflow: <code>.github/workflows/vulnerability-scan-on-demand.yml</code></p> <p>This workflow provides on-demand security scanning of the project's configuration and Docker images using the Trivy security scanner.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>). It requires an <code>image_tag</code> as input to specify which published Docker image to scan.</li> <li>Process:<ol> <li>Configuration Scan: Scans the <code>Dockerfile</code> and other repository configuration files for potential security misconfigurations.</li> <li>Image Scan: Pulls the specified Docker image from the GitHub Container Registry and scans its operating system packages and Python libraries for known vulnerabilities (CVEs) with <code>CRITICAL</code> or <code>HIGH</code> severity.</li> <li>Artifacts: The JSON reports from both the configuration scan and the image scan are uploaded as build artifacts for review.</li> </ol> </li> </ul>"},{"location":"automations/#static-code-analysis","title":"Static Code Analysis","text":"<p>Workflow: <code>.github/workflows/code-scan-on-demand.yml</code></p> <p>This workflow performs in-depth static code analysis using GitHub CodeQL to find potential bugs, security vulnerabilities, and quality issues in the codebase.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>It runs two separate analysis jobs: one for the Python source code and one for the GitHub Actions workflow files themselves.</li> <li>For each language, it initializes CodeQL using an extended set of security and quality queries.</li> <li>It performs the analysis and uploads the results directly to the repository's \"Security\" tab under \"Code scanning alerts\".</li> </ol> </li> </ul>"},{"location":"automations/#code-metrics","title":"Code Metrics","text":"<p>Workflow: <code>.github/workflows/code-metrics-on-demand.yml</code></p> <p>This workflow generates a report on the complexity and maintainability of the application's Python code using the Radon library.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process:<ol> <li>Installs the <code>radon</code> Python package.</li> <li>Runs three types of analysis on the <code>cross_dataset_discovery/api_datagems_cross_dataset_discovery/app</code> directory:<ul> <li>Maintainability Index (MI): A score from 0-100 indicating how easy the code is to maintain.</li> <li>Cyclomatic Complexity (CC): Measures the number of independent paths through the code, indicating its complexity.</li> <li>Raw Lines of Code (LOC): Provides basic code size metrics.</li> </ul> </li> <li>The complete report is compiled into a text file and uploaded as a build artifact named <code>code-metrics-report</code>.</li> </ol> </li> </ul>"},{"location":"automations/#documentation","title":"Documentation","text":"<p>Workflow: <code>.github/workflows/deploy-docs-on-demand.yml</code></p> <p>The deployment of this documentation site is automated by a dedicated GitHub Action workflow.</p> <ul> <li>Trigger: This workflow is run manually from the GitHub Actions UI (<code>workflow_dispatch</code>).</li> <li>Process: When triggered with a version number (e.g., <code>1.0.0</code>), the workflow:<ol> <li>Installs <code>mkdocs</code>, <code>mike</code>, and other required tools.</li> <li>Builds the static HTML site from the markdown source files in the <code>docs/</code> directory.</li> <li>Uses the <code>mike</code> tool to commit the built site to the <code>gh-pages</code> branch, organized under the specified version.</li> <li>Updates the <code>latest</code> alias to point to this newly deployed version, ensuring visitors see the most recent documentation by default.</li> </ol> </li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>The Cross-Dataset Discovery service is configured using a combination of a local configuration file and environment variables.</p>"},{"location":"configuration/#configuration-files","title":"Configuration files","text":"<p>For local development, the service can be configured using a <code>.env</code> file placed in the root of the repository. The application will automatically load variables from this file on startup.</p> <p>Warning The <code>.env</code> file is intended for local development only and should be added to your <code>.gitignore</code> file to prevent committing secrets to version control.</p>"},{"location":"configuration/#example-env-file","title":"Example <code>.env</code> file","text":"<pre><code># .env\n\n# OIDC Authentication\nOIDC_ISSUER_URL=\"https://datagems-dev.scayle.es/oauth/realms/dev\"\nOIDC_AUDIENCE=\"cross-dataset-discovery-api\"\nGATEWAY_API_URL=\"https://datagems-dev.scayle.es/gw\"\n\n# Database\nDB_CONNECTION_STRING=\"postgresql://user:password@localhost:5432/datagems_db\"\nTABLE_NAME=\"your_table_name\"\n\n# Application &amp; Search Index\nROOT_PATH=\"\"\nINDEX_PATH=\"./search_index\"\n\n# Secrets (for local use only)\nIdpClientSecret=\"your-local-dev-secret\"\n</code></pre>"},{"location":"configuration/#environment-overrides","title":"Environment Overrides","text":"<p>In any environment, especially in production, environment variables will always override values set in the <code>.env</code> file. This is the standard and recommended way to configure the service when deploying it as a container.</p> <p>The following table lists all available configuration variables:</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider for token validation. https://datagems-dev.scayle.es/oauth/realms/dev OIDC_AUDIENCE The audience claim that must be present in the JWT. cross-dataset-discovery-api GATEWAY_API_URL The base URL of the DataGEMS Gateway API, used to fetch user permissions. https://datagems-dev.scayle.es/gw DB_CONNECTION_STRING The full connection string for the PostgreSQL database. postgresql://user:pass@host:port/dbname TABLE_NAME The name of the table to check in the database during health checks. your_table_name IdpClientSecret The client secret for the identity provider. your-secret ROOT_PATH The path prefix for the API if it's running behind a reverse proxy. /cdd INDEX_PATH The path inside the container to the directory containing the search index files. /app/search_index"},{"location":"configuration/#secrets","title":"Secrets","text":"<p>Certain configuration variables contain sensitive information and must be handled securely.</p>"},{"location":"configuration/#identified-secrets","title":"Identified Secrets","text":"<ul> <li>DB_CONNECTION_STRING: Contains the database username and password.</li> <li>IdpClientSecret: The client secret used for communication with the identity provider.</li> </ul>"},{"location":"datastore/","title":"Datastores","text":"<p>The Cross-Dataset Discovery service relies on a search index for fast retrieval.</p>"},{"location":"datastore/#search-index","title":"Search Index","text":"<p>The core of the retrieval functionality is powered by pre-built search indexes. The API is configured to use an index compatible with Pyserini (which is based on Apache Lucene).</p> <ul> <li>Technology: Apache Lucene</li> <li>Purpose: Provides fast, full-text search capabilities for the <code>/search/</code> endpoint.</li> <li>Location: The index files are stored on the local filesystem. The path to the index directory must be provided to the running container via a volume mount.</li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>The Cross-Dataset Discovery service is designed for containerized deployment using Docker. This guide provides instructions for building the image, configuring the service, and understanding its dependencies.</p>"},{"location":"deployment/#docker","title":"Docker","text":"<p>The primary method for deploying the service is via a Docker container. The repository includes a <code>Dockerfile</code> for the build process.</p>"},{"location":"deployment/#dockerfile-stages","title":"Dockerfile Stages","text":"<ol> <li> <p>Builder Stage:</p> <ul> <li>Starts from a <code>python:3.11-slim</code> base image.</li> <li>Installs OpenJDK, which is a required dependency for the underlying search library (Pyserini/Lucene).</li> <li>Installs all Python dependencies from <code>requirements.txt</code>.</li> </ul> </li> <li> <p>Final Stage:</p> <ul> <li>Starts from a fresh <code>python:3.11-slim</code> image.</li> <li>Creates a non-root user (<code>appuser</code>) for security.</li> <li>Copies the installed OpenJDK and Python packages from the builder stage.</li> <li>Copies the application source code into the container.</li> <li>Sets <code>appuser</code> as the active user.</li> <li>Exposes port <code>8000</code>.</li> <li>Includes a <code>HEALTHCHECK</code> instruction that queries the <code>/health</code> endpoint to monitor container health.</li> <li>The default command (<code>CMD</code>) starts the application using <code>gunicorn</code> with a <code>uvicorn</code> worker, making it production-ready.</li> </ul> </li> </ol>"},{"location":"deployment/#build-and-run","title":"Build and Run","text":"<p>To build and run the container, use the standard Docker commands:</p> <pre><code># 1. Build the Docker image\ndocker build -t cross-dataset-discovery .\n\n# 2. Run the container\n# Note: You must provide a volume mount for the search index and all required environment variables.\ndocker run -p 8000:8000 \\\n  -v /path/to/your/search_index:/app/search_index \\\n  -e DB_CONNECTION_STRING=\"your-db-connection-string\" \\\n  -e OIDC_ISSUER_URL=\"your-oidc-issuer-url\" \\\n  -e INDEX_PATH=\"/app/search_index\" \\\n  --name cdd-service \\\n  cross-dataset-discovery\n</code></pre>"},{"location":"deployment/#configuration","title":"Configuration","text":"<p>The service is configured entirely through environment variables. This allows for flexible deployment across different environments without changing the container image.</p> Variable Description Example OIDC_ISSUER_URL The base URL of the OIDC identity provider for token validation. https://datagems-dev.scayle.es/oauth/realms/dev OIDC_AUDIENCE The audience claim that must be present in the JWT. cross-dataset-discovery-api GATEWAY_API_URL The base URL of the DataGEMS Gateway API, used to fetch user permissions. https://datagems-dev.scayle.es/gw DB_CONNECTION_STRING The full connection string for the PostgreSQL database (taken as secret from Vault). postgresql://user:pass@host:port/dbname TABLE_NAME The name of the table to check in the database during health checks. your_table_name IdpClientSecret The client secret for the identity provider (taken as secret from Vault). your-secret ROOT_PATH The path prefix for the API if it's running behind a reverse proxy. /cdd INDEX_PATH The path inside the container to the directory containing the search index files. /app/search_index"},{"location":"deployment/#dependencies","title":"Dependencies","text":"<p>The service requires several external systems and resources to be available at runtime to function correctly.</p>"},{"location":"deployment/#runtime-dependencies","title":"Runtime Dependencies","text":"<p>Search Index: - Description: A pre-built Pyserini/Lucene index is required for the core search functionality. The service does not create this index; it only reads from it. - Requirement: The index directory must be mounted into the container at the path specified by the INDEX_PATH environment variable.</p> <p>OIDC Provider: - Description: An OpenID Connect compliant identity provider (e.g., Keycloak) is necessary for authenticating users by validating their JWTs. - Requirement: The OIDC_ISSUER_URL and OIDC_AUDIENCE must be correctly configured to point to the identity provider.</p> <p>DataGEMS Gateway: - Description: The service communicates with the DataGEMS Gateway API to fetch dataset-level permissions for users. This is needed for enforcing data access policies. - Requirement: The GATEWAY_API_URL must be configured, and the service must have network access to it.</p>"},{"location":"deployment/#build-time-dependencies","title":"Build-time Dependencies","text":"<p>OpenJDK: - Description: The Java Development Kit is required by the Pyserini library, which is a Python wrapper around the Java-based Lucene search engine. - Requirement: It is automatically installed during the Docker build process as defined in the Dockerfile.</p>"},{"location":"error-codes/","title":"Status &amp; Error Codes","text":"<p>The API uses standard HTTP status codes to indicate the success or failure of a request.</p> Status Code Meaning Description <code>200 OK</code> Success The request was successful. <code>400 Bad Request</code> Validation Error The request body is malformed or contains invalid data (e.g., <code>k</code> is out of range, <code>dataset_ids</code> is an empty list). The response body will contain details about the validation error. <code>401 Unauthorized</code> Authentication Error The request lacks a valid JWT, the token is expired, or its signature is invalid. <code>403 Forbidden</code> Authorization Error The user is authenticated but does not have the required role (<code>user</code> or <code>dg_user</code>) to perform the action. <code>424 Failed Dependency</code> Downstream Service Error The API could not communicate with a required dependency, such as the OIDC provider or the database. The response body will contain details about the source of the failure. <code>500 Internal Server Error</code> Unexpected Error An unexpected error occurred on the server while processing the request. <code>503 Service Unavailable</code> Service Not Ready A core component, like the searcher, failed to initialize at startup. This is typically seen during health checks."},{"location":"faq/","title":"FAQ & Known Issues","text":"<p>TODO</p>"},{"location":"license/","title":"License","text":""},{"location":"logging/","title":"Logging &amp; Accounting","text":"<p>The service uses <code>structlog</code> for structured, JSON-formatted logging.</p>"},{"location":"logging/#log-structure","title":"Log Structure","text":"<p>All log entries are formatted as JSON objects with a consistent structure, including:</p> <ul> <li><code>@t</code>: ISO 8601 timestamp.</li> <li><code>@mt</code>: The log message (event).</li> <li><code>@l</code>: The log level (e.g., \"Info\", \"Warning\", \"Error\").</li> <li><code>DGCorrelationId</code>: A unique ID that ties together all log entries for a single HTTP request.</li> <li><code>SourceContext</code>: The name of the logger that produced the message.</li> <li><code>Application</code>: The name of the application (<code>cross-dataset-discovery-api</code>).</li> </ul> <p>Additional key-value pairs are added to provide context for specific events.</p>"},{"location":"logging/#correlation-id","title":"Correlation ID","text":"<p>Every HTTP request is assigned a correlation ID. - If the incoming request includes an <code>x-tracking-correlation</code> header, its value is used. - Otherwise, a new UUID is generated.</p> <p>This ID is included in every log message generated during the processing of that request and is also returned in the <code>x-tracking-correlation</code> response header. This allows for easy tracing of a request's entire lifecycle through the system and across different services.</p>"},{"location":"logging/#request-logging","title":"Request Logging","text":"<p>A middleware automatically logs the end of every HTTP request (except for health checks), capturing:</p> <ul> <li><code>RequestMethod</code> (e.g., GET, POST)</li> <li><code>RequestPath</code></li> <li><code>StatusCode</code></li> <li><code>ProcessTimeMS</code></li> <li><code>ResponseSize</code></li> </ul>"},{"location":"maintenance/","title":"Maintenance","text":"<p>TODO</p>"},{"location":"maintenance/#re-indexing","title":"Re-indexing","text":"<p>A key maintenance task for this service is updating the search index.</p>"},{"location":"onboarding/","title":"Onboarding (Adding Features)","text":"<p>TODO</p>"},{"location":"openapi/","title":"OpenAPI Specification","text":"<p>This service provides an OpenAPI 3.0 specification for its API, which can be used to generate clients or for integration with other tools.</p>"},{"location":"openapi/#specification-file","title":"Specification File","text":"<p>The OpenAPI specification is located in the documentation source directory.</p> <p>View openapi.json</p>"},{"location":"openapi/#postman-collection","title":"Postman Collection","text":"<p>Download Postman Collection</p>"},{"location":"openapi/#interactive-api-documentation","title":"Interactive API Documentation","text":"<p>The following is an interactive documentation panel generated directly from the <code>openapi.json</code> specification.</p>"},{"location":"qa/","title":"Quality Assurance (Evaluation)","text":"<p>TODO</p>"},{"location":"security/","title":"Security","text":""},{"location":"security/#authentication","title":"Authentication","text":"<p>Authentication is handled via the OAuth 2.0 and OpenID Connect (OIDC) protocols. Every request to a protected endpoint must include a valid JSON Web Token (JWT) in the <code>Authorization</code> header as a Bearer token.</p> <p>The service validates incoming JWTs against the OIDC provider's public keys. The token's signature, issuer, and audience are all verified to ensure its authenticity.</p>"},{"location":"security/#authorization","title":"Authorization","text":"<p>Authorization is role-based. The service checks for specific roles within the validated JWT's claims.</p> <ul> <li>Required Roles: To access the <code>/search/</code> endpoint, the user's token must contain at least one of the following roles in its <code>realm_access.roles</code> claim:<ul> <li><code>user</code></li> <li><code>dg_user</code></li> </ul> </li> </ul> <p>If a user attempts to access an endpoint without the required role, the API will respond with a <code>403 Forbidden</code> error.</p>"},{"location":"security/#dataset-level-permissions","title":"Dataset-Level Permissions","text":"<p>In addition to role-based access, the service enforces dataset-level permissions.</p> <p>Before executing a search, the API uses the user's token to call the DataGEMS Gateway's <code>/api/principal/me/context-grants</code> endpoint. This returns a list of all dataset IDs the user is permitted to access.</p> <ul> <li>Search Filtering:<ul> <li>If a user's search request includes <code>dataset_ids</code>, the service intersects this list with the user's authorized datasets. The search is only performed on the datasets present in both lists.</li> <li>All search results, regardless of the input filter, are always filtered against the user's authorized dataset list before being returned. This ensures a user can never see data from a dataset they are not permitted to access.</li> </ul> </li> </ul>"}]}